{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3630381d-38a0-4ecc-9a8a-cb1434cb9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ å¿«é€Ÿè™•ç†æ¨¡å¼...\n",
      "ğŸ“‚ æ‰¾åˆ° 164 å€‹æª”æ¡ˆ\n",
      "   âœ“ Air: 80, Pineapple: 11 é¡†\n",
      "   âœ“ 01_20250201: (900, 50)\n",
      "   âœ“ 01_20250202: (900, 50)\n",
      "   âœ“ 01_20250203: (900, 50)\n",
      "   âœ“ 01_20250204: (900, 50)\n",
      "   âœ“ 01_20250205: (900, 50)\n",
      "   âœ“ 01_20250206: (900, 50)\n",
      "   âœ“ 01_20250207: (900, 50)\n",
      "   âœ“ 01_20250208: (900, 50)\n",
      "   âœ“ 01_20250210: (900, 50)\n",
      "   âœ“ 01_20250211: (900, 54)\n",
      "   âœ“ 01_20260209: (900, 50)\n",
      "   âœ“ 02_20250202: (900, 50)\n",
      "   âœ“ 02_20250203: (900, 50)\n",
      "   âœ“ 02_20250204: (900, 50)\n",
      "   âœ“ 02_20250205: (900, 50)\n",
      "   âœ“ 02_20250206: (900, 50)\n",
      "   âœ“ 02_20250207: (900, 50)\n",
      "   âœ“ 02_20250208: (900, 50)\n",
      "   âœ“ 02_20250210: (900, 50)\n",
      "   âœ“ 02_20250211: (900, 54)\n",
      "   âœ“ 02_20260209: (900, 50)\n",
      "   âœ“ 03_20250203: (900, 50)\n",
      "   âœ“ 03_20250204: (900, 54)\n",
      "   âœ“ 03_20250205: (900, 54)\n",
      "   âœ“ 03_20250206: (900, 54)\n",
      "   âœ“ 03_20260207: (900, 54)\n",
      "   âœ“ 03_20260208: (900, 54)\n",
      "   âœ“ 03_20260209: (900, 54)\n",
      "   âœ“ 03_20260210: (900, 54)\n",
      "   âœ“ 03_20260211: (900, 54)\n",
      "   âœ“ 04_20250203: (900, 50)\n",
      "   âœ“ 04_20250204: (900, 54)\n",
      "   âœ“ 04_20250205: (899, 54)\n",
      "   âœ“ 04_20250206: (900, 50)\n",
      "   âœ“ 04_20260207: (900, 54)\n",
      "   âœ“ 04_20260208: (900, 54)\n",
      "   âœ“ 04_20260209: (900, 54)\n",
      "   âœ“ 04_20260210: (900, 54)\n",
      "   âœ“ 04_20260211: (900, 54)\n",
      "   âœ“ 05_20250202: (900, 50)\n",
      "   âœ“ 05_20250203: (900, 50)\n",
      "   âœ“ 05_20260204: (900, 50)\n",
      "   âœ“ 05_20260205: (900, 50)\n",
      "   âœ“ 05_20260206: (900, 50)\n",
      "   âœ“ 05_20260207: (900, 50)\n",
      "   âœ“ 05_20260208: (900, 50)\n",
      "   âœ“ 05_20260209: (900, 50)\n",
      "   âœ“ 05_20260210: (900, 50)\n",
      "   âœ“ 06_20250202: (900, 50)\n",
      "   âœ“ 06_20250203: (900, 50)\n",
      "   âœ“ 06_20250204: (900, 50)\n",
      "   âœ“ 06_20250205: (899, 50)\n",
      "   âœ“ 06_20260206: (900, 50)\n",
      "   âœ“ 06_20260209: (900, 50)\n",
      "   âœ“ 06_20260210: (900, 50)\n",
      "   âœ“ 07_20250203: (900, 50)\n",
      "   âœ“ 07_20250204: (900, 50)\n",
      "   âœ“ 07_20250205: (899, 50)\n",
      "   âœ“ 07_20260206: (900, 50)\n",
      "   âœ“ 07_20260207: (899, 50)\n",
      "   âœ“ 07_20260208: (900, 50)\n",
      "   âœ“ 07_20260209: (900, 50)\n",
      "   âœ“ 07_20260210: (900, 50)\n",
      "   âœ“ 08_20250203: (900, 50)\n",
      "   âœ“ 08_20250204: (900, 50)\n",
      "   âœ“ 08_20250205: (900, 50)\n",
      "   âœ“ 08_20260206: (900, 50)\n",
      "   âœ“ 08_20260207: (900, 50)\n",
      "   âœ“ 08_20260208: (900, 50)\n",
      "   âœ“ 08_20260209: (900, 50)\n",
      "   âœ“ 08_20260210: (900, 50)\n",
      "   âœ“ 09_20250210: (900, 50)\n",
      "   âœ“ 09_20250211: (900, 54)\n",
      "   âœ“ 09_20260207: (900, 50)\n",
      "   âœ“ 09_20260208: (900, 50)\n",
      "   âœ“ 09_20260209: (900, 50)\n",
      "   âœ“ 10_20260208: (900, 54)\n",
      "   âœ“ 10_20260209: (900, 54)\n",
      "   âœ“ 10_20260210: (900, 54)\n",
      "   âœ“ 10_20260211: (900, 54)\n",
      "   âœ“ 11_20260208: (900, 54)\n",
      "   âœ“ 11_20260209: (900, 54)\n",
      "   âœ“ 11_20260210: (900, 54)\n",
      "   âœ“ 11_20260211: (900, 54)\n",
      "âœ… å®Œæˆï¼ç”¢ç”Ÿ 11 é¡†é³³æ¢¨çš„ç‰¹å¾µ\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: è¼‰å…¥å¿…è¦å¥—ä»¶ =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ===== Cell 2: å¿«é€Ÿè¼‰å…¥å™¨ =====\n",
    "class QuickLoader:\n",
    "    \"\"\"ç°¡åŒ–ç‰ˆè¼‰å…¥å™¨ï¼Œç›´æ¥ç”¢ç”Ÿ arduino_features\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='data/raw'):\n",
    "        self.data_dir = data_dir\n",
    "        self.sensor_cols = ['MQ2_raw', 'MQ3_raw', 'MQ9_raw', 'MQ135_raw', 'TGS2602_raw']\n",
    "        \n",
    "        # R0 æ ¡æ­£åƒè€ƒå€¼\n",
    "        self.R0_reference = {\n",
    "            'MQ2_raw': 10.0,\n",
    "            'MQ3_raw': 60.0,\n",
    "            'MQ9_raw': 9.8,\n",
    "            'MQ135_raw': 76.63,\n",
    "            'TGS2602_raw': 2.5\n",
    "        }\n",
    "    \n",
    "    def load_and_process(self):\n",
    "        \"\"\"ä¸€æ¬¡æ€§è¼‰å…¥ä¸¦è™•ç†åˆ° arduino_features\"\"\"\n",
    "        print(\"ğŸ”§ å¿«é€Ÿè™•ç†æ¨¡å¼...\")\n",
    "        \n",
    "        # 1. è¼‰å…¥æ‰€æœ‰æª”æ¡ˆ\n",
    "        air_data, pineapple_data = self._load_files()\n",
    "        \n",
    "        if not pineapple_data:\n",
    "            print(\"âŒ æ‰¾ä¸åˆ°é³³æ¢¨æ•¸æ“š\")\n",
    "            return None\n",
    "        \n",
    "        # 2. è¨ˆç®— Arduino ç‰¹å¾µ\n",
    "        arduino_features = self._calculate_features(air_data, pineapple_data)\n",
    "        \n",
    "        print(f\"âœ… å®Œæˆï¼ç”¢ç”Ÿ {len(arduino_features)} é¡†é³³æ¢¨çš„ç‰¹å¾µ\")\n",
    "        return arduino_features\n",
    "    \n",
    "    def _load_files(self):\n",
    "        \"\"\"è¼‰å…¥ Excel æª”æ¡ˆ\"\"\"\n",
    "        xlsx_files = glob.glob(os.path.join(self.data_dir, '*.xlsx'))\n",
    "        print(f\"ğŸ“‚ æ‰¾åˆ° {len(xlsx_files)} å€‹æª”æ¡ˆ\")\n",
    "        \n",
    "        air_data = {}\n",
    "        pineapple_data = {}\n",
    "        \n",
    "        for file_path in xlsx_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            parts = filename.replace('.xlsx', '').split('_')\n",
    "            \n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            is_air = 'air' in filename\n",
    "            \n",
    "            # è®€å–æª”æ¡ˆ\n",
    "            try:\n",
    "                # å˜—è©¦è·³éç¬¬ä¸€è¡Œ\n",
    "                df = pd.read_excel(file_path, engine='openpyxl', skiprows=1)\n",
    "                \n",
    "                # æª¢æŸ¥æ˜¯å¦æœ‰æ­£ç¢ºçš„æ¬„ä½\n",
    "                if 'timestamp_ms' not in df.columns:\n",
    "                    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "                \n",
    "                # è½‰æ›æ•¸å­—å‹åˆ¥\n",
    "                for col in self.sensor_cols + ['timestamp_ms']:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # æ¸…ç†ç¼ºå€¼\n",
    "                df = df.dropna().reset_index(drop=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  è¼‰å…¥å¤±æ•— {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # åˆ†é¡å„²å­˜\n",
    "            if is_air:\n",
    "                if len(parts) == 3:\n",
    "                    # å…±ç”¨ air\n",
    "                    date = parts[1]\n",
    "                    air_data[f\"shared_{date}\"] = df\n",
    "                else:\n",
    "                    # å°ˆå±¬ air\n",
    "                    pid = parts[1]\n",
    "                    date = parts[2]\n",
    "                    air_data[f\"{pid}_{date}\"] = df\n",
    "            else:\n",
    "                pid = parts[1]\n",
    "                date = parts[2]\n",
    "                \n",
    "                if pid not in pineapple_data:\n",
    "                    pineapple_data[pid] = {}\n",
    "                pineapple_data[pid][date] = df\n",
    "        \n",
    "        print(f\"   âœ“ Air: {len(air_data)}, Pineapple: {len(pineapple_data)} é¡†\")\n",
    "        return air_data, pineapple_data\n",
    "    \n",
    "    def _calculate_features(self, air_data, pineapple_data):\n",
    "        \"\"\"è¨ˆç®— Arduino ç‰¹å¾µ\"\"\"\n",
    "        arduino_features = {}\n",
    "        \n",
    "        for pid, date_dict in pineapple_data.items():\n",
    "            arduino_features[pid] = {}\n",
    "            \n",
    "            for date, pine_df in date_dict.items():\n",
    "                # æ‰¾å°æ‡‰çš„ air baseline\n",
    "                air_df = self._get_air(air_data, pid, date)\n",
    "                \n",
    "                if air_df is None:\n",
    "                    print(f\"âš ï¸  {pid}_{date} æ‰¾ä¸åˆ° air baseline\")\n",
    "                    continue\n",
    "                \n",
    "                # è¨ˆç®— Delta\n",
    "                min_len = min(len(pine_df), len(air_df))\n",
    "                delta_df = pine_df.iloc[:min_len].copy()\n",
    "                \n",
    "                for col in self.sensor_cols:\n",
    "                    if col in pine_df.columns and col in air_df.columns:\n",
    "                        delta_df[f'{col}_delta'] = (\n",
    "                            pine_df[col].iloc[:min_len].values - \n",
    "                            air_df[col].iloc[:min_len].values\n",
    "                        )\n",
    "                \n",
    "                # è¨ˆç®— Arduino ç‰¹å¾µ\n",
    "                features_df = delta_df.copy()\n",
    "                \n",
    "                for col in self.sensor_cols:\n",
    "                    if col in delta_df.columns:\n",
    "                        sensor_name = col.replace('_raw', '')\n",
    "                        R0 = self.R0_reference.get(col, 10.0)\n",
    "                        \n",
    "                        # Rs/R0\n",
    "                        features_df[f'{sensor_name}_Rs_R0'] = delta_df[col] / R0\n",
    "                        \n",
    "                        # Delta Rs/R0\n",
    "                        delta_col = f'{col}_delta'\n",
    "                        if delta_col in delta_df.columns:\n",
    "                            features_df[f'{sensor_name}_delta_Rs_R0'] = delta_df[delta_col] / R0\n",
    "                        \n",
    "                        # ç§»å‹•å¹³å‡\n",
    "                        features_df[f'{sensor_name}_ma10'] = (\n",
    "                            delta_df[col].rolling(window=10, center=True).mean()\n",
    "                        )\n",
    "                        \n",
    "                        # ç§»å‹•æ¨™æº–å·®\n",
    "                        features_df[f'{sensor_name}_std10'] = (\n",
    "                            delta_df[col].rolling(window=10, center=True).std()\n",
    "                        )\n",
    "                \n",
    "                arduino_features[pid][date] = features_df\n",
    "                print(f\"   âœ“ {pid}_{date}: {features_df.shape}\")\n",
    "        \n",
    "        return arduino_features\n",
    "    \n",
    "    def _get_air(self, air_data, pid, date):\n",
    "        \"\"\"å–å¾—å°æ‡‰çš„ air baseline\"\"\"\n",
    "        # å°ˆå±¬ air\n",
    "        key = f\"{pid}_{date}\"\n",
    "        if key in air_data:\n",
    "            return air_data[key]\n",
    "        \n",
    "        # å…±ç”¨ air\n",
    "        shared_key = f\"shared_{date}\"\n",
    "        if shared_key in air_data:\n",
    "            return air_data[shared_key]\n",
    "        \n",
    "        return None\n",
    "\n",
    "# åŸ·è¡Œè¼‰å…¥\n",
    "loader = QuickLoader('data/raw')\n",
    "arduino_features = loader.load_and_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8306df2-bb8a-4981-9527-9dac78e24760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… data/processed/feature_data.pkl\n",
      "âœ… data/processed/feature_matrix.csv\n",
      "âœ… data/processed/labels.npy\n"
     ]
    }
   ],
   "source": [
    "# å¿«é€Ÿæª¢æŸ¥\n",
    "import os\n",
    "files_to_check = [\n",
    "    'data/processed/feature_data.pkl',\n",
    "    'data/processed/feature_matrix.csv',\n",
    "    'data/processed/labels.npy'\n",
    "]\n",
    "\n",
    "for file in files_to_check:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"âœ… {file}\")\n",
    "    else:\n",
    "        print(f\"âŒ {file} ä¸å­˜åœ¨ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63096d13-f197-4b6f-bb31-883b10766a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eed5c76-96ad-4122-9be0-f71ddb493886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f65b06-ccf6-4a17-b61d-103c7f7b2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” è¼‰å…¥çœŸå¯¦æˆç†Ÿåº¦æ¨™ç±¤...\n",
      "============================================================\n",
      "âœ… æˆåŠŸè¼‰å…¥: data/processed/maturity_labels.pkl\n",
      "\n",
      "ğŸ“Š æ¨™ç±¤æ‘˜è¦:\n",
      "   é³³æ¢¨æ•¸é‡: 11\n",
      "   é³³æ¢¨ IDs: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11']\n",
      "\n",
      "   å¯¦éš›é¡åˆ¥æ•¸: 23\n",
      "   é¡åˆ¥ç¯„åœ: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "\n",
      "   é¡åˆ¥åˆ†å¸ƒ:\n",
      "      é¡åˆ¥ 0: 7720 å€‹æ¨£æœ¬ ( 10.2%)\n",
      "      é¡åˆ¥ 1: 7420 å€‹æ¨£æœ¬ (  9.8%)\n",
      "      é¡åˆ¥ 2: 7240 å€‹æ¨£æœ¬ (  9.6%)\n",
      "      é¡åˆ¥ 3: 7470 å€‹æ¨£æœ¬ (  9.9%)\n",
      "      é¡åˆ¥ 4: 7950 å€‹æ¨£æœ¬ ( 10.5%)\n",
      "      é¡åˆ¥ 5: 5190 å€‹æ¨£æœ¬ (  6.9%)\n",
      "      é¡åˆ¥ 6: 5310 å€‹æ¨£æœ¬ (  7.0%)\n",
      "      é¡åˆ¥ 7: 4319 å€‹æ¨£æœ¬ (  5.7%)\n",
      "      é¡åˆ¥ 8: 4080 å€‹æ¨£æœ¬ (  5.4%)\n",
      "      é¡åˆ¥ 9: 4910 å€‹æ¨£æœ¬ (  6.5%)\n",
      "      é¡åˆ¥ 10: 3400 å€‹æ¨£æœ¬ (  4.5%)\n",
      "      é¡åˆ¥ 11: 2339 å€‹æ¨£æœ¬ (  3.1%)\n",
      "      é¡åˆ¥ 12: 1350 å€‹æ¨£æœ¬ (  1.8%)\n",
      "      é¡åˆ¥ 13: 1100 å€‹æ¨£æœ¬ (  1.5%)\n",
      "      é¡åˆ¥ 14: 1000 å€‹æ¨£æœ¬ (  1.3%)\n",
      "      é¡åˆ¥ 15:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "      é¡åˆ¥ 16:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "      é¡åˆ¥ 17:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "      é¡åˆ¥ 18:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "      é¡åˆ¥ 19:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "      é¡åˆ¥ 20:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "      é¡åˆ¥ 21:  660 å€‹æ¨£æœ¬ (  0.9%)\n",
      "      é¡åˆ¥ 22:  538 å€‹æ¨£æœ¬ (  0.7%)\n",
      "\n",
      "   ç¸½æ¨£æœ¬æ•¸: 75596\n",
      "\n",
      "   æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\n",
      "      Pineapple 01: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900), np.int64(5): np.int64(900), np.int64(6): np.int64(900), np.int64(7): np.int64(900), np.int64(8): np.int64(900), np.int64(9): np.int64(900), np.int64(10): np.int64(900)}\n",
      "      Pineapple 02: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900), np.int64(5): np.int64(900), np.int64(6): np.int64(900), np.int64(7): np.int64(900), np.int64(8): np.int64(900), np.int64(9): np.int64(900)}\n",
      "      Pineapple 03: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(450), np.int64(4): np.int64(450), np.int64(5): np.int64(390), np.int64(6): np.int64(510), np.int64(7): np.int64(390), np.int64(8): np.int64(510), np.int64(9): np.int64(410), np.int64(10): np.int64(490), np.int64(11): np.int64(370), np.int64(12): np.int64(530), np.int64(13): np.int64(500), np.int64(14): np.int64(400)}\n",
      "      Pineapple 04: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(300), np.int64(3): np.int64(600), np.int64(4): np.int64(900), np.int64(5): np.int64(900), np.int64(6): np.int64(900), np.int64(7): np.int64(330), np.int64(8): np.int64(570), np.int64(9): np.int64(900), np.int64(10): np.int64(510), np.int64(11): np.int64(389)}\n",
      "      Pineapple 05: {np.int64(0): np.int64(500), np.int64(1): np.int64(400), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(600), np.int64(5): np.int64(300), np.int64(6): np.int64(900), np.int64(7): np.int64(300), np.int64(8): np.int64(600), np.int64(9): np.int64(900), np.int64(10): np.int64(900), np.int64(11): np.int64(900)}\n",
      "      Pineapple 06: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900), np.int64(5): np.int64(300), np.int64(6): np.int64(600), np.int64(7): np.int64(899)}\n",
      "      Pineapple 07: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(300), np.int64(7): np.int64(300), np.int64(8): np.int64(300), np.int64(9): np.int64(300), np.int64(10): np.int64(300), np.int64(11): np.int64(300), np.int64(12): np.int64(300), np.int64(13): np.int64(300), np.int64(14): np.int64(300), np.int64(15): np.int64(300), np.int64(16): np.int64(300), np.int64(17): np.int64(300), np.int64(18): np.int64(300), np.int64(19): np.int64(300), np.int64(20): np.int64(300), np.int64(21): np.int64(360), np.int64(22): np.int64(538)}\n",
      "      Pineapple 08: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(300), np.int64(7): np.int64(300), np.int64(8): np.int64(300), np.int64(9): np.int64(600), np.int64(10): np.int64(300), np.int64(11): np.int64(380), np.int64(12): np.int64(520), np.int64(13): np.int64(300), np.int64(14): np.int64(300), np.int64(15): np.int64(300), np.int64(16): np.int64(300), np.int64(17): np.int64(300), np.int64(18): np.int64(300), np.int64(19): np.int64(300), np.int64(20): np.int64(300), np.int64(21): np.int64(300)}\n",
      "      Pineapple 09: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900)}\n",
      "      Pineapple 10: {np.int64(0): np.int64(900), np.int64(1): np.int64(440), np.int64(2): np.int64(460), np.int64(3): np.int64(900), np.int64(4): np.int64(900)}\n",
      "      Pineapple 11: {np.int64(0): np.int64(320), np.int64(1): np.int64(580), np.int64(2): np.int64(480), np.int64(3): np.int64(420), np.int64(4): np.int64(900), np.int64(5): np.int64(900)}\n",
      "\n",
      "âœ… çœŸå¯¦æ¨™ç±¤è¼‰å…¥å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ===== è¼‰å…¥çœŸå¯¦æˆç†Ÿåº¦æ¨™ç±¤ =====\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ” è¼‰å…¥çœŸå¯¦æˆç†Ÿåº¦æ¨™ç±¤...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å˜—è©¦è¼‰å…¥ pkl æˆ– json\n",
    "label_path_pkl = 'data/processed/maturity_labels.pkl'\n",
    "label_path_json = 'data/processed/maturity_labels.json'\n",
    "\n",
    "maturity_levels = None\n",
    "\n",
    "# 1. å„ªå…ˆå˜—è©¦ pkl\n",
    "if os.path.exists(label_path_pkl):\n",
    "    try:\n",
    "        with open(label_path_pkl, 'rb') as f:\n",
    "            maturity_levels = pickle.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥: {label_path_pkl}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥ pkl å¤±æ•—: {e}\")\n",
    "\n",
    "# 2. å¦‚æœ pkl å¤±æ•—ï¼Œå˜—è©¦ json\n",
    "if maturity_levels is None and os.path.exists(label_path_json):\n",
    "    try:\n",
    "        with open(label_path_json, 'r') as f:\n",
    "            maturity_levels_raw = json.load(f)\n",
    "        # è½‰æ›æˆ numpy array\n",
    "        maturity_levels = {k: np.array(v) for k, v in maturity_levels_raw.items()}\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥: {label_path_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥ json å¤±æ•—: {e}\")\n",
    "\n",
    "# 3. æª¢æŸ¥è¼‰å…¥çµæœ\n",
    "if maturity_levels is not None:\n",
    "    print(\"\\nğŸ“Š æ¨™ç±¤æ‘˜è¦:\")\n",
    "    print(f\"   é³³æ¢¨æ•¸é‡: {len(maturity_levels)}\")\n",
    "    print(f\"   é³³æ¢¨ IDs: {list(maturity_levels.keys())}\")\n",
    "    \n",
    "    # æŸ¥çœ‹æ‰€æœ‰å”¯ä¸€é¡åˆ¥\n",
    "    all_labels = []\n",
    "    for pid, labels in maturity_levels.items():\n",
    "        all_labels.extend(labels.tolist())\n",
    "    \n",
    "    unique_labels = np.unique(all_labels)\n",
    "    print(f\"\\n   å¯¦éš›é¡åˆ¥æ•¸: {len(unique_labels)}\")\n",
    "    print(f\"   é¡åˆ¥ç¯„åœ: {unique_labels}\")\n",
    "    \n",
    "    # é¡åˆ¥åˆ†å¸ƒ\n",
    "    print(f\"\\n   é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "    for label in unique_labels:\n",
    "        count = np.sum(np.array(all_labels) == label)\n",
    "        percentage = count / len(all_labels) * 100\n",
    "        print(f\"      é¡åˆ¥ {label}: {count:4d} å€‹æ¨£æœ¬ ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   ç¸½æ¨£æœ¬æ•¸: {len(all_labels)}\")\n",
    "    \n",
    "    # æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤åˆ†å¸ƒ\n",
    "    print(\"\\n   æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\")\n",
    "    for pid, labels in maturity_levels.items():\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"      Pineapple {pid}: {dict(zip(unique, counts))}\")\n",
    "    \n",
    "    print(\"\\nâœ… çœŸå¯¦æ¨™ç±¤è¼‰å…¥å®Œæˆï¼\")\n",
    "else:\n",
    "    print(\"\\nâŒ ç„¡æ³•è¼‰å…¥æ¨™ç±¤ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘\")\n",
    "    print(f\"   æŸ¥æ‰¾è·¯å¾‘: {label_path_pkl}\")\n",
    "    print(f\"   æˆ–: {label_path_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c407fff-d134-4df8-a089-74590db151c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æª¢æŸ¥çœŸå¯¦æ¨™ç±¤...\n",
      "============================================================\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 23\n",
      "é¡åˆ¥ç¯„åœ: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "\n",
      "é¡åˆ¥åˆ†å¸ƒ:\n",
      "  é¡åˆ¥ 0: 7720 å€‹æ¨£æœ¬ ( 10.2%)\n",
      "  é¡åˆ¥ 1: 7420 å€‹æ¨£æœ¬ (  9.8%)\n",
      "  é¡åˆ¥ 2: 7240 å€‹æ¨£æœ¬ (  9.6%)\n",
      "  é¡åˆ¥ 3: 7470 å€‹æ¨£æœ¬ (  9.9%)\n",
      "  é¡åˆ¥ 4: 7950 å€‹æ¨£æœ¬ ( 10.5%)\n",
      "  é¡åˆ¥ 5: 5190 å€‹æ¨£æœ¬ (  6.9%)\n",
      "  é¡åˆ¥ 6: 5310 å€‹æ¨£æœ¬ (  7.0%)\n",
      "  é¡åˆ¥ 7: 4319 å€‹æ¨£æœ¬ (  5.7%)\n",
      "  é¡åˆ¥ 8: 4080 å€‹æ¨£æœ¬ (  5.4%)\n",
      "  é¡åˆ¥ 9: 4910 å€‹æ¨£æœ¬ (  6.5%)\n",
      "  é¡åˆ¥ 10: 3400 å€‹æ¨£æœ¬ (  4.5%)\n",
      "  é¡åˆ¥ 11: 2339 å€‹æ¨£æœ¬ (  3.1%)\n",
      "  é¡åˆ¥ 12: 1350 å€‹æ¨£æœ¬ (  1.8%)\n",
      "  é¡åˆ¥ 13: 1100 å€‹æ¨£æœ¬ (  1.5%)\n",
      "  é¡åˆ¥ 14: 1000 å€‹æ¨£æœ¬ (  1.3%)\n",
      "  é¡åˆ¥ 15:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "  é¡åˆ¥ 16:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "  é¡åˆ¥ 17:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "  é¡åˆ¥ 18:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "  é¡åˆ¥ 19:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "  é¡åˆ¥ 20:  600 å€‹æ¨£æœ¬ (  0.8%)\n",
      "  é¡åˆ¥ 21:  660 å€‹æ¨£æœ¬ (  0.9%)\n",
      "  é¡åˆ¥ 22:  538 å€‹æ¨£æœ¬ (  0.7%)\n",
      "\n",
      "ç¸½æ¨£æœ¬æ•¸: 75596\n",
      "\n",
      "æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\n",
      "  Pineapple 01: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900), np.int64(5): np.int64(900), np.int64(6): np.int64(900), np.int64(7): np.int64(900), np.int64(8): np.int64(900), np.int64(9): np.int64(900), np.int64(10): np.int64(900)}\n",
      "  Pineapple 02: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900), np.int64(5): np.int64(900), np.int64(6): np.int64(900), np.int64(7): np.int64(900), np.int64(8): np.int64(900), np.int64(9): np.int64(900)}\n",
      "  Pineapple 03: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(450), np.int64(4): np.int64(450), np.int64(5): np.int64(390), np.int64(6): np.int64(510), np.int64(7): np.int64(390), np.int64(8): np.int64(510), np.int64(9): np.int64(410), np.int64(10): np.int64(490), np.int64(11): np.int64(370), np.int64(12): np.int64(530), np.int64(13): np.int64(500), np.int64(14): np.int64(400)}\n",
      "  Pineapple 04: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(300), np.int64(3): np.int64(600), np.int64(4): np.int64(900), np.int64(5): np.int64(900), np.int64(6): np.int64(900), np.int64(7): np.int64(330), np.int64(8): np.int64(570), np.int64(9): np.int64(900), np.int64(10): np.int64(510), np.int64(11): np.int64(389)}\n",
      "  Pineapple 05: {np.int64(0): np.int64(500), np.int64(1): np.int64(400), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(600), np.int64(5): np.int64(300), np.int64(6): np.int64(900), np.int64(7): np.int64(300), np.int64(8): np.int64(600), np.int64(9): np.int64(900), np.int64(10): np.int64(900), np.int64(11): np.int64(900)}\n",
      "  Pineapple 06: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900), np.int64(5): np.int64(300), np.int64(6): np.int64(600), np.int64(7): np.int64(899)}\n",
      "  Pineapple 07: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(300), np.int64(7): np.int64(300), np.int64(8): np.int64(300), np.int64(9): np.int64(300), np.int64(10): np.int64(300), np.int64(11): np.int64(300), np.int64(12): np.int64(300), np.int64(13): np.int64(300), np.int64(14): np.int64(300), np.int64(15): np.int64(300), np.int64(16): np.int64(300), np.int64(17): np.int64(300), np.int64(18): np.int64(300), np.int64(19): np.int64(300), np.int64(20): np.int64(300), np.int64(21): np.int64(360), np.int64(22): np.int64(538)}\n",
      "  Pineapple 08: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(300), np.int64(7): np.int64(300), np.int64(8): np.int64(300), np.int64(9): np.int64(600), np.int64(10): np.int64(300), np.int64(11): np.int64(380), np.int64(12): np.int64(520), np.int64(13): np.int64(300), np.int64(14): np.int64(300), np.int64(15): np.int64(300), np.int64(16): np.int64(300), np.int64(17): np.int64(300), np.int64(18): np.int64(300), np.int64(19): np.int64(300), np.int64(20): np.int64(300), np.int64(21): np.int64(300)}\n",
      "  Pineapple 09: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900)}\n",
      "  Pineapple 10: {np.int64(0): np.int64(900), np.int64(1): np.int64(440), np.int64(2): np.int64(460), np.int64(3): np.int64(900), np.int64(4): np.int64(900)}\n",
      "  Pineapple 11: {np.int64(0): np.int64(320), np.int64(1): np.int64(580), np.int64(2): np.int64(480), np.int64(3): np.int64(420), np.int64(4): np.int64(900), np.int64(5): np.int64(900)}\n"
     ]
    }
   ],
   "source": [
    "# ===== æª¢æŸ¥çœŸå¯¦æ¨™ç±¤çš„å¯¦éš›åˆ†å¸ƒ =====\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ” æª¢æŸ¥çœŸå¯¦æ¨™ç±¤...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. æŸ¥çœ‹æ‰€æœ‰å”¯ä¸€é¡åˆ¥\n",
    "all_labels = []\n",
    "for pid, labels in maturity_levels.items():\n",
    "    all_labels.extend(labels.tolist())\n",
    "\n",
    "unique_labels = np.unique(all_labels)\n",
    "label_counts = np.bincount(all_labels)\n",
    "\n",
    "print(f\"å¯¦éš›é¡åˆ¥æ•¸: {len(unique_labels)}\")\n",
    "print(f\"é¡åˆ¥ç¯„åœ: {unique_labels}\")\n",
    "print(f\"\\né¡åˆ¥åˆ†å¸ƒ:\")\n",
    "for label in unique_labels:\n",
    "    count = label_counts[label]\n",
    "    percentage = count / len(all_labels) * 100\n",
    "    print(f\"  é¡åˆ¥ {label}: {count:4d} å€‹æ¨£æœ¬ ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nç¸½æ¨£æœ¬æ•¸: {len(all_labels)}\")\n",
    "\n",
    "# 2. æŸ¥çœ‹æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤åˆ†å¸ƒ\n",
    "print(\"\\næ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\")\n",
    "for pid, labels in maturity_levels.items():\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  Pineapple {pid}: {dict(zip(unique, counts))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5fd541-06af-4653-9351-3b38ddc9cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é‡æ–°æ˜ å°„æ¨™ç±¤...\n",
      "============================================================\n",
      "æ˜ å°„è¦å‰‡:\n",
      "  0,1   â†’ Stage 0 (æœªæˆç†Ÿ)\n",
      "  2,3   â†’ Stage 1 (ç¨å¾®æˆç†Ÿ)\n",
      "  4,5   â†’ Stage 2 (æˆç†Ÿ)\n",
      "  6,7,8 â†’ Stage 3 (éç†Ÿ)\n",
      "\n",
      "æ˜ å°„å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ:\n",
      "  Pineapple 01: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1800), np.int64(2): np.int64(1800), np.int64(3): np.int64(4500)}\n",
      "  Pineapple 02: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1800), np.int64(2): np.int64(1800), np.int64(3): np.int64(3600)}\n",
      "  Pineapple 03: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1350), np.int64(2): np.int64(840), np.int64(3): np.int64(4110)}\n",
      "  Pineapple 04: {np.int64(0): np.int64(1800), np.int64(1): np.int64(900), np.int64(2): np.int64(1800), np.int64(3): np.int64(3599)}\n",
      "  Pineapple 05: {np.int64(0): np.int64(900), np.int64(1): np.int64(1800), np.int64(2): np.int64(900), np.int64(3): np.int64(4500)}\n",
      "  Pineapple 06: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1800), np.int64(2): np.int64(1200), np.int64(3): np.int64(1499)}\n",
      "  Pineapple 07: {np.int64(0): np.int64(600), np.int64(1): np.int64(600), np.int64(2): np.int64(600), np.int64(3): np.int64(5398)}\n",
      "  Pineapple 08: {np.int64(0): np.int64(600), np.int64(1): np.int64(600), np.int64(2): np.int64(600), np.int64(3): np.int64(5400)}\n",
      "  Pineapple 09: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1800), np.int64(2): np.int64(900)}\n",
      "  Pineapple 10: {np.int64(0): np.int64(1340), np.int64(1): np.int64(1360), np.int64(2): np.int64(900)}\n",
      "  Pineapple 11: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(1800)}\n",
      "\n",
      "ç¸½é«”åˆ†å¸ƒ:\n",
      "  Stage 0: 15140 å€‹æ¨£æœ¬ ( 20.0%)\n",
      "  Stage 1: 14710 å€‹æ¨£æœ¬ ( 19.5%)\n",
      "  Stage 2: 13140 å€‹æ¨£æœ¬ ( 17.4%)\n",
      "  Stage 3: 32606 å€‹æ¨£æœ¬ ( 43.1%)\n",
      "\n",
      "ç¸½æ¨£æœ¬æ•¸: 75596\n",
      "\n",
      "âœ… æ¨™ç±¤å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´šï¼\n",
      "âœ… å¯ä»¥é‡æ–°åŸ·è¡Œ Cell 11ï¼ˆç‰¹å¾µå·¥ç¨‹ï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ===== é‡æ–°æ˜ å°„æ¨™ç±¤ï¼š9ç­‰ç´š â†’ 4ç­‰ç´š ===== \n",
    "import numpy as np\n",
    "\n",
    "def remap_labels_9to4(labels):\n",
    "    \"\"\"\n",
    "    å°‡åŸå§‹ç´°åˆ†æˆç†Ÿåº¦æ¨™ç±¤æ˜ å°„åˆ° 4 ç­‰ç´šï¼ˆ0~3ï¼‰\n",
    "\n",
    "    ä¿ç•™åŸæœ¬é‚è¼¯ï¼š\n",
    "    0,1 â†’ 0\n",
    "    2,3 â†’ 1\n",
    "    4,5 â†’ 2\n",
    "    6 ä»¥ä¸Š â†’ 3   ï¼ˆåŒ…å« 6,7,8,9,10,11... éƒ½è¦–ç‚ºéç†Ÿï¼‰\n",
    "    \"\"\"\n",
    "    labels = np.array(labels, dtype=float)   # å…ˆè½‰æˆæ•¸å€¼ï¼ˆé¿å…å­—ä¸²/æµ®é»ï¼‰\n",
    "    labels_int = labels.astype(int)\n",
    "\n",
    "    mapped = np.empty_like(labels_int)\n",
    "\n",
    "    mapped[(labels_int == 0) | (labels_int == 1)] = 0\n",
    "    mapped[(labels_int == 2) | (labels_int == 3)] = 1\n",
    "    mapped[(labels_int == 4) | (labels_int == 5)] = 2\n",
    "    mapped[labels_int >= 6] = 3\n",
    "\n",
    "    return mapped.astype(int)\n",
    "\n",
    "print(\"ğŸ”„ é‡æ–°æ˜ å°„æ¨™ç±¤...\")\n",
    "print(\"=\"*60)\n",
    "print(\"æ˜ å°„è¦å‰‡:\")\n",
    "print(\"  0,1   â†’ Stage 0 (æœªæˆç†Ÿ)\")\n",
    "print(\"  2,3   â†’ Stage 1 (ç¨å¾®æˆç†Ÿ)\")\n",
    "print(\"  4,5   â†’ Stage 2 (æˆç†Ÿ)\")\n",
    "print(\"  6,7,8 â†’ Stage 3 (éç†Ÿ)\")\n",
    "print()\n",
    "\n",
    "# é‡æ–°æ˜ å°„æ‰€æœ‰é³³æ¢¨çš„æ¨™ç±¤\n",
    "maturity_levels_4stage = {}\n",
    "for pid, labels in maturity_levels.items():\n",
    "    maturity_levels_4stage[pid] = remap_labels_9to4(labels)\n",
    "\n",
    "# æª¢æŸ¥æ˜ å°„çµæœ\n",
    "print(\"æ˜ å°„å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "all_labels_4stage = []\n",
    "for pid, labels in maturity_levels_4stage.items():\n",
    "    all_labels_4stage.extend(labels.tolist())\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  Pineapple {pid}: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# ç¸½é«”åˆ†å¸ƒ\n",
    "print(\"\\nç¸½é«”åˆ†å¸ƒ:\")\n",
    "unique_4stage, counts_4stage = np.unique(all_labels_4stage, return_counts=True)\n",
    "for stage, count in zip(unique_4stage, counts_4stage):\n",
    "    percentage = count / len(all_labels_4stage) * 100\n",
    "    print(f\"  Stage {stage}: {count:5d} å€‹æ¨£æœ¬ ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nç¸½æ¨£æœ¬æ•¸: {len(all_labels_4stage)}\")\n",
    "\n",
    "# æ›¿æ›åŸæœ¬çš„ maturity_levels\n",
    "maturity_levels = maturity_levels_4stage\n",
    "\n",
    "print(\"\\nâœ… æ¨™ç±¤å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´šï¼\")\n",
    "print(\"âœ… å¯ä»¥é‡æ–°åŸ·è¡Œ Cell 11ï¼ˆç‰¹å¾µå·¥ç¨‹ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebc96a0-4dce-4d24-8198-c03f2894f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 120 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 82 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 67 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 67 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 67 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 52 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 37 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (626, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 626\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(127), np.int64(1): np.int64(123), np.int64(2): np.int64(108), np.int64(3): np.int64(268)}\n",
      "============================================================\n",
      "\n",
      "ğŸ’¾ å„²å­˜ç‰¹å¾µ...\n",
      "   âœ… CSV: data/processed\\feature_matrix.csv\n",
      "   âœ… Labels: data/processed\\labels.npy\n",
      "   âœ… Metadata: data/processed\\metadata.csv\n",
      "   âœ… Pickle: data/processed\\feature_data.pkl\n",
      "\n",
      "âœ… Step 4 å®Œæˆï¼å¯ä»¥é€²å…¥ Step 5ï¼ˆæ¨¡å‹è¨“ç·´ï¼‰\n",
      "\n",
      "ğŸ“Š ç‰¹å¾µæ‘˜è¦:\n",
      "   ç¸½æ¨£æœ¬æ•¸: 626\n",
      "   ç‰¹å¾µç¶­åº¦: 53\n",
      "   ç‰¹å¾µåˆ—è¡¨: ['MQ2_mean', 'MQ2_std', 'MQ2_min', 'MQ2_max', 'MQ2_range', 'MQ2_slope', 'MQ2_auc', 'MQ2_delta_mean', 'MQ2_delta_std', 'MQ2_delta_max_abs']... (å‰10å€‹)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 11: Step 4 - ç‰¹å¾µå·¥ç¨‹ï¼ˆä¿®æ­£ç‰ˆï¼‰=====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class FeatureEngineering:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å›ºå®šæ™‚é–“çª—å£æå–ç‰¹å¾µ\n",
    "    ç›®æ¨™ï¼šå°‡æ™‚é–“åºåˆ—è½‰æ›ç‚º (n_samples, n_features) çš„ç‰¹å¾µçŸ©é™£\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, arduino_features, maturity_labels, window_size=120):\n",
    "        \"\"\"\n",
    "        åƒæ•¸:\n",
    "            window_size: æ™‚é–“çª—å£å¤§å°ï¼ˆç§’ï¼‰\n",
    "                        å‡è¨­å–æ¨£ç‡ 1Hzï¼Œ120ç§’ = 120 å€‹æ¨£æœ¬é»\n",
    "        \"\"\"\n",
    "        self.arduino_features = arduino_features\n",
    "        self.maturity_labels = maturity_labels\n",
    "        self.window_size = window_size\n",
    "        self.sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "        \n",
    "        self.feature_matrix = None\n",
    "        self.labels = None\n",
    "        self.metadata = None\n",
    "        \n",
    "        self.output_dir = 'data/processed'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"æ‰¹æ¬¡æå–æ‰€æœ‰ç‰¹å¾µ\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"çª—å£å¤§å°: {self.window_size} ç§’\\n\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        for pid in self.arduino_features.keys():\n",
    "            if pid not in self.maturity_labels:\n",
    "                print(f\"âš ï¸  {pid}: æ‰¾ä¸åˆ°å°æ‡‰çš„æˆç†Ÿåº¦æ¨™ç±¤ï¼Œè·³é\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ğŸ è™•ç† Pineapple {pid}...\")\n",
    "            \n",
    "            # åˆä½µè©²é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\n",
    "            combined_df, combined_labels = self._combine_pineapple_data(pid)\n",
    "            \n",
    "            if combined_df is None or combined_labels is None:\n",
    "                print(f\"   âš ï¸  {pid}: æ²’æœ‰å¯ç”¨è³‡æ–™ï¼Œè·³é\\n\")\n",
    "                continue\n",
    "            \n",
    "            # æ»‘å‹•çª—å£æå–ç‰¹å¾µ\n",
    "            features, labels, metadata = self._sliding_window_extraction(\n",
    "                combined_df, combined_labels, pid\n",
    "            )\n",
    "            \n",
    "            # å¦‚æœé€™é¡†é³³æ¢¨æ²’æœ‰ç”¢ç”Ÿä»»ä½•è¦–çª—ï¼Œå°±è·³é\n",
    "            if features.size == 0:\n",
    "                print(f\"   âš ï¸  {pid}: æ²’æœ‰æœ‰æ•ˆçª—å£è¢«æå–ï¼Œè·³é\\n\")\n",
    "                continue\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            print(f\"   âœ“ æå– {len(features)} å€‹çª—å£ï¼Œæ¯å€‹ {features.shape[1]} ç¶­ç‰¹å¾µ\\n\")\n",
    "        \n",
    "        if not all_features:\n",
    "            raise ValueError(\"æ²’æœ‰ä»»ä½•é³³æ¢¨æˆåŠŸæå–ç‰¹å¾µï¼Œè«‹æª¢æŸ¥ maturity_labels é•·åº¦æ˜¯å¦èˆ‡åŸå§‹è³‡æ–™å°é½Šã€‚\")\n",
    "        \n",
    "        # åˆä½µæ‰€æœ‰é³³æ¢¨çš„ç‰¹å¾µ\n",
    "        self.feature_matrix = pd.DataFrame(\n",
    "            np.vstack(all_features),\n",
    "            columns=self._get_feature_names()\n",
    "        )\n",
    "        self.labels = np.hstack(all_labels)\n",
    "        self.metadata = pd.DataFrame(all_metadata)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"âœ… ç‰¹å¾µæå–å®Œæˆï¼\")\n",
    "        print(f\"   ç‰¹å¾µçŸ©é™£: {self.feature_matrix.shape}\")\n",
    "        print(f\"   æ¨™ç±¤æ•¸é‡: {len(self.labels)}\")\n",
    "        print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.feature_matrix, self.labels, self.metadata\n",
    "    \n",
    "    def _combine_pineapple_data(self, pid):\n",
    "        \"\"\"åˆä½µå–®é¡†é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\"\"\"\n",
    "        df_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        date_dict = self.arduino_features[pid]\n",
    "        labels = self.maturity_labels[pid]\n",
    "        \n",
    "        offset = 0\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date].copy()\n",
    "            n_samples = len(df)\n",
    "            \n",
    "            # å°é½Šè©²æ—¥æœŸçš„æ¨™ç±¤é•·åº¦ï¼ˆé¿å…è¶…å‡º maturity_labels ç¯„åœï¼‰\n",
    "            window_labels = labels[offset:offset + n_samples]\n",
    "            if len(window_labels) == 0:\n",
    "                # é€™å€‹æ—¥æœŸå®Œå…¨æ²’æœ‰å°æ‡‰æ¨™ç±¤ï¼Œè·³é\n",
    "                print(f\"   âš ï¸  {pid} @ {date}: æ‰¾ä¸åˆ°å°æ‡‰æ¨™ç±¤ï¼Œè·³éé€™ä¸€å¤©\")\n",
    "                continue\n",
    "            \n",
    "            # å¦‚æœæ¨™ç±¤æ•¸é‡æ¯”è³‡æ–™å°‘ï¼Œå°±åªä¿ç•™å‰é¢æœ‰æ¨™ç±¤çš„éƒ¨åˆ†\n",
    "            if len(window_labels) < n_samples:\n",
    "                df = df.iloc[:len(window_labels)]\n",
    "                n_samples = len(df)\n",
    "                print(f\"   âš ï¸  {pid} @ {date}: æ¨™ç±¤è¼ƒå°‘ï¼Œæˆªæ–·è³‡æ–™è‡³ {n_samples} ç­†\")\n",
    "            \n",
    "            df_list.append(df)\n",
    "            label_list.append(window_labels)\n",
    "            \n",
    "            offset += n_samples\n",
    "        \n",
    "        if not df_list:\n",
    "            return None, None\n",
    "        \n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        combined_labels = np.hstack(label_list)\n",
    "        \n",
    "        return combined_df, combined_labels\n",
    "    \n",
    "    def _sliding_window_extraction(self, df, labels, pid):\n",
    "        \"\"\"æ»‘å‹•çª—å£æå–ç‰¹å¾µ\"\"\"\n",
    "        features = []\n",
    "        window_labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        # åªåœ¨ã€Œè³‡æ–™èˆ‡æ¨™ç±¤éƒ½æœ‰ã€çš„ç¯„åœå…§æ»‘å‹•\n",
    "        n_samples = min(len(df), len(labels))\n",
    "        step_size = self.window_size  # ä¸é‡ç–Šçª—å£\n",
    "        \n",
    "        if n_samples < self.window_size:\n",
    "            print(f\"   âš ï¸  {pid}: æœ‰æ•ˆè³‡æ–™é•·åº¦ {n_samples} å°æ–¼çª—å£å¤§å° {self.window_size}ï¼Œç„¡æ³•ç”¢ç”Ÿè¦–çª—\")\n",
    "            return np.empty((0, len(self._get_feature_names()))), np.empty(0, dtype=int), []\n",
    "        \n",
    "        for start_idx in range(0, n_samples - self.window_size + 1, step_size):\n",
    "            end_idx = start_idx + self.window_size\n",
    "            \n",
    "            window_df = df.iloc[start_idx:end_idx]\n",
    "            window_label_array = labels[start_idx:end_idx]\n",
    "            \n",
    "            # å¦‚æœé€™å€‹è¦–çª—æ²’æœ‰æ¨™ç±¤ï¼Œç›´æ¥è·³é\n",
    "            if len(window_label_array) == 0:\n",
    "                print(f\"   âš ï¸  {pid}: è¦–çª— {start_idx}-{end_idx} æ²’æœ‰å°æ‡‰æ¨™ç±¤ï¼Œè·³é\")\n",
    "                continue\n",
    "            \n",
    "            # è©²çª—å£çš„ä¸»è¦æ¨™ç±¤ï¼ˆçœ¾æ•¸ï¼‰\n",
    "            unique, counts = np.unique(window_label_array, return_counts=True)\n",
    "            majority_label = unique[np.argmax(counts)]\n",
    "            \n",
    "            # æå–ç‰¹å¾µ\n",
    "            feature_vector = self._extract_window_features(window_df)\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            window_labels.append(majority_label)\n",
    "            metadata.append({\n",
    "                'pineapple_id': pid,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'majority_label': int(majority_label),\n",
    "                'label_purity': max(counts) / len(window_label_array)\n",
    "            })\n",
    "        \n",
    "        if not features:\n",
    "            return np.empty((0, len(self._get_feature_names()))), np.empty(0, dtype=int), []\n",
    "        \n",
    "        return np.array(features), np.array(window_labels), metadata\n",
    "    \n",
    "    def _extract_window_features(self, window_df):\n",
    "        \"\"\"å¾å–®ä¸€çª—å£æå–ç‰¹å¾µ\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            # 1. Rs/R0 ç‰¹å¾µ\n",
    "            col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "            if col_rs_r0 in window_df.columns:\n",
    "                data = window_df[col_rs_r0].values\n",
    "                \n",
    "                # çµ±è¨ˆç‰¹å¾µ\n",
    "                features.append(np.mean(data))           # å¹³å‡å€¼\n",
    "                features.append(np.std(data))            # æ¨™æº–å·®\n",
    "                features.append(np.min(data))            # æœ€å°å€¼\n",
    "                features.append(np.max(data))            # æœ€å¤§å€¼\n",
    "                features.append(np.max(data) - np.min(data))  # åæ‡‰å¹…åº¦\n",
    "                \n",
    "                # è®ŠåŒ–é€Ÿç‡ï¼ˆæ–œç‡ï¼‰\n",
    "                if len(data) > 1:\n",
    "                    x = np.arange(len(data))\n",
    "                    slope, _, _, _, _ = linregress(x, data)\n",
    "                    features.append(slope)\n",
    "                else:\n",
    "                    features.append(0)\n",
    "                \n",
    "                # AUCï¼ˆç´¯ç©åæ‡‰ï¼‰- ä¿®æ­£ç‰ˆ\n",
    "                try:\n",
    "                    auc = np.trapezoid(data, dx=1)  # numpy >= 2.0\n",
    "                except AttributeError:\n",
    "                    # fallback: æ‰‹å‹•æ¢¯å½¢ç©åˆ†\n",
    "                    if len(data) > 1:\n",
    "                        auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                    else:\n",
    "                        auc = data[0] if len(data) > 0 else 0\n",
    "                \n",
    "                features.append(auc)\n",
    "            else:\n",
    "                features.extend([0] * 7)  # å¡«å…… 7 å€‹ç‰¹å¾µ\n",
    "            \n",
    "            # 2. Delta Rs/R0 ç‰¹å¾µ\n",
    "            col_delta = f'{sensor}_delta_Rs_R0'\n",
    "            if col_delta in window_df.columns:\n",
    "                data = window_df[col_delta].values\n",
    "                \n",
    "                features.append(np.mean(data))\n",
    "                features.append(np.std(data))\n",
    "                features.append(np.max(np.abs(data)))    # æœ€å¤§çµ•å°è®ŠåŒ–\n",
    "            else:\n",
    "                features.extend([0] * 3)\n",
    "        \n",
    "        # 3. è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µï¼ˆæ¯”ä¾‹ç‰¹å¾µï¼‰\n",
    "        # MQ3/MQ2 æ¯”ä¾‹ï¼ˆé…’ç²¾ vs å¯ç‡ƒæ°£é«”ï¼‰\n",
    "        mq3 = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0\n",
    "        mq2 = window_df['MQ2_Rs_R0'].mean() if 'MQ2_Rs_R0' in window_df.columns else 0\n",
    "        features.append(mq3 / (mq2 + 1e-6))\n",
    "        \n",
    "        # MQ135/TGS2602 æ¯”ä¾‹ï¼ˆç©ºæ°£å“è³ª vs VOCï¼‰\n",
    "        mq135 = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0\n",
    "        tgs = window_df['TGS2602_Rs_R0'].mean() if 'TGS2602_Rs_R0' in window_df.columns else 0\n",
    "        features.append(mq135 / (tgs + 1e-6))\n",
    "        \n",
    "        # æ‰€æœ‰æ„Ÿæ¸¬å™¨å¹³å‡\n",
    "        all_means = [window_df[f'{s}_Rs_R0'].mean() \n",
    "                     for s in self.sensor_cols \n",
    "                     if f'{s}_Rs_R0' in window_df.columns]\n",
    "        features.append(np.mean(all_means) if all_means else 0)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    \n",
    "    def _get_feature_names(self):\n",
    "        \"\"\"ç”Ÿæˆç‰¹å¾µåç¨±\"\"\"\n",
    "        names = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            # Rs/R0 ç‰¹å¾µ\n",
    "            names.extend([\n",
    "                f'{sensor}_mean',\n",
    "                f'{sensor}_std',\n",
    "                f'{sensor}_min',\n",
    "                f'{sensor}_max',\n",
    "                f'{sensor}_range',\n",
    "                f'{sensor}_slope',\n",
    "                f'{sensor}_auc'\n",
    "            ])\n",
    "            \n",
    "            # Delta Rs/R0 ç‰¹å¾µ\n",
    "            names.extend([\n",
    "                f'{sensor}_delta_mean',\n",
    "                f'{sensor}_delta_std',\n",
    "                f'{sensor}_delta_max_abs'\n",
    "            ])\n",
    "        \n",
    "        # è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µ\n",
    "        names.extend([\n",
    "            'MQ3_MQ2_ratio',\n",
    "            'MQ135_TGS2602_ratio',\n",
    "            'all_sensors_mean'\n",
    "        ])\n",
    "        \n",
    "        return names\n",
    "    \n",
    "    def save_features(self):\n",
    "        \"\"\"å„²å­˜ç‰¹å¾µèˆ‡æ¨™ç±¤\"\"\"\n",
    "        print(\"\\nğŸ’¾ å„²å­˜ç‰¹å¾µ...\")\n",
    "        \n",
    "        # 1. ç‰¹å¾µçŸ©é™£ï¼ˆCSVï¼‰\n",
    "        csv_path = os.path.join(self.output_dir, 'feature_matrix.csv')\n",
    "        self.feature_matrix.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… CSV: {csv_path}\")\n",
    "        \n",
    "        # 2. æ¨™ç±¤ï¼ˆnumpyï¼‰\n",
    "        label_path = os.path.join(self.output_dir, 'labels.npy')\n",
    "        np.save(label_path, self.labels)\n",
    "        print(f\"   âœ… Labels: {label_path}\")\n",
    "        \n",
    "        # 3. Metadataï¼ˆCSVï¼‰\n",
    "        meta_path = os.path.join(self.output_dir, 'metadata.csv')\n",
    "        self.metadata.to_csv(meta_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… Metadata: {meta_path}\")\n",
    "        \n",
    "        # 4. å®Œæ•´ pickleï¼ˆæ–¹ä¾¿è¼‰å…¥ï¼‰\n",
    "        pkl_path = os.path.join(self.output_dir, 'feature_data.pkl')\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'features': self.feature_matrix,\n",
    "                'labels': self.labels,\n",
    "                'metadata': self.metadata,\n",
    "                'feature_names': self.feature_matrix.columns.tolist(),\n",
    "                'window_size': self.window_size\n",
    "            }, f)\n",
    "        print(f\"   âœ… Pickle: {pkl_path}\")\n",
    "        \n",
    "        print(\"\\nâœ… Step 4 å®Œæˆï¼å¯ä»¥é€²å…¥ Step 5ï¼ˆæ¨¡å‹è¨“ç·´ï¼‰\")\n",
    "\n",
    "# åŸ·è¡Œç‰¹å¾µå·¥ç¨‹\n",
    "feature_engineer = FeatureEngineering(\n",
    "    arduino_features, \n",
    "    maturity_levels,  # ä½¿ç”¨é‡æ–°æ˜ å°„å¾Œçš„ 4 ç­‰ç´šæ¨™ç±¤\n",
    "    window_size=120   # 120 ç§’çª—å£\n",
    ")\n",
    "\n",
    "X, y, metadata = feature_engineer.extract_all_features()\n",
    "feature_engineer.save_features()\n",
    "\n",
    "# é¡¯ç¤ºç‰¹å¾µæ‘˜è¦\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾µæ‘˜è¦:\")\n",
    "print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "print(f\"   ç‰¹å¾µç¶­åº¦: {X.shape[1]}\")\n",
    "print(f\"   ç‰¹å¾µåˆ—è¡¨: {X.columns.tolist()[:10]}... (å‰10å€‹)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc76852e-d62f-4a53-96e5-fe0acc98b402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æª¢æŸ¥ç‰¹å¾µçŸ©é™£æ˜¯å¦åŒ…å« NaN...\n",
      "âœ… æœªç™¼ç¾ NaNï¼Œç›´æ¥é€²å…¥è¨“ç·´ã€‚\n",
      "============================================================\n",
      "ğŸ¤– Step 5: æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š è¨“ç·´ SVM (RBF kernel)...\n",
      "   C=1, gamma=scale: 0.613 (+/- 0.026)\n",
      "   C=10, gamma=scale: 0.764 (+/- 0.027)\n",
      "   C=100, gamma=scale: 0.871 (+/- 0.016)\n",
      "   C=1, gamma=auto: 0.602 (+/- 0.020)\n",
      "\n",
      "   âœ… æœ€ä½³åƒæ•¸: {'C': 100, 'gamma': 'scale'}\n",
      "   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.871\n",
      "\n",
      "ğŸ“Š è¨“ç·´ Random Forest...\n",
      "   n_est=100, depth=None: 0.955 (+/- 0.011)\n",
      "   n_est=200, depth=10: 0.949 (+/- 0.011)\n",
      "   n_est=100, depth=5: 0.893 (+/- 0.016)\n",
      "\n",
      "   âœ… æœ€ä½³åƒæ•¸: {'n_estimators': 100, 'max_depth': None}\n",
      "   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.955\n",
      "\n",
      "ğŸ“Š æ¨¡å‹æ¯”è¼ƒ...\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ\n",
      "============================================================\n",
      "\n",
      "ğŸ¤– SVM (RBF)\n",
      "   äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.871\n",
      "\n",
      "   åˆ†é¡å ±å‘Š:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "        Stage 0      0.855     0.882     0.868       127\n",
      "        Stage 1      0.851     0.837     0.844       123\n",
      "        Stage 2      0.830     0.769     0.798       108\n",
      "        Stage 3      0.901     0.922     0.911       268\n",
      "   \n",
      "       accuracy                          0.871       626\n",
      "      macro avg      0.859     0.852     0.855       626\n",
      "   weighted avg      0.870     0.871     0.870       626\n",
      "   \n",
      "\n",
      "   æ··æ·†çŸ©é™£:\n",
      "   [[112   5   5   5]\n",
      " [  7 103   4   9]\n",
      " [  5   7  83  13]\n",
      " [  7   6   8 247]]\n",
      "\n",
      "ğŸ¤– Random Forest\n",
      "   äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.955\n",
      "\n",
      "   åˆ†é¡å ±å‘Š:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "        Stage 0      0.945     0.953     0.949       127\n",
      "        Stage 1      0.939     0.878     0.908       123\n",
      "        Stage 2      0.920     0.954     0.936       108\n",
      "        Stage 3      0.982     0.993     0.987       268\n",
      "   \n",
      "       accuracy                          0.955       626\n",
      "      macro avg      0.946     0.944     0.945       626\n",
      "   weighted avg      0.955     0.955     0.955       626\n",
      "   \n",
      "\n",
      "   æ··æ·†çŸ©é™£:\n",
      "   [[121   6   0   0]\n",
      " [  7 108   7   1]\n",
      " [  0   1 103   4]\n",
      " [  0   0   2 266]]\n",
      "\n",
      "âœ… æ¨è–¦æ¨¡å‹: Random Forest\n",
      "   æº–ç¢ºç‡: 0.955\n",
      "\n",
      "ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ...\n",
      "\n",
      "ğŸ” Top 10 é‡è¦ç‰¹å¾µ (Random Forest):\n",
      "    1. MQ135_auc                     : 0.0460\n",
      "    2. MQ135_TGS2602_ratio           : 0.0390\n",
      "    3. MQ9_auc                       : 0.0378\n",
      "    4. TGS2602_auc                   : 0.0368\n",
      "    5. MQ135_mean                    : 0.0365\n",
      "    6. MQ135_max                     : 0.0359\n",
      "    7. MQ135_min                     : 0.0358\n",
      "    8. MQ3_delta_mean                : 0.0351\n",
      "    9. MQ3_auc                       : 0.0346\n",
      "   10. MQ9_min                       : 0.0335\n",
      "\n",
      "   ğŸ’¾ å®Œæ•´ç‰¹å¾µé‡è¦æ€§: models\\feature_importance.csv\n",
      "\n",
      "ğŸ“Š éŒ¯èª¤æ¡ˆä¾‹åˆ†æ...\n",
      "\n",
      "âŒ éŒ¯èª¤æ¡ˆä¾‹åˆ†æ (SVM):\n",
      "   éŒ¯èª¤æ¨£æœ¬æ•¸: 81 / 626 (12.9%)\n",
      "\n",
      "   æœ€å¸¸è¦‹çš„æ··æ·† (çœŸå¯¦ â†’ é æ¸¬):\n",
      "      Stage 2 â†’ Stage 3: 13 æ¬¡\n",
      "      Stage 1 â†’ Stage 3: 9 æ¬¡\n",
      "      Stage 3 â†’ Stage 2: 8 æ¬¡\n",
      "      Stage 1 â†’ Stage 0: 7 æ¬¡\n",
      "      Stage 3 â†’ Stage 0: 7 æ¬¡\n",
      "\n",
      "   ğŸ’¾ éŒ¯èª¤æ¡ˆä¾‹è©³æƒ…: models\\error_cases.csv\n",
      "\n",
      "ğŸ’¾ å„²å­˜æ¨¡å‹...\n",
      "   âœ… SVM: models\\svm_model.pkl\n",
      "   âœ… Random Forest: models\\rf_model.pkl\n",
      "\n",
      "âœ… æ¨¡å‹å·²å„²å­˜ï¼Œå¯ä»¥é€²å…¥ Step 6ï¼ˆéƒ¨ç½²ï¼‰\n",
      "\n",
      "============================================================\n",
      "âœ… Step 5 å®Œæˆï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 12: Step 5 - æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    è¨“ç·´ä¸¦æ¯”è¼ƒå¤šå€‹åˆ†é¡æ¨¡å‹\n",
    "    ä¸»æ¨¡å‹ï¼šSVM (RBF kernel)\n",
    "    å°æ¯”æ¨¡å‹ï¼šRandom Forest\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, metadata):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.metadata = metadata\n",
    "        \n",
    "        # æ¨™æº–åŒ–ç‰¹å¾µ\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # æ¨¡å‹\n",
    "        self.svm_model = None\n",
    "        self.rf_model = None\n",
    "        \n",
    "        # çµæœ\n",
    "        self.results = {}\n",
    "        \n",
    "        self.output_dir = 'models'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def train_and_evaluate_all(self):\n",
    "        \"\"\"è¨“ç·´ä¸¦è©•ä¼°æ‰€æœ‰æ¨¡å‹\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ¤– Step 5: æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. SVM (ä¸»æ¨¡å‹)\n",
    "        print(\"\\nğŸ“Š è¨“ç·´ SVM (RBF kernel)...\")\n",
    "        self._train_svm()\n",
    "        \n",
    "        # 2. Random Forest (å°æ¯”)\n",
    "        print(\"\\nğŸ“Š è¨“ç·´ Random Forest...\")\n",
    "        self._train_random_forest()\n",
    "        \n",
    "        # 3. æ¯”è¼ƒçµæœ\n",
    "        print(\"\\nğŸ“Š æ¨¡å‹æ¯”è¼ƒ...\")\n",
    "        self._compare_models()\n",
    "        \n",
    "        # 4. ç‰¹å¾µé‡è¦æ€§åˆ†æ\n",
    "        print(\"\\nğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ...\")\n",
    "        self._analyze_feature_importance()\n",
    "        \n",
    "        # 5. éŒ¯èª¤æ¡ˆä¾‹åˆ†æ\n",
    "        print(\"\\nğŸ“Š éŒ¯èª¤æ¡ˆä¾‹åˆ†æ...\")\n",
    "        self._analyze_errors()\n",
    "        \n",
    "        # 6. å„²å­˜æ¨¡å‹\n",
    "        print(\"\\nğŸ’¾ å„²å­˜æ¨¡å‹...\")\n",
    "        self._save_models()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… Step 5 å®Œæˆï¼\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def _train_svm(self):\n",
    "        \"\"\"è¨“ç·´ SVM æ¨¡å‹\"\"\"\n",
    "        # ä½¿ç”¨ä¸åŒçš„ C å’Œ gamma åƒæ•¸\n",
    "        param_grid = [\n",
    "            {'C': 1, 'gamma': 'scale'},\n",
    "            {'C': 10, 'gamma': 'scale'},\n",
    "            {'C': 100, 'gamma': 'scale'},\n",
    "            {'C': 1, 'gamma': 'auto'},\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        \n",
    "        for params in param_grid:\n",
    "            svm = SVC(kernel='rbf', **params, random_state=42)\n",
    "            \n",
    "            # 5-fold äº¤å‰é©—è­‰\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(svm, self.X_scaled, self.y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            print(f\"   C={params['C']}, gamma={params['gamma']}: {mean_score:.3f} (+/- {scores.std():.3f})\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"\\n   âœ… æœ€ä½³åƒæ•¸: {best_params}\")\n",
    "        print(f\"   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: {best_score:.3f}\")\n",
    "        \n",
    "        # ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹\n",
    "        self.svm_model = SVC(kernel='rbf', **best_params, random_state=42)\n",
    "        self.svm_model.fit(self.X_scaled, self.y)\n",
    "        \n",
    "        # äº¤å‰é©—è­‰é æ¸¬ï¼ˆç”¨æ–¼æ··æ·†çŸ©é™£ï¼‰\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        y_pred_cv = cross_val_predict(self.svm_model, self.X_scaled, self.y, cv=cv)\n",
    "        \n",
    "        self.results['svm'] = {\n",
    "            'model': self.svm_model,\n",
    "            'cv_score': best_score,\n",
    "            'y_pred_cv': y_pred_cv,\n",
    "            'params': best_params\n",
    "        }\n",
    "    \n",
    "    def _train_random_forest(self):\n",
    "        \"\"\"è¨“ç·´ Random Forest æ¨¡å‹\"\"\"\n",
    "        # å˜—è©¦ä¸åŒçš„åƒæ•¸\n",
    "        param_grid = [\n",
    "            {'n_estimators': 100, 'max_depth': None},\n",
    "            {'n_estimators': 200, 'max_depth': 10},\n",
    "            {'n_estimators': 100, 'max_depth': 5},\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        \n",
    "        for params in param_grid:\n",
    "            rf = RandomForestClassifier(**params, random_state=42)\n",
    "            \n",
    "            # 5-fold äº¤å‰é©—è­‰\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(rf, self.X_scaled, self.y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            print(f\"   n_est={params['n_estimators']}, depth={params['max_depth']}: {mean_score:.3f} (+/- {scores.std():.3f})\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"\\n   âœ… æœ€ä½³åƒæ•¸: {best_params}\")\n",
    "        print(f\"   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: {best_score:.3f}\")\n",
    "        \n",
    "        # ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹\n",
    "        self.rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "        self.rf_model.fit(self.X_scaled, self.y)\n",
    "        \n",
    "        # äº¤å‰é©—è­‰é æ¸¬\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        y_pred_cv = cross_val_predict(self.rf_model, self.X_scaled, self.y, cv=cv)\n",
    "        \n",
    "        self.results['rf'] = {\n",
    "            'model': self.rf_model,\n",
    "            'cv_score': best_score,\n",
    "            'y_pred_cv': y_pred_cv,\n",
    "            'params': best_params\n",
    "        }\n",
    "    \n",
    "    def _compare_models(self):\n",
    "        \"\"\"æ¯”è¼ƒæ¨¡å‹è¡¨ç¾\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name, result in self.results.items():\n",
    "            model_display = \"SVM (RBF)\" if model_name == 'svm' else \"Random Forest\"\n",
    "            print(f\"\\nğŸ¤– {model_display}\")\n",
    "            print(f\"   äº¤å‰é©—è­‰æº–ç¢ºç‡: {result['cv_score']:.3f}\")\n",
    "            \n",
    "            # åˆ†é¡å ±å‘Š\n",
    "            y_pred = result['y_pred_cv']\n",
    "            print(\"\\n   åˆ†é¡å ±å‘Š:\")\n",
    "            report = classification_report(self.y, y_pred, \n",
    "                                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                                          digits=3)\n",
    "            print(\"   \" + report.replace(\"\\n\", \"\\n   \"))\n",
    "            \n",
    "            # æ··æ·†çŸ©é™£\n",
    "            cm = confusion_matrix(self.y, y_pred)\n",
    "            print(f\"\\n   æ··æ·†çŸ©é™£:\")\n",
    "            print(f\"   {cm}\")\n",
    "        \n",
    "        # æ¨è–¦æ¨¡å‹\n",
    "        best_model = max(self.results.items(), key=lambda x: x[1]['cv_score'])\n",
    "        print(f\"\\nâœ… æ¨è–¦æ¨¡å‹: {'SVM (RBF)' if best_model[0] == 'svm' else 'Random Forest'}\")\n",
    "        print(f\"   æº–ç¢ºç‡: {best_model[1]['cv_score']:.3f}\")\n",
    "    \n",
    "    def _analyze_feature_importance(self):\n",
    "        \"\"\"åˆ†æç‰¹å¾µé‡è¦æ€§ï¼ˆRandom Forestï¼‰\"\"\"\n",
    "        if 'rf' not in self.results:\n",
    "            return\n",
    "        \n",
    "        rf_model = self.results['rf']['model']\n",
    "        feature_names = self.X.columns if hasattr(self.X, 'columns') else [f'F{i}' for i in range(self.X.shape[1])]\n",
    "        \n",
    "        # å–å¾—ç‰¹å¾µé‡è¦æ€§\n",
    "        importances = rf_model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        print(\"\\nğŸ” Top 10 é‡è¦ç‰¹å¾µ (Random Forest):\")\n",
    "        for i, idx in enumerate(indices[:10], 1):\n",
    "            print(f\"   {i:2d}. {feature_names[idx]:30s}: {importances[idx]:.4f}\")\n",
    "        \n",
    "        # å„²å­˜å®Œæ•´ç‰¹å¾µé‡è¦æ€§\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'feature_importance.csv')\n",
    "        importance_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n   ğŸ’¾ å®Œæ•´ç‰¹å¾µé‡è¦æ€§: {csv_path}\")\n",
    "    \n",
    "    def _analyze_errors(self):\n",
    "        \"\"\"åˆ†æéŒ¯èª¤æ¡ˆä¾‹\"\"\"\n",
    "        print(\"\\nâŒ éŒ¯èª¤æ¡ˆä¾‹åˆ†æ (SVM):\")\n",
    "        \n",
    "        if 'svm' not in self.results:\n",
    "            return\n",
    "        \n",
    "        y_pred = self.results['svm']['y_pred_cv']\n",
    "        errors = self.y != y_pred\n",
    "        \n",
    "        print(f\"   éŒ¯èª¤æ¨£æœ¬æ•¸: {errors.sum()} / {len(self.y)} ({errors.sum()/len(self.y)*100:.1f}%)\")\n",
    "        \n",
    "        # åˆ†æå“ªäº›é¡åˆ¥å®¹æ˜“æ··æ·†\n",
    "        error_pairs = []\n",
    "        for true_label, pred_label in zip(self.y[errors], y_pred[errors]):\n",
    "            error_pairs.append((int(true_label), int(pred_label)))\n",
    "        \n",
    "        from collections import Counter\n",
    "        error_counts = Counter(error_pairs)\n",
    "        \n",
    "        print(\"\\n   æœ€å¸¸è¦‹çš„æ··æ·† (çœŸå¯¦ â†’ é æ¸¬):\")\n",
    "        for (true_l, pred_l), count in error_counts.most_common(5):\n",
    "            print(f\"      Stage {true_l} â†’ Stage {pred_l}: {count} æ¬¡\")\n",
    "        \n",
    "        # å„²å­˜éŒ¯èª¤æ¡ˆä¾‹\n",
    "        error_df = self.metadata[errors].copy()\n",
    "        error_df['true_label'] = self.y[errors]\n",
    "        error_df['predicted_label'] = y_pred[errors]\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'error_cases.csv')\n",
    "        error_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n   ğŸ’¾ éŒ¯èª¤æ¡ˆä¾‹è©³æƒ…: {csv_path}\")\n",
    "    \n",
    "    def _save_models(self):\n",
    "        \"\"\"å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹\"\"\"\n",
    "        # å„²å­˜ SVM\n",
    "        if 'svm' in self.results:\n",
    "            svm_path = os.path.join(self.output_dir, 'svm_model.pkl')\n",
    "            with open(svm_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.results['svm']['model'],\n",
    "                    'scaler': self.scaler,\n",
    "                    'params': self.results['svm']['params'],\n",
    "                    'cv_score': self.results['svm']['cv_score']\n",
    "                }, f)\n",
    "            print(f\"   âœ… SVM: {svm_path}\")\n",
    "        \n",
    "        # å„²å­˜ Random Forest\n",
    "        if 'rf' in self.results:\n",
    "            rf_path = os.path.join(self.output_dir, 'rf_model.pkl')\n",
    "            with open(rf_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.results['rf']['model'],\n",
    "                    'scaler': self.scaler,\n",
    "                    'params': self.results['rf']['params'],\n",
    "                    'cv_score': self.results['rf']['cv_score']\n",
    "                }, f)\n",
    "            print(f\"   âœ… Random Forest: {rf_path}\")\n",
    "        \n",
    "        print(\"\\nâœ… æ¨¡å‹å·²å„²å­˜ï¼Œå¯ä»¥é€²å…¥ Step 6ï¼ˆéƒ¨ç½²ï¼‰\")\n",
    "\n",
    "# è¼‰å…¥ç‰¹å¾µ\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features']\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "\n",
    "# ===== æ–°å¢ï¼šå…ˆæŠŠå« NaN çš„æ¨£æœ¬æ¿¾æ‰ï¼ˆä¸å‹• ModelTrainer å…§éƒ¨é‚è¼¯ï¼‰=====\n",
    "print(\"ğŸ” æª¢æŸ¥ç‰¹å¾µçŸ©é™£æ˜¯å¦åŒ…å« NaN...\")\n",
    "\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    nan_mask = X.isna().any(axis=1)   # æ¯ä¸€åˆ—åªè¦æœ‰ä¸€å€‹ NaN å°±æ¨™è¨˜ True\n",
    "else:\n",
    "    nan_mask = np.isnan(X).any(axis=1)\n",
    "\n",
    "n_nan_rows = nan_mask.sum()\n",
    "\n",
    "if n_nan_rows > 0:\n",
    "    print(f\"âš ï¸ ç™¼ç¾ {n_nan_rows} ç­†æ¨£æœ¬åŒ…å« NaNï¼Œå°‡å¾è¨“ç·´è³‡æ–™ä¸­æ’é™¤ã€‚\")\n",
    "\n",
    "    # Xï¼šåªä¿ç•™æ²’æœ‰ NaN çš„åˆ—\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.loc[~nan_mask].reset_index(drop=True)\n",
    "    else:\n",
    "        X = X[~nan_mask]\n",
    "\n",
    "    # y ä¹Ÿè¦å°é½ŠåŒæ¨£çš„åˆ—\n",
    "    if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "        y = y.loc[~nan_mask].reset_index(drop=True)\n",
    "    else:\n",
    "        y = y[~nan_mask]\n",
    "\n",
    "    # metadata ä¸€èµ·å°é½Šï¼ˆçµ¦éŒ¯èª¤åˆ†æç”¨ï¼‰\n",
    "    if isinstance(metadata, pd.DataFrame):\n",
    "        metadata = metadata.loc[~nan_mask].reset_index(drop=True)\n",
    "    elif isinstance(metadata, np.ndarray):\n",
    "        metadata = metadata[~nan_mask]\n",
    "\n",
    "    print(f\"âœ… å·²ç§»é™¤å« NaN çš„æ¨£æœ¬ï¼Œå‰©é¤˜æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "else:\n",
    "    print(\"âœ… æœªç™¼ç¾ NaNï¼Œç›´æ¥é€²å…¥è¨“ç·´ã€‚\")\n",
    "\n",
    "# ===== è¨“ç·´æ¨¡å‹ï¼ˆä¸‹é¢å®Œå…¨ç…§ä½ åŸæœ¬çš„é‚è¼¯ï¼‰=====\n",
    "trainer = ModelTrainer(X, y, metadata)\n",
    "trainer.train_and_evaluate_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307dd5-29aa-493e-ba29-cf9b5a3fffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da50e85-8ce9-450e-967e-666ac6817137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥ä¸‹ç‚ºå˜—è©¦èª¿æ•´æ”¹é€²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f17f23-31a3-4399-b853-f2b383a8b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ğŸ“Š å¯¦é©— 1ï¼šé¡åˆ¥æ¬Šé‡èª¿æ•´\n",
    "#æ¸¬è©¦ 3 ç¨®ç­–ç•¥ï¼š\n",
    "\n",
    "#Noneï¼ˆåŸºæº–ï¼‰\n",
    "\n",
    "#Balancedï¼ˆè‡ªå‹•å¹³è¡¡ï¼‰\n",
    "\n",
    "#Customï¼ˆå¼·åŒ– Stage 2ï¼Œçµ¦ 1.5 å€æ¬Šé‡ï¼‰\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 2ï¼šä¸åŒçª—å£å¤§å°\n",
    "#æ¸¬è©¦ 4 ç¨®çª—å£ï¼š\n",
    "\n",
    "#60 ç§’ï¼šæ›´ç´°ç·»ï¼Œæ¨£æœ¬æ›´å¤š\n",
    "\n",
    "#120 ç§’ï¼šç•¶å‰åŸºæº–\n",
    "\n",
    "#180 ç§’ï¼šæ›´ç©©å®š\n",
    "\n",
    "#240 ç§’ï¼šæœ€ç©©å®šä½†æ¨£æœ¬è¼ƒå°‘\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 3ï¼šç‰¹å¾µé¸æ“‡\n",
    "#æ¸¬è©¦ä½¿ç”¨ 10, 20, 30, 40, 53 å€‹ç‰¹å¾µçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82898acd-dbd9-430e-b371-5cc0b85f744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_50s_final.pkl\n",
      "features_60s.pkl\n",
      "feature_data.pkl\n",
      "feature_matrix.csv\n",
      "labels.npy\n",
      "maturity_labels.json\n",
      "maturity_labels.pkl\n",
      "maturity_labels_summary.csv\n",
      "maturity_levels_4class.pkl\n",
      "metadata.csv\n",
      "step3_quality_report.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# å®šç¾©è·¯å¾‘\n",
    "path = './data/processed/'\n",
    "\n",
    "# å–å¾—è©²ç›®éŒ„ä¸‹æ‰€æœ‰æª”æ¡ˆèˆ‡è³‡æ–™å¤¾åç¨±\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# æ‰“å°å‡ºä¾†\n",
    "for file_name in file_list:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68280ca-1511-4d9c-9aa6-68b3d80ddb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è¼‰å…¥å¿…è¦è³‡æ–™...\n",
      "âœ… maturity_levels: 11 é¡†é³³æ¢¨\n",
      "âœ… arduino_features: 11 é¡†é³³æ¢¨\n",
      "\n",
      "âœ… è®Šæ•¸è¼‰å…¥å®Œæˆï¼å¯ä»¥åŸ·è¡Œ Cell 14\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 14a: è¼‰å…¥å¿…è¦è®Šæ•¸ =====\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"ğŸ“‚ è¼‰å…¥å¿…è¦è³‡æ–™...\")\n",
    "\n",
    "# 1. è¼‰å…¥ maturity_levelsï¼ˆ4ç­‰ç´šï¼‰\n",
    "with open('data/processed/maturity_levels_4class.pkl', 'rb') as f:\n",
    "    maturity_levels = pickle.load(f)\n",
    "print(f\"âœ… maturity_levels: {len(maturity_levels)} é¡†é³³æ¢¨\")\n",
    "\n",
    "# 2. æª¢æŸ¥ arduino_features æ˜¯å¦å­˜åœ¨\n",
    "if 'arduino_features' not in globals():\n",
    "    print(\"\\nâš ï¸  arduino_features ä¸å­˜åœ¨\")\n",
    "    print(\"   è«‹é‡æ–°åŸ·è¡Œ Cell 2 (å¿«é€Ÿè¼‰å…¥å™¨)\")\n",
    "    print(\"   æˆ–åŸ·è¡Œä»¥ä¸‹ç¨‹å¼ç¢¼é‡æ–°è¼‰å…¥...\\n\")\n",
    "    \n",
    "    # å¿«é€Ÿé‡æ–°è¼‰å…¥ï¼ˆå¦‚æœä½ æœ‰åŸå§‹ CSVï¼‰\n",
    "    # é€™è£¡éœ€è¦é‡æ–°åŸ·è¡Œ Cell 2 çš„ç¨‹å¼ç¢¼\n",
    "else:\n",
    "    print(f\"âœ… arduino_features: {len(arduino_features)} é¡†é³³æ¢¨\")\n",
    "\n",
    "print(\"\\nâœ… è®Šæ•¸è¼‰å…¥å®Œæˆï¼å¯ä»¥åŸ·è¡Œ Cell 14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e904cb3-b689-46d2-b949-1836dd59aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ğŸ“Š å¯¦é©— 1ï¼šé¡åˆ¥æ¬Šé‡èª¿æ•´\n",
    "#æ¸¬è©¦ 3 ç¨®ç­–ç•¥ï¼š\n",
    "\n",
    "#Noneï¼ˆåŸºæº–ï¼‰\n",
    "\n",
    "#Balancedï¼ˆè‡ªå‹•å¹³è¡¡ï¼‰\n",
    "\n",
    "#Customï¼ˆå¼·åŒ– Stage 2ï¼Œçµ¦ 1.5 å€æ¬Šé‡ï¼‰\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 2ï¼šä¸åŒçª—å£å¤§å°\n",
    "#æ¸¬è©¦ 4 ç¨®çª—å£ï¼š\n",
    "\n",
    "#60 ç§’ï¼šæ›´ç´°ç·»ï¼Œæ¨£æœ¬æ›´å¤š\n",
    "\n",
    "#120 ç§’ï¼šç•¶å‰åŸºæº–\n",
    "\n",
    "#180 ç§’ï¼šæ›´ç©©å®š\n",
    "\n",
    "#240 ç§’ï¼šæœ€ç©©å®šä½†æ¨£æœ¬è¼ƒå°‘\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 3ï¼šç‰¹å¾µé¸æ“‡\n",
    "#æ¸¬è©¦ä½¿ç”¨ 10, 20, 30, 40, 53 å€‹ç‰¹å¾µçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff02202-370f-4c80-96b0-51c7d2122748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æº–å‚™åŸ·è¡Œæ”¹é€²å¯¦é©—...\n",
      "   ç¢ºä¿å·²åŸ·è¡Œï¼š\n",
      "   - arduino_features å·²è¼‰å…¥\n",
      "   - maturity_levels å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´š\n",
      "   - FeatureEngineering å·²å®šç¾©ï¼ˆCell 11ï¼‰\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ æ¨¡å‹æ”¹é€²å¯¦é©—ï¼ˆåŸºæ–¼çœŸå¯¦æ¨™ç±¤ï¼‰\n",
      "======================================================================\n",
      "åŸºæº–æ¨¡å‹: Random Forest, æº–ç¢ºç‡ = 0.955\n",
      "\n",
      "ğŸ“Š å¯¦é©— 1: é¡åˆ¥æ¬Šé‡èª¿æ•´\n",
      "----------------------------------------------------------------------\n",
      "   æª¢æ¸¬åˆ°çš„é¡åˆ¥: [0 1 2 3]\n",
      "ğŸ“‰ None (åŸºæº–)                     : æº–ç¢ºç‡=0.949 (-0.006)\n",
      "   âš ï¸ Stage 1 Recall: 0.862 (åŸºæº–=0.901)\n",
      "ğŸ“‰ Balanced                      : æº–ç¢ºç‡=0.947 (-0.008)\n",
      "   âš ï¸ Stage 1 Recall: 0.846 (åŸºæº–=0.901)\n",
      "ğŸ“‰ Custom (å¼·åŒ–Stage1)             : æº–ç¢ºç‡=0.949 (-0.006)\n",
      "   âš ï¸ Stage 1 Recall: 0.862 (åŸºæº–=0.901)\n",
      "ğŸ“‰ Custom (å¼·åŒ–Stage2)             : æº–ç¢ºç‡=0.946 (-0.009)\n",
      "   âš ï¸ Stage 1 Recall: 0.862 (åŸºæº–=0.901)\n",
      "\n",
      "ğŸ“Š å¯¦é©— 2: ä¸åŒçª—å£å¤§å°\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 60 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 165 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 150 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 134 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 104 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 119 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (1257, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 1257\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(365), np.int64(1): np.int64(295), np.int64(2): np.int64(255), np.int64(3): np.int64(342)}\n",
      "============================================================\n",
      "ğŸ“ˆ çª—å£ 60s: æº–ç¢ºç‡=0.997 (+0.042), æ¨£æœ¬æ•¸=1257\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 120 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 120 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 82 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 67 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 67 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 67 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 52 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 37 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (626, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 626\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(184), np.int64(1): np.int64(148), np.int64(2): np.int64(128), np.int64(3): np.int64(166)}\n",
      "============================================================\n",
      "ğŸ“‰ çª—å£ 120s: æº–ç¢ºç‡=0.941 (-0.014), æ¨£æœ¬æ•¸=626\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 180 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 180 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 55 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 50 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 44 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 34 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 39 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 40 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 25 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 20 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 20 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (417, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 417\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(122), np.int64(1): np.int64(98), np.int64(2): np.int64(85), np.int64(3): np.int64(112)}\n",
      "============================================================\n",
      "ğŸ“‰ çª—å£ 180s: æº–ç¢ºç‡=0.942 (-0.013), æ¨£æœ¬æ•¸=417\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 240 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 240 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 41 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 37 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 33 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 33 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 33 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 26 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 29 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 18 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (310, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 310\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(94), np.int64(1): np.int64(72), np.int64(2): np.int64(63), np.int64(3): np.int64(81)}\n",
      "============================================================\n",
      "ğŸ“‰ çª—å£ 240s: æº–ç¢ºç‡=0.865 (-0.090), æ¨£æœ¬æ•¸=310\n",
      "\n",
      "ğŸ“Š å¯¦é©— 3: ç‰¹å¾µé¸æ“‡\n",
      "----------------------------------------------------------------------\n",
      "ğŸ“‰ ä½¿ç”¨ 10 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.934 (-0.021)\n",
      "\n",
      "   Top 20 ç‰¹å¾µ:\n",
      "       1. MQ2_mean\n",
      "       2. MQ2_min\n",
      "       3. MQ2_max\n",
      "       4. MQ2_auc\n",
      "       5. MQ2_delta_mean\n",
      "       6. MQ2_delta_max_abs\n",
      "       7. MQ3_mean\n",
      "       8. MQ3_min\n",
      "       9. MQ3_max\n",
      "      10. MQ3_auc\n",
      "ğŸ“ˆ ä½¿ç”¨ 20 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.955 (+0.000)\n",
      "ğŸ“‰ ä½¿ç”¨ 30 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.947 (-0.008)\n",
      "ğŸ“‰ ä½¿ç”¨ 40 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.954 (-0.001)\n",
      "ğŸ“‰ ä½¿ç”¨ 53 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.949 (-0.006)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ å¯¦é©—ç¸½çµå ±å‘Š\n",
      "======================================================================\n",
      "\n",
      "ğŸ† æœ€ä½³æ–¹æ¡ˆ: window_60s\n",
      "   æº–ç¢ºç‡: 0.997\n",
      "   æ”¹é€²å¹…åº¦: +0.042\n",
      "\n",
      "âœ… æ‰¾åˆ°æ”¹é€²æ–¹æ¡ˆï¼æ¯”åŸºæº–æå‡ 4.38%\n",
      "   å¾ 0.955 â†’ 0.997\n",
      "\n",
      "ğŸ’¾ å®Œæ•´å ±å‘Š: models/improvements\\improvement_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 24375 (\\N{CJK UNIFIED IDEOGRAPH-5F37}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 36914 (\\N{CJK UNIFIED IDEOGRAPH-9032}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 23526 (\\N{CJK UNIFIED IDEOGRAPH-5BE6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 23565 (\\N{CJK UNIFIED IDEOGRAPH-5C0D}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 65288 (\\N{FULLWIDTH LEFT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 27161 (\\N{CJK UNIFIED IDEOGRAPH-6A19}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 31844 (\\N{CJK UNIFIED IDEOGRAPH-7C64}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:339: UserWarning: Glyph 65289 (\\N{FULLWIDTH RIGHT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 24375 (\\N{CJK UNIFIED IDEOGRAPH-5F37}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 36914 (\\N{CJK UNIFIED IDEOGRAPH-9032}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 23526 (\\N{CJK UNIFIED IDEOGRAPH-5BE6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 23565 (\\N{CJK UNIFIED IDEOGRAPH-5C0D}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 65288 (\\N{FULLWIDTH LEFT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 27161 (\\N{CJK UNIFIED IDEOGRAPH-6A19}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 31844 (\\N{CJK UNIFIED IDEOGRAPH-7C64}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 65289 (\\N{FULLWIDTH RIGHT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12756\\109504649.py:342: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š å°æ¯”åœ–: models/improvements\\improvement_comparison.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 24375 (\\N{CJK UNIFIED IDEOGRAPH-5F37}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 36914 (\\N{CJK UNIFIED IDEOGRAPH-9032}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 23526 (\\N{CJK UNIFIED IDEOGRAPH-5BE6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 23565 (\\N{CJK UNIFIED IDEOGRAPH-5C0D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 65288 (\\N{FULLWIDTH LEFT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 27161 (\\N{CJK UNIFIED IDEOGRAPH-6A19}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 31844 (\\N{CJK UNIFIED IDEOGRAPH-7C64}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 65289 (\\N{FULLWIDTH RIGHT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 13: æ¨¡å‹æ”¹é€²å¯¦é©—ï¼ˆæ›´æ–°ç‰ˆï¼‰===== \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "class ModelImprovement:\n",
    "    \"\"\"ç³»çµ±åŒ–æ¸¬è©¦å¤šç¨®æ”¹é€²æ–¹æ¡ˆ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = {}\n",
    "        self.baseline_score = 0.955  # âœ… æ›´æ–°ç‚ºçœŸå¯¦åŸºæº–\n",
    "        self.output_dir = 'models/improvements'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        \"\"\"åŸ·è¡Œæ‰€æœ‰æ”¹é€²å¯¦é©—\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸ”¬ æ¨¡å‹æ”¹é€²å¯¦é©—ï¼ˆåŸºæ–¼çœŸå¯¦æ¨™ç±¤ï¼‰\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"åŸºæº–æ¨¡å‹: Random Forest, æº–ç¢ºç‡ = {self.baseline_score:.3f}\\n\")\n",
    "        \n",
    "        # å¯¦é©— 1: é¡åˆ¥æ¬Šé‡\n",
    "        print(\"ğŸ“Š å¯¦é©— 1: é¡åˆ¥æ¬Šé‡èª¿æ•´\")\n",
    "        print(\"-\"*70)\n",
    "        self._experiment_class_weights()\n",
    "        \n",
    "        # å¯¦é©— 2: ä¸åŒçª—å£å¤§å°\n",
    "        print(\"\\nğŸ“Š å¯¦é©— 2: ä¸åŒçª—å£å¤§å°\")\n",
    "        print(\"-\"*70)\n",
    "        self._experiment_window_sizes()\n",
    "        \n",
    "        # å¯¦é©— 3: ç‰¹å¾µé¸æ“‡\n",
    "        print(\"\\nğŸ“Š å¯¦é©— 3: ç‰¹å¾µé¸æ“‡\")\n",
    "        print(\"-\"*70)\n",
    "        self._experiment_feature_selection()\n",
    "        \n",
    "        # ç¸½çµå ±å‘Š\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“‹ å¯¦é©—ç¸½çµå ±å‘Š\")\n",
    "        print(\"=\"*70)\n",
    "        self._generate_summary_report()\n",
    "        \n",
    "        return self.experiments\n",
    "    \n",
    "    def _experiment_class_weights(self):\n",
    "        \"\"\"å¯¦é©— 1: æ¸¬è©¦é¡åˆ¥æ¬Šé‡\"\"\"\n",
    "        # è¼‰å…¥åŸºæº–ç‰¹å¾µ\n",
    "        with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X, y = data['features'], data['labels']\n",
    "        \n",
    "        # === æ–°å¢ï¼šNaN æ¿¾é™¤ï¼ˆé‚è¼¯ä¸è®Šï¼Œåªä¸Ÿæ‰ä¸å®Œæ•´æ¨£æœ¬ï¼‰ ===\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            nan_mask = X.isna().any(axis=1)\n",
    "        else:\n",
    "            nan_mask = np.isnan(X).any(axis=1)\n",
    "        n_nan = nan_mask.sum()\n",
    "        if n_nan > 0:\n",
    "            print(f\"   âš ï¸ å¯¦é©— 1: ç™¼ç¾ {n_nan} ç­†å« NaN æ¨£æœ¬ï¼Œå·²æ’é™¤ã€‚\")\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X = X.loc[~nan_mask].reset_index(drop=True)\n",
    "            else:\n",
    "                X = X[~nan_mask]\n",
    "            if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "                y = y.loc[~nan_mask].reset_index(drop=True)\n",
    "            else:\n",
    "                y = y[~nan_mask]\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # è‡ªå‹•æª¢æ¸¬é¡åˆ¥æ•¸\n",
    "        unique_classes = np.unique(y)\n",
    "        print(f\"   æª¢æ¸¬åˆ°çš„é¡åˆ¥: {unique_classes}\")\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒæ¬Šé‡ç­–ç•¥\n",
    "        weight_strategies = {\n",
    "            'None (åŸºæº–)': None,\n",
    "            'Balanced': 'balanced',\n",
    "            'Custom (å¼·åŒ–Stage1)': {c: (1.3 if c == 1 else 1.0) for c in unique_classes},  # Stage 1 recall è¼ƒä½\n",
    "            'Custom (å¼·åŒ–Stage2)': {c: (1.2 if c == 2 else 1.0) for c in unique_classes}\n",
    "        }\n",
    "        \n",
    "        for name, weights in weight_strategies.items():\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                class_weight=weights,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            # è©³ç´°åˆ†é¡å ±å‘Š\n",
    "            y_pred = cross_val_predict(rf, X_scaled, y, cv=cv)\n",
    "            report = classification_report(y, y_pred, \n",
    "                                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                                          output_dict=True)\n",
    "            \n",
    "            # ç‰¹åˆ¥é—œæ³¨ Stage 1 (recall æœ€ä½)\n",
    "            stage1_recall = report['Stage 1']['recall']\n",
    "            \n",
    "            self.experiments[f'class_weight_{name}'] = {\n",
    "                'accuracy': mean_score,\n",
    "                'stage1_recall': stage1_recall,\n",
    "                'improvement': mean_score - self.baseline_score\n",
    "            }\n",
    "            \n",
    "            improvement_icon = \"ğŸ“ˆ\" if mean_score > self.baseline_score else \"ğŸ“‰\" if mean_score < self.baseline_score else \"â¡ï¸\"\n",
    "            stage1_icon = \"âœ…\" if stage1_recall > 0.901 else \"âš ï¸\"\n",
    "            \n",
    "            print(f\"{improvement_icon} {name:30s}: æº–ç¢ºç‡={mean_score:.3f} ({mean_score-self.baseline_score:+.3f})\")\n",
    "            print(f\"   {stage1_icon} Stage 1 Recall: {stage1_recall:.3f} (åŸºæº–=0.901)\")\n",
    "    \n",
    "    def _experiment_window_sizes(self):\n",
    "        \"\"\"å¯¦é©— 2: æ¸¬è©¦ä¸åŒçª—å£å¤§å°\"\"\"\n",
    "        # æª¢æŸ¥ FeatureEngineering æ˜¯å¦å¯ç”¨\n",
    "        try:\n",
    "            FeatureEngineering\n",
    "        except NameError:\n",
    "            print(\"âš ï¸  FeatureEngineering æœªå®šç¾©ï¼Œè·³éæ­¤å¯¦é©—\")\n",
    "            print(\"   æç¤ºï¼šéœ€è¦é‡æ–°åŸ·è¡Œ Cell 11 æ‰èƒ½æ¸¬è©¦ä¸åŒçª—å£å¤§å°\")\n",
    "            return\n",
    "        \n",
    "        window_sizes = [60, 120, 180, 240]\n",
    "        \n",
    "        for ws in window_sizes:\n",
    "            print(f\"\\nğŸ” æ¸¬è©¦çª—å£å¤§å°: {ws} ç§’\")\n",
    "            \n",
    "            try:\n",
    "                # é‡æ–°æå–ç‰¹å¾µ\n",
    "                feature_engineer = FeatureEngineering(\n",
    "                    arduino_features, \n",
    "                    maturity_levels,\n",
    "                    window_size=ws\n",
    "                )\n",
    "                \n",
    "                X, y, metadata = feature_engineer.extract_all_features()\n",
    "                \n",
    "                # === æ–°å¢ï¼šNaN æ¿¾é™¤ ===\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    nan_mask = X.isna().any(axis=1)\n",
    "                else:\n",
    "                    nan_mask = np.isnan(X).any(axis=1)\n",
    "                n_nan = nan_mask.sum()\n",
    "                if n_nan > 0:\n",
    "                    print(f\"   âš ï¸ çª—å£ {ws}s: ç™¼ç¾ {n_nan} ç­†å« NaN æ¨£æœ¬ï¼Œå·²æ’é™¤ã€‚\")\n",
    "                    if isinstance(X, pd.DataFrame):\n",
    "                        X = X.loc[~nan_mask].reset_index(drop=True)\n",
    "                    else:\n",
    "                        X = X[~nan_mask]\n",
    "                    if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "                        y = y.loc[~nan_mask].reset_index(drop=True)\n",
    "                    else:\n",
    "                        y = y[~nan_mask]\n",
    "                \n",
    "                # æ¨™æº–åŒ–\n",
    "                scaler = StandardScaler()\n",
    "                X_scaled = scaler.fit_transform(X)\n",
    "                \n",
    "                # è¨“ç·´ Random Forest\n",
    "                rf = RandomForestClassifier(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=10,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "                mean_score = scores.mean()\n",
    "                \n",
    "                # è©³ç´°åˆ†æ\n",
    "                y_pred = cross_val_predict(rf, X_scaled, y, cv=cv)\n",
    "                report = classification_report(y, y_pred, \n",
    "                                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                                              output_dict=True)\n",
    "                \n",
    "                self.experiments[f'window_{ws}s'] = {\n",
    "                    'accuracy': mean_score,\n",
    "                    'n_samples': len(y),\n",
    "                    'stage1_recall': report['Stage 1']['recall'],\n",
    "                    'improvement': mean_score - self.baseline_score\n",
    "                }\n",
    "                \n",
    "                improvement_icon = \"ğŸ“ˆ\" if mean_score > self.baseline_score else \"ğŸ“‰\" if mean_score < self.baseline_score else \"â¡ï¸\"\n",
    "                \n",
    "                print(f\"{improvement_icon} çª—å£ {ws}s: æº–ç¢ºç‡={mean_score:.3f} ({mean_score-self.baseline_score:+.3f}), æ¨£æœ¬æ•¸={len(y)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ çª—å£ {ws}s å¤±æ•—: {e}\")\n",
    "                self.experiments[f'window_{ws}s'] = {'accuracy': 0, 'error': str(e)}\n",
    "    \n",
    "    def _experiment_feature_selection(self):\n",
    "        \"\"\"å¯¦é©— 3: æ¸¬è©¦ç‰¹å¾µé¸æ“‡\"\"\"\n",
    "        # è¼‰å…¥åŸºæº–ç‰¹å¾µ\n",
    "        with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X, y = data['features'], data['labels']\n",
    "        \n",
    "        # === æ–°å¢ï¼šNaN æ¿¾é™¤ ===\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            nan_mask = X.isna().any(axis=1)\n",
    "        else:\n",
    "            nan_mask = np.isnan(X).any(axis=1)\n",
    "        n_nan = nan_mask.sum()\n",
    "        if n_nan > 0:\n",
    "            print(f\"   âš ï¸ å¯¦é©— 3: ç™¼ç¾ {n_nan} ç­†å« NaN æ¨£æœ¬ï¼Œå·²æ’é™¤ã€‚\")\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X = X.loc[~nan_mask].reset_index(drop=True)\n",
    "            else:\n",
    "                X = X[~nan_mask]\n",
    "            if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "                y = y.loc[~nan_mask].reset_index(drop=True)\n",
    "            else:\n",
    "                y = y[~nan_mask]\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒæ•¸é‡çš„ç‰¹å¾µ\n",
    "        n_features_list = [10, 20, 30, 40, 53]\n",
    "        \n",
    "        for n_features in n_features_list:\n",
    "            if n_features >= X.shape[1]:\n",
    "                # ä½¿ç”¨å…¨éƒ¨ç‰¹å¾µ\n",
    "                X_selected = X_scaled\n",
    "            else:\n",
    "                # ç‰¹å¾µé¸æ“‡\n",
    "                selector = SelectKBest(f_classif, k=n_features)\n",
    "                X_selected = selector.fit_transform(X_scaled, y)\n",
    "                \n",
    "                # é¡¯ç¤ºé¸ä¸­çš„ç‰¹å¾µ\n",
    "                if n_features == 20:\n",
    "                    selected_features = selector.get_support(indices=True)\n",
    "                    feature_names = data['features'].columns\n",
    "                    print(f\"\\n   Top {n_features} ç‰¹å¾µ:\")\n",
    "                    for i, idx in enumerate(selected_features[:10], 1):\n",
    "                        print(f\"      {i:2d}. {feature_names[idx]}\")\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(rf, X_selected, y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            self.experiments[f'features_{n_features}'] = {\n",
    "                'accuracy': mean_score,\n",
    "                'n_features': n_features,\n",
    "                'improvement': mean_score - self.baseline_score\n",
    "            }\n",
    "            \n",
    "            improvement_icon = \"ğŸ“ˆ\" if mean_score > self.baseline_score else \"ğŸ“‰\" if mean_score < self.baseline_score else \"â¡ï¸\"\n",
    "            \n",
    "            print(f\"{improvement_icon} ä½¿ç”¨ {n_features:2d} å€‹ç‰¹å¾µ: æº–ç¢ºç‡={mean_score:.3f} ({mean_score-self.baseline_score:+.3f})\")\n",
    "    \n",
    "    def _generate_summary_report(self):\n",
    "        \"\"\"ç”Ÿæˆç¸½çµå ±å‘Š\"\"\"\n",
    "        # æ‰¾å‡ºæœ€ä½³æ–¹æ¡ˆ\n",
    "        valid_experiments = {k: v for k, v in self.experiments.items() \n",
    "                            if 'accuracy' in v and v['accuracy'] > 0}\n",
    "        \n",
    "        if not valid_experiments:\n",
    "            print(\"âš ï¸  æ²’æœ‰æœ‰æ•ˆçš„å¯¦é©—çµæœ\")\n",
    "            return\n",
    "        \n",
    "        best_exp = max(valid_experiments.items(), key=lambda x: x[1]['accuracy'])\n",
    "        best_name, best_result = best_exp\n",
    "        \n",
    "        print(f\"\\nğŸ† æœ€ä½³æ–¹æ¡ˆ: {best_name}\")\n",
    "        print(f\"   æº–ç¢ºç‡: {best_result['accuracy']:.3f}\")\n",
    "        print(f\"   æ”¹é€²å¹…åº¦: {best_result['improvement']:+.3f}\")\n",
    "        \n",
    "        if best_result['accuracy'] > self.baseline_score:\n",
    "            improvement_pct = (best_result['improvement'] / self.baseline_score) * 100\n",
    "            print(f\"\\nâœ… æ‰¾åˆ°æ”¹é€²æ–¹æ¡ˆï¼æ¯”åŸºæº–æå‡ {improvement_pct:.2f}%\")\n",
    "            print(f\"   å¾ {self.baseline_score:.3f} â†’ {best_result['accuracy']:.3f}\")\n",
    "        elif best_result['accuracy'] == self.baseline_score:\n",
    "            print(f\"\\nâ¡ï¸  æ”¹é€²æ–¹æ¡ˆèˆ‡åŸºæº–æŒå¹³ï¼Œæ¨¡å‹å·²é”æœ€å„ª\")\n",
    "        else:\n",
    "            print(f\"\\nâ¡ï¸  åŸºæº–æ¨¡å‹ (120sçª—å£ + 53ç‰¹å¾µ) å·²ç¶“æ˜¯æœ€å„ªé…ç½®\")\n",
    "        \n",
    "        # å„²å­˜å ±å‘Š\n",
    "        report_df = pd.DataFrame([\n",
    "            {\n",
    "                'experiment': name,\n",
    "                'accuracy': result.get('accuracy', 0),\n",
    "                'improvement': result.get('improvement', 0),\n",
    "                **{k: v for k, v in result.items() if k not in ['accuracy', 'improvement']}\n",
    "            }\n",
    "            for name, result in self.experiments.items()\n",
    "        ]).sort_values('accuracy', ascending=False)\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'improvement_report.csv')\n",
    "        report_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ’¾ å®Œæ•´å ±å‘Š: {csv_path}\")\n",
    "        \n",
    "        # è¦–è¦ºåŒ–å°æ¯”\n",
    "        self._plot_comparison(report_df)\n",
    "    \n",
    "    def _plot_comparison(self, report_df):\n",
    "        \"\"\"ç¹ªè£½å°æ¯”åœ–\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            experiments = report_df['experiment'].tolist()\n",
    "            accuracies = report_df['accuracy'].tolist()\n",
    "            \n",
    "            colors = ['green' if acc > self.baseline_score else 'red' if acc < self.baseline_score else 'gray' \n",
    "                     for acc in accuracies]\n",
    "            \n",
    "            ax.barh(experiments, accuracies, color=colors, alpha=0.6)\n",
    "            ax.axvline(self.baseline_score, color='blue', linestyle='--', linewidth=2,\n",
    "                      label=f'åŸºæº– ({self.baseline_score:.3f})')\n",
    "            \n",
    "            ax.set_xlabel('æº–ç¢ºç‡', fontsize=12)\n",
    "            ax.set_title('æ¨¡å‹æ”¹é€²å¯¦é©—å°æ¯”ï¼ˆçœŸå¯¦æ¨™ç±¤ï¼‰', fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = os.path.join(self.output_dir, 'improvement_comparison.png')\n",
    "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"ğŸ“Š å°æ¯”åœ–: {plot_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ç„¡æ³•ç¹ªè£½åœ–è¡¨: {e}\")\n",
    "\n",
    "# åŸ·è¡Œæ”¹é€²å¯¦é©—\n",
    "print(\"âœ… æº–å‚™åŸ·è¡Œæ”¹é€²å¯¦é©—...\")\n",
    "print(\"   ç¢ºä¿å·²åŸ·è¡Œï¼š\")\n",
    "print(\"   - arduino_features å·²è¼‰å…¥\")\n",
    "print(\"   - maturity_levels å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´š\")\n",
    "print(\"   - FeatureEngineering å·²å®šç¾©ï¼ˆCell 11ï¼‰\\n\")\n",
    "\n",
    "improver = ModelImprovement()\n",
    "results = improver.run_all_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516887c3-586d-4d40-a617-c5de710620f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹åš´æ ¼é©—è­‰...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ æ¸¬è©¦æ›´å°çª—å£ï¼ˆ30s, 45sï¼‰\n",
      "======================================================================\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 30 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 30 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 330 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 300 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 270 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 269 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 270 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 209 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 239 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 240 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 150 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (2517, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 2517\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(730), np.int64(1): np.int64(589), np.int64(2): np.int64(511), np.int64(3): np.int64(687)}\n",
      "============================================================\n",
      "âœ… çª—å£ 30s (LOSO): æº–ç¢ºç‡=0.641, æ¨£æœ¬æ•¸=2517\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 45 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 45 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 220 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 200 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 180 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 179 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 180 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 139 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 159 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 160 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 100 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 80 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 80 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (1677, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 1677\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(487), np.int64(1): np.int64(393), np.int64(2): np.int64(340), np.int64(3): np.int64(457)}\n",
      "============================================================\n",
      "âœ… çª—å£ 45s (LOSO): æº–ç¢ºç‡=0.632, æ¨£æœ¬æ•¸=1677\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ LOSO åš´æ ¼é©—è­‰ï¼š60ç§’çª—å£\n",
      "======================================================================\n",
      "èªªæ˜: Leave-One-Subject-Out (LOSO)\n",
      "      æ¯æ¬¡ç•™ä¸€å€‹é³³æ¢¨åšæ¸¬è©¦ï¼Œç¢ºä¿æ¨¡å‹èƒ½æ³›åŒ–åˆ°æ–°é³³æ¢¨\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 165 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 150 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 134 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 104 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 119 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (1257, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 1257\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(365), np.int64(1): np.int64(295), np.int64(2): np.int64(255), np.int64(3): np.int64(342)}\n",
      "============================================================\n",
      "   ğŸ Pineapple 01: æ¸¬è©¦æ¨£æœ¬=165, æº–ç¢ºç‡=0.679\n",
      "   ğŸ Pineapple 02: æ¸¬è©¦æ¨£æœ¬=150, æº–ç¢ºç‡=0.747\n",
      "   ğŸ Pineapple 03: æ¸¬è©¦æ¨£æœ¬=135, æº–ç¢ºç‡=0.333\n",
      "   ğŸ Pineapple 04: æ¸¬è©¦æ¨£æœ¬=134, æº–ç¢ºç‡=0.552\n",
      "   ğŸ Pineapple 05: æ¸¬è©¦æ¨£æœ¬=135, æº–ç¢ºç‡=0.756\n",
      "   ğŸ Pineapple 06: æ¸¬è©¦æ¨£æœ¬=104, æº–ç¢ºç‡=0.663\n",
      "   ğŸ Pineapple 07: æ¸¬è©¦æ¨£æœ¬=119, æº–ç¢ºç‡=0.681\n",
      "   ğŸ Pineapple 08: æ¸¬è©¦æ¨£æœ¬=120, æº–ç¢ºç‡=0.692\n",
      "   ğŸ Pineapple 09: æ¸¬è©¦æ¨£æœ¬= 75, æº–ç¢ºç‡=0.520\n",
      "   ğŸ Pineapple 10: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.667\n",
      "   ğŸ Pineapple 11: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.617\n",
      "\n",
      "   è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "        Stage 0      0.626     0.641     0.633       365\n",
      "        Stage 1      0.492     0.512     0.502       295\n",
      "        Stage 2      0.718     0.698     0.708       255\n",
      "        Stage 3      0.704     0.675     0.690       342\n",
      "   \n",
      "       accuracy                          0.632      1257\n",
      "      macro avg      0.635     0.632     0.633      1257\n",
      "   weighted avg      0.634     0.632     0.633      1257\n",
      "   \n",
      "\n",
      "   æ··æ·†çŸ©é™£:\n",
      "   [[234  75   7  49]\n",
      "    [101 151  31  12]\n",
      "    [  0  41 178  36]\n",
      "    [ 39  40  32 231]]\n",
      "\n",
      "======================================================================\n",
      "âœ… LOSO æœ€çµ‚çµæœ\n",
      "======================================================================\n",
      "   çª—å£å¤§å°: 60 ç§’\n",
      "   ç¸½æ¨£æœ¬æ•¸: 1257\n",
      "   ç¸½é³³æ¢¨æ•¸: 11\n",
      "   LOSO æº–ç¢ºç‡: 0.632\n",
      "\n",
      "   âœ… æ­¤æº–ç¢ºç‡ä»£è¡¨ã€Œæ¨¡å‹èƒ½æ³›åŒ–åˆ°å¾æœªè¦‹éçš„é³³æ¢¨ã€çš„èƒ½åŠ›\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•\n",
      "======================================================================\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 165 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 150 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 134 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 104 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 119 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (1257, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 1257\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(365), np.int64(1): np.int64(295), np.int64(2): np.int64(255), np.int64(3): np.int64(342)}\n",
      "============================================================\n",
      "\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "æ–¹æ³•                             æº–ç¢ºç‡        èªªæ˜\n",
      "----------------------------------------------------------------------\n",
      "Stratified 5-Fold (ç›®å‰)         0.997      æ¨™æº–é©—è­‰\n",
      "LOSO (æœ€åš´æ ¼)                     0.632      æ³›åŒ–åˆ°æ–°é³³æ¢¨\n",
      "\n",
      "å·®ç•°: 0.365 (LOSOè¼ƒåš´æ ¼)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ åš´æ ¼é©—è­‰æœ€çµ‚å ±å‘Š\n",
      "======================================================================\n",
      "\n",
      "ğŸ† æœ€ä½³é…ç½®ï¼ˆLOSO é©—è­‰ï¼‰\n",
      "   window_30s\n",
      "   æº–ç¢ºç‡: 0.641\n",
      "   æ¨£æœ¬æ•¸: 2517\n",
      "\n",
      "ğŸ’¾ è©³ç´°çµæœ: models/strict_validation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== åš´æ ¼é©—è­‰ï¼šLOSO + æ›´å°çª—å£æ¸¬è©¦ =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "class StrictValidation:\n",
    "    \"\"\"æ›´åš´æ ¼çš„æ¨¡å‹é©—è­‰\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def test_smaller_windows(self):\n",
    "        \"\"\"æ¸¬è©¦æ›´å°çš„çª—å£\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸ”¬ æ¸¬è©¦æ›´å°çª—å£ï¼ˆ30s, 45sï¼‰\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        window_sizes = [30, 45]\n",
    "        \n",
    "        for ws in window_sizes:\n",
    "            print(f\"\\nğŸ” æ¸¬è©¦çª—å£å¤§å°: {ws} ç§’\")\n",
    "            \n",
    "            try:\n",
    "                # é‡æ–°æå–ç‰¹å¾µ\n",
    "                feature_engineer = FeatureEngineering(\n",
    "                    arduino_features, \n",
    "                    maturity_levels,\n",
    "                    window_size=ws\n",
    "                )\n",
    "                \n",
    "                X, y, metadata = feature_engineer.extract_all_features()\n",
    "                \n",
    "                # æ¨™æº–åŒ–\n",
    "                scaler = StandardScaler()\n",
    "                X_scaled = scaler.fit_transform(X)\n",
    "                \n",
    "                # LOSO é©—è­‰\n",
    "                loso_acc = self._loso_validation(X_scaled, y, metadata, ws)\n",
    "                \n",
    "                self.results[f'window_{ws}s'] = {\n",
    "                    'accuracy': loso_acc,\n",
    "                    'n_samples': len(y),\n",
    "                    'validation': 'LOSO'\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ… çª—å£ {ws}s (LOSO): æº–ç¢ºç‡={loso_acc:.3f}, æ¨£æœ¬æ•¸={len(y)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ çª—å£ {ws}s å¤±æ•—: {e}\")\n",
    "    \n",
    "    def validate_best_model(self, window_size=60):\n",
    "        \"\"\"ç”¨ LOSO åš´æ ¼é©—è­‰æœ€ä½³æ¨¡å‹ï¼ˆ60ç§’çª—å£ï¼‰\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸ”¬ LOSO åš´æ ¼é©—è­‰ï¼š{window_size}ç§’çª—å£\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"èªªæ˜: Leave-One-Subject-Out (LOSO)\")\n",
    "        print(\"      æ¯æ¬¡ç•™ä¸€å€‹é³³æ¢¨åšæ¸¬è©¦ï¼Œç¢ºä¿æ¨¡å‹èƒ½æ³›åŒ–åˆ°æ–°é³³æ¢¨\\n\")\n",
    "        \n",
    "        # é‡æ–°æå–ç‰¹å¾µ\n",
    "        feature_engineer = FeatureEngineering(\n",
    "            arduino_features, \n",
    "            maturity_levels,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        \n",
    "        X, y, metadata = feature_engineer.extract_all_features()\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # LOSO é©—è­‰\n",
    "        loso_acc = self._loso_validation(X_scaled, y, metadata, window_size, verbose=True)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"âœ… LOSO æœ€çµ‚çµæœ\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"   çª—å£å¤§å°: {window_size} ç§’\")\n",
    "        print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "        print(f\"   ç¸½é³³æ¢¨æ•¸: {len(np.unique(metadata['pineapple_id']))}\")\n",
    "        print(f\"   LOSO æº–ç¢ºç‡: {loso_acc:.3f}\")\n",
    "        print(f\"\\n   âœ… æ­¤æº–ç¢ºç‡ä»£è¡¨ã€Œæ¨¡å‹èƒ½æ³›åŒ–åˆ°å¾æœªè¦‹éçš„é³³æ¢¨ã€çš„èƒ½åŠ›\")\n",
    "        \n",
    "        self.results[f'best_model_window_{window_size}s_LOSO'] = {\n",
    "            'accuracy': loso_acc,\n",
    "            'validation': 'LOSO',\n",
    "            'n_samples': len(y)\n",
    "        }\n",
    "        \n",
    "        return loso_acc\n",
    "    \n",
    "    def _loso_validation(self, X, y, metadata, window_size, verbose=False):\n",
    "        \"\"\"åŸ·è¡Œ Leave-One-Subject-Out äº¤å‰é©—è­‰\"\"\"\n",
    "        pineapple_ids = metadata['pineapple_id'].unique()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        for test_pid in pineapple_ids:\n",
    "            # è¨“ç·´é›†ï¼šæ’é™¤ç•¶å‰é³³æ¢¨\n",
    "            train_mask = metadata['pineapple_id'] != test_pid\n",
    "            test_mask = metadata['pineapple_id'] == test_pid\n",
    "            \n",
    "            X_train, y_train = X[train_mask], y[train_mask]\n",
    "            X_test, y_test = X[test_mask], y[test_mask]\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            # é æ¸¬\n",
    "            y_pred = rf.predict(X_test)\n",
    "            \n",
    "            # è¨ˆç®—è©²é³³æ¢¨çš„æº–ç¢ºç‡\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   ğŸ Pineapple {test_pid}: \"\n",
    "                      f\"æ¸¬è©¦æ¨£æœ¬={len(y_test):3d}, \"\n",
    "                      f\"æº–ç¢ºç‡={acc:.3f}\")\n",
    "        \n",
    "        # ç¸½é«”æº–ç¢ºç‡\n",
    "        overall_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n   è©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "            report = classification_report(\n",
    "                all_true, all_preds,\n",
    "                target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                digits=3\n",
    "            )\n",
    "            print(\"   \" + report.replace(\"\\n\", \"\\n   \"))\n",
    "            \n",
    "            print(f\"\\n   æ··æ·†çŸ©é™£:\")\n",
    "            cm = confusion_matrix(all_true, all_preds)\n",
    "            print(\"   \" + str(cm).replace(\"\\n\", \"\\n   \"))\n",
    "        \n",
    "        return overall_acc\n",
    "    \n",
    "    def compare_validation_methods(self, window_size=60):\n",
    "        \"\"\"æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•çš„çµæœ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # è¼‰å…¥ 60ç§’çª—å£çš„ç‰¹å¾µ\n",
    "        feature_engineer = FeatureEngineering(\n",
    "            arduino_features, \n",
    "            maturity_levels,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        X, y, metadata = feature_engineer.extract_all_features()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # 1. Stratified 5-Foldï¼ˆç›®å‰ç”¨çš„ï¼‰\n",
    "        from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        kfold_scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "        kfold_acc = kfold_scores.mean()\n",
    "        \n",
    "        # 2. LOSOï¼ˆæœ€åš´æ ¼ï¼‰\n",
    "        loso_acc = self._loso_validation(X_scaled, y, metadata, window_size)\n",
    "        \n",
    "        print(f\"\\nçª—å£å¤§å°: {window_size} ç§’\\n\")\n",
    "        print(f\"{'æ–¹æ³•':<30s} {'æº–ç¢ºç‡':<10s} {'èªªæ˜'}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Stratified 5-Fold (ç›®å‰)':<30s} {kfold_acc:.3f}      æ¨™æº–é©—è­‰\")\n",
    "        print(f\"{'LOSO (æœ€åš´æ ¼)':<30s} {loso_acc:.3f}      æ³›åŒ–åˆ°æ–°é³³æ¢¨\")\n",
    "        print(f\"\\nå·®ç•°: {abs(kfold_acc - loso_acc):.3f} \"\n",
    "              f\"({'LOSOè¼ƒåš´æ ¼' if loso_acc < kfold_acc else 'çµæœä¸€è‡´'})\")\n",
    "        \n",
    "        return {\n",
    "            'kfold': kfold_acc,\n",
    "            'loso': loso_acc\n",
    "        }\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"ç”Ÿæˆæœ€çµ‚å ±å‘Š\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“‹ åš´æ ¼é©—è­‰æœ€çµ‚å ±å‘Š\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"âš ï¸  æ²’æœ‰é©—è­‰çµæœ\")\n",
    "            return\n",
    "        \n",
    "        # æ‰¾å‡ºæœ€ä½³é…ç½®\n",
    "        best = max(self.results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        best_name, best_result = best\n",
    "        \n",
    "        print(f\"\\nğŸ† æœ€ä½³é…ç½®ï¼ˆLOSO é©—è­‰ï¼‰\")\n",
    "        print(f\"   {best_name}\")\n",
    "        print(f\"   æº–ç¢ºç‡: {best_result['accuracy']:.3f}\")\n",
    "        print(f\"   æ¨£æœ¬æ•¸: {best_result['n_samples']}\")\n",
    "        \n",
    "        # å„²å­˜çµæœ\n",
    "        results_df = pd.DataFrame([\n",
    "            {'config': name, **result}\n",
    "            for name, result in self.results.items()\n",
    "        ]).sort_values('accuracy', ascending=False)\n",
    "        \n",
    "        results_df.to_csv('models/strict_validation_results.csv', \n",
    "                         index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ’¾ è©³ç´°çµæœ: models/strict_validation_results.csv\")\n",
    "\n",
    "# ===== åŸ·è¡Œåš´æ ¼é©—è­‰ =====\n",
    "print(\"ğŸš€ é–‹å§‹åš´æ ¼é©—è­‰...\\n\")\n",
    "\n",
    "validator = StrictValidation()\n",
    "\n",
    "# 1. æ¸¬è©¦æ›´å°çª—å£ï¼ˆ30s, 45sï¼‰\n",
    "validator.test_smaller_windows()\n",
    "\n",
    "# 2. ç”¨ LOSO é©—è­‰æœ€ä½³æ¨¡å‹ï¼ˆ60sï¼‰\n",
    "validator.validate_best_model(window_size=60)\n",
    "\n",
    "# 3. æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•\n",
    "validator.compare_validation_methods(window_size=60)\n",
    "\n",
    "# 4. ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "validator.generate_final_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f45272-3afa-4fe6-9529-7e1d9573707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åŸ·è¡Œæ–¹æ¡ˆ Aï¼šæ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆæ¶ˆé™¤å€‹é«”å·®ç•°ç‰ˆï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "æ”¹é€²ç­–ç•¥:\n",
      "  âœ“ é³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\n",
      "  âœ“ ç›¸å°è®ŠåŒ–ç‰¹å¾µ\n",
      "  âœ“ æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 165 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 150 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 134 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 135 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 104 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 119 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 10...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 11...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (1257, 66)\n",
      "   æ¨™ç±¤æ•¸é‡: 1257\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(365), np.int64(1): np.int64(295), np.int64(2): np.int64(255), np.int64(3): np.int64(342)}\n",
      "============================================================\n",
      "\n",
      "ğŸ’¾ å„²å­˜æ”¹é€²ç‰ˆç‰¹å¾µ...\n",
      "   âœ… CSV: data/processed_normalized\\feature_matrix_normalized.csv\n",
      "   âœ… Labels: data/processed_normalized\\labels_normalized.npy\n",
      "   âœ… Metadata: data/processed_normalized\\metadata_normalized.csv\n",
      "   âœ… Pickle: data/processed_normalized\\feature_data_normalized.pkl\n",
      "\n",
      "âœ… æ”¹é€²ç‰ˆç‰¹å¾µå„²å­˜å®Œæˆï¼\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ”¹é€²ç‰ˆç‰¹å¾µæ‘˜è¦:\n",
      "   ç¸½æ¨£æœ¬æ•¸: 1257\n",
      "   ç‰¹å¾µç¶­åº¦: 66 (åŸæœ¬ 53 â†’ ç¾åœ¨ 66)\n",
      "   æ–°å¢ç‰¹å¾µ: äºŒéšå·®åˆ†ã€ç›¸å°è®ŠåŒ–ã€æ„Ÿæ¸¬å™¨ç›¸é—œæ€§\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ A: æ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹ =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ImprovedFeatureEngineering:\n",
    "    \"\"\"\n",
    "    æ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹\n",
    "    \n",
    "    æ”¹é€²ç­–ç•¥:\n",
    "    1. é³³æ¢¨å…§éƒ¨æ­£è¦åŒ–ï¼ˆZ-score per pineappleï¼‰\n",
    "    2. ç›¸å°è®ŠåŒ–ç‰¹å¾µï¼ˆç›¸å°æ–¼åˆå§‹ç‹€æ…‹ï¼‰\n",
    "    3. æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µ\n",
    "    4. æ¸›å°‘çµ•å°æ•¸å€¼ä¾è³´\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, arduino_features, maturity_labels, window_size=60):\n",
    "        self.arduino_features = arduino_features\n",
    "        self.maturity_labels = maturity_labels\n",
    "        self.window_size = window_size\n",
    "        self.sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "        \n",
    "        self.feature_matrix = None\n",
    "        self.labels = None\n",
    "        self.metadata = None\n",
    "        \n",
    "        self.output_dir = 'data/processed_normalized'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"æ‰¹æ¬¡æå–æ‰€æœ‰ç‰¹å¾µï¼ˆæ”¹é€²ç‰ˆï¼‰\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆæ¶ˆé™¤å€‹é«”å·®ç•°ç‰ˆï¼‰\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"çª—å£å¤§å°: {self.window_size} ç§’\")\n",
    "        print(\"æ”¹é€²ç­–ç•¥:\")\n",
    "        print(\"  âœ“ é³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\")\n",
    "        print(\"  âœ“ ç›¸å°è®ŠåŒ–ç‰¹å¾µ\")\n",
    "        print(\"  âœ“ æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µ\\n\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        for pid in self.arduino_features.keys():\n",
    "            if pid not in self.maturity_labels:\n",
    "                print(f\"âš ï¸  {pid}: æ‰¾ä¸åˆ°å°æ‡‰çš„æˆç†Ÿåº¦æ¨™ç±¤ï¼Œè·³é\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ğŸ è™•ç† Pineapple {pid}...\")\n",
    "            \n",
    "            # åˆä½µè©²é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\n",
    "            combined_df, combined_labels = self._combine_pineapple_data(pid)\n",
    "            \n",
    "            if combined_df is None:\n",
    "                continue\n",
    "            \n",
    "            # ğŸ”¥ é—œéµæ”¹é€²ï¼šé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\n",
    "            combined_df = self._normalize_per_pineapple(combined_df, pid)\n",
    "            \n",
    "            # æ»‘å‹•çª—å£æå–ç‰¹å¾µ\n",
    "            features, labels, metadata = self._sliding_window_extraction(\n",
    "                combined_df, combined_labels, pid\n",
    "            )\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            print(f\"   âœ“ æå– {len(features)} å€‹çª—å£ï¼Œæ¯å€‹ {features.shape[1]} ç¶­ç‰¹å¾µ\\n\")\n",
    "        \n",
    "        # åˆä½µæ‰€æœ‰é³³æ¢¨çš„ç‰¹å¾µ\n",
    "        self.feature_matrix = pd.DataFrame(\n",
    "            np.vstack(all_features),\n",
    "            columns=self._get_feature_names()\n",
    "        )\n",
    "        self.labels = np.hstack(all_labels)\n",
    "        self.metadata = pd.DataFrame(all_metadata)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"âœ… ç‰¹å¾µæå–å®Œæˆï¼\")\n",
    "        print(f\"   ç‰¹å¾µçŸ©é™£: {self.feature_matrix.shape}\")\n",
    "        print(f\"   æ¨™ç±¤æ•¸é‡: {len(self.labels)}\")\n",
    "        print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.feature_matrix, self.labels, self.metadata\n",
    "    \n",
    "    def _normalize_per_pineapple(self, df, pid):\n",
    "        \"\"\"\n",
    "        ğŸ”¥ é—œéµæ”¹é€²ï¼šé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\n",
    "        \n",
    "        å°æ¯é¡†é³³æ¢¨çš„æ„Ÿæ¸¬å™¨æ•¸æ“šåš Z-score æ¨™æº–åŒ–ï¼Œ\n",
    "        æ¶ˆé™¤å€‹é«”åŸºç·šå·®ç•°\n",
    "        \"\"\"\n",
    "        df_normalized = df.copy()\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "            if col_rs_r0 in df.columns:\n",
    "                values = df[col_rs_r0].values\n",
    "                \n",
    "                # Z-score æ¨™æº–åŒ–ï¼ˆè©²é³³æ¢¨å…§éƒ¨ï¼‰\n",
    "                mean = np.mean(values)\n",
    "                std = np.std(values)\n",
    "                \n",
    "                if std > 0:\n",
    "                    df_normalized[col_rs_r0] = (values - mean) / std\n",
    "                else:\n",
    "                    df_normalized[col_rs_r0] = 0\n",
    "        \n",
    "        return df_normalized\n",
    "    \n",
    "    def _combine_pineapple_data(self, pid):\n",
    "        \"\"\"åˆä½µå–®é¡†é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\"\"\"\n",
    "        df_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        date_dict = self.arduino_features[pid]\n",
    "        labels = self.maturity_labels[pid]\n",
    "        \n",
    "        offset = 0\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date].copy()\n",
    "            n_samples = len(df)\n",
    "            \n",
    "            window_labels = labels[offset:offset+n_samples]\n",
    "            \n",
    "            df_list.append(df)\n",
    "            label_list.append(window_labels)\n",
    "            \n",
    "            offset += n_samples\n",
    "        \n",
    "        if not df_list:\n",
    "            return None, None\n",
    "        \n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        combined_labels = np.hstack(label_list)\n",
    "        \n",
    "        return combined_df, combined_labels\n",
    "    \n",
    "    def _sliding_window_extraction(self, df, labels, pid):\n",
    "        \"\"\"æ»‘å‹•çª—å£æå–ç‰¹å¾µ\"\"\"\n",
    "        features = []\n",
    "        window_labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        n_samples = len(df)\n",
    "        step_size = self.window_size\n",
    "        \n",
    "        for start_idx in range(0, n_samples - self.window_size + 1, step_size):\n",
    "            end_idx = start_idx + self.window_size\n",
    "            \n",
    "            window_df = df.iloc[start_idx:end_idx]\n",
    "            window_label_array = labels[start_idx:end_idx]\n",
    "            \n",
    "            # è©²çª—å£çš„ä¸»è¦æ¨™ç±¤\n",
    "            unique, counts = np.unique(window_label_array, return_counts=True)\n",
    "            if len(unique) == 0:\n",
    "                continue\n",
    "            majority_label = unique[np.argmax(counts)]\n",
    "            \n",
    "            # ğŸ”¥ æå–æ”¹é€²ç‰ˆç‰¹å¾µ\n",
    "            feature_vector = self._extract_improved_features(window_df, start_idx, end_idx)\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            window_labels.append(majority_label)\n",
    "            metadata.append({\n",
    "                'pineapple_id': pid,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'majority_label': int(majority_label),\n",
    "                'label_purity': max(counts) / len(window_label_array)\n",
    "            })\n",
    "        \n",
    "        return np.array(features), np.array(window_labels), metadata\n",
    "    \n",
    "    def _extract_improved_features(self, window_df, start_idx, end_idx):\n",
    "        \"\"\"\n",
    "        ğŸ”¥ æ”¹é€²ç‰ˆç‰¹å¾µæå–\n",
    "        \n",
    "        é‡é»ï¼š\n",
    "        1. å·²ç¶“åšéé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–ï¼ˆZ-scoreï¼‰ï¼Œæ‰€ä»¥å‡å€¼/æœ€å€¼æ›´æœ‰æ„ç¾©\n",
    "        2. å¢åŠ æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µï¼ˆä¸€éšã€äºŒéšï¼‰\n",
    "        3. å¢åŠ ç›¸å°è®ŠåŒ–ç‰¹å¾µ\n",
    "        4. æ¸›å°‘è·¨æ„Ÿæ¸¬å™¨æ¯”ä¾‹ï¼ˆå› ç‚ºå·²æ­£è¦åŒ–ï¼Œæ¯”ä¾‹æ„ç¾©é™ä½ï¼‰\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "            col_delta = f'{sensor}_delta_Rs_R0'\n",
    "            \n",
    "            if col_rs_r0 in window_df.columns:\n",
    "                data = window_df[col_rs_r0].values\n",
    "                \n",
    "                # === åŸºæœ¬çµ±è¨ˆç‰¹å¾µï¼ˆæ­£è¦åŒ–å¾Œï¼‰===\n",
    "                features.append(np.mean(data))           # å¹³å‡å€¼\n",
    "                features.append(np.std(data))            # æ¨™æº–å·®\n",
    "                features.append(np.min(data))            # æœ€å°å€¼\n",
    "                features.append(np.max(data))            # æœ€å¤§å€¼\n",
    "                features.append(np.max(data) - np.min(data))  # åæ‡‰å¹…åº¦\n",
    "                \n",
    "                # === æ™‚é–“åºåˆ—ç‰¹å¾µ ===\n",
    "                # æ–œç‡ï¼ˆä¸€éšè®ŠåŒ–è¶¨å‹¢ï¼‰\n",
    "                if len(data) > 1:\n",
    "                    x = np.arange(len(data))\n",
    "                    slope, _, _, _, _ = linregress(x, data)\n",
    "                    features.append(slope)\n",
    "                else:\n",
    "                    features.append(0)\n",
    "                \n",
    "                # äºŒéšå·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼Œè®ŠåŒ–çš„è®ŠåŒ–ï¼‰\n",
    "                if len(data) > 2:\n",
    "                    first_diff = np.diff(data)\n",
    "                    second_diff = np.diff(first_diff)\n",
    "                    features.append(np.mean(second_diff))    # å¹³å‡åŠ é€Ÿåº¦\n",
    "                    features.append(np.std(second_diff))     # åŠ é€Ÿåº¦è®Šç•°\n",
    "                else:\n",
    "                    features.extend([0, 0])\n",
    "                \n",
    "                # AUCï¼ˆç´¯ç©åæ‡‰ï¼‰\n",
    "                try:\n",
    "                    auc = np.trapezoid(data, dx=1)\n",
    "                except AttributeError:\n",
    "                    if len(data) > 1:\n",
    "                        auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                    else:\n",
    "                        auc = data[0] if len(data) > 0 else 0\n",
    "                features.append(auc)\n",
    "                \n",
    "                # === ç›¸å°è®ŠåŒ–ç‰¹å¾µï¼ˆç›¸å°æ–¼çª—å£åˆå§‹å€¼ï¼‰===\n",
    "                if len(data) > 0 and abs(data[0]) > 1e-6:\n",
    "                    relative_change = (data[-1] - data[0]) / (abs(data[0]) + 1e-6)\n",
    "                    features.append(relative_change)\n",
    "                else:\n",
    "                    features.append(0)\n",
    "            else:\n",
    "                features.extend([0] * 10)  # 10 å€‹ç‰¹å¾µ\n",
    "            \n",
    "            # === Delta ç‰¹å¾µï¼ˆç¬æ™‚è®ŠåŒ–ï¼‰===\n",
    "            if col_delta in window_df.columns:\n",
    "                data = window_df[col_delta].values\n",
    "                \n",
    "                features.append(np.mean(data))\n",
    "                features.append(np.std(data))\n",
    "                features.append(np.max(np.abs(data)))\n",
    "            else:\n",
    "                features.extend([0] * 3)\n",
    "        \n",
    "        # === è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µï¼ˆæ¸›å°‘ï¼Œå› ç‚ºæ­£è¦åŒ–å¾Œæ¯”ä¾‹æ„ç¾©é™ä½ï¼‰===\n",
    "        # åªä¿ç•™é—œéµçš„å”æ–¹å·®ç‰¹å¾µ\n",
    "        mq3_mean = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0\n",
    "        mq135_mean = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0\n",
    "        \n",
    "        # MQ3 vs MQ135 çš„å”åŒè®ŠåŒ–ï¼ˆç›¸é—œæ€§ï¼‰\n",
    "        if 'MQ3_Rs_R0' in window_df.columns and 'MQ135_Rs_R0' in window_df.columns:\n",
    "            if len(window_df) > 1:\n",
    "                corr = np.corrcoef(window_df['MQ3_Rs_R0'], window_df['MQ135_Rs_R0'])[0, 1]\n",
    "                if np.isnan(corr):\n",
    "                    corr = 0\n",
    "            else:\n",
    "                corr = 0\n",
    "        else:\n",
    "            corr = 0\n",
    "        features.append(corr)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _get_feature_names(self):\n",
    "        \"\"\"ç”Ÿæˆç‰¹å¾µåç¨±\"\"\"\n",
    "        names = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            # åŸºæœ¬ç‰¹å¾µï¼ˆ10å€‹ï¼‰\n",
    "            names.extend([\n",
    "                f'{sensor}_mean_norm',           # æ­£è¦åŒ–å¾Œçš„å‡å€¼\n",
    "                f'{sensor}_std_norm',\n",
    "                f'{sensor}_min_norm',\n",
    "                f'{sensor}_max_norm',\n",
    "                f'{sensor}_range_norm',\n",
    "                f'{sensor}_slope',\n",
    "                f'{sensor}_accel_mean',          # äºŒéšå·®åˆ†\n",
    "                f'{sensor}_accel_std',\n",
    "                f'{sensor}_auc_norm',\n",
    "                f'{sensor}_relative_change'      # ç›¸å°è®ŠåŒ–\n",
    "            ])\n",
    "            \n",
    "            # Delta ç‰¹å¾µï¼ˆ3å€‹ï¼‰\n",
    "            names.extend([\n",
    "                f'{sensor}_delta_mean',\n",
    "                f'{sensor}_delta_std',\n",
    "                f'{sensor}_delta_max_abs'\n",
    "            ])\n",
    "        \n",
    "        # è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µï¼ˆ1å€‹ï¼‰\n",
    "        names.append('MQ3_MQ135_correlation')\n",
    "        \n",
    "        return names\n",
    "    \n",
    "    def save_features(self):\n",
    "        \"\"\"å„²å­˜ç‰¹å¾µèˆ‡æ¨™ç±¤\"\"\"\n",
    "        print(\"\\nğŸ’¾ å„²å­˜æ”¹é€²ç‰ˆç‰¹å¾µ...\")\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'feature_matrix_normalized.csv')\n",
    "        self.feature_matrix.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… CSV: {csv_path}\")\n",
    "        \n",
    "        label_path = os.path.join(self.output_dir, 'labels_normalized.npy')\n",
    "        np.save(label_path, self.labels)\n",
    "        print(f\"   âœ… Labels: {label_path}\")\n",
    "        \n",
    "        meta_path = os.path.join(self.output_dir, 'metadata_normalized.csv')\n",
    "        self.metadata.to_csv(meta_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… Metadata: {meta_path}\")\n",
    "        \n",
    "        pkl_path = os.path.join(self.output_dir, 'feature_data_normalized.pkl')\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'features': self.feature_matrix,\n",
    "                'labels': self.labels,\n",
    "                'metadata': self.metadata,\n",
    "                'feature_names': self.feature_matrix.columns.tolist(),\n",
    "                'window_size': self.window_size\n",
    "            }, f)\n",
    "        print(f\"   âœ… Pickle: {pkl_path}\")\n",
    "        \n",
    "        print(\"\\nâœ… æ”¹é€²ç‰ˆç‰¹å¾µå„²å­˜å®Œæˆï¼\")\n",
    "\n",
    "# ===== åŸ·è¡Œæ”¹é€²ç‰ˆç‰¹å¾µå·¥ç¨‹ =====\n",
    "print(\"ğŸš€ åŸ·è¡Œæ–¹æ¡ˆ Aï¼šæ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹\\n\")\n",
    "\n",
    "improved_engineer = ImprovedFeatureEngineering(\n",
    "    arduino_features, \n",
    "    maturity_levels,\n",
    "    window_size=60  # å…ˆç”¨ 60 ç§’æ¸¬è©¦\n",
    ")\n",
    "\n",
    "X_improved, y_improved, metadata_improved = improved_engineer.extract_all_features()\n",
    "improved_engineer.save_features()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ”¹é€²ç‰ˆç‰¹å¾µæ‘˜è¦:\")\n",
    "print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(y_improved)}\")\n",
    "print(f\"   ç‰¹å¾µç¶­åº¦: {X_improved.shape[1]} (åŸæœ¬ 53 â†’ ç¾åœ¨ 66)\")\n",
    "print(f\"   æ–°å¢ç‰¹å¾µ: äºŒéšå·®åˆ†ã€ç›¸å°è®ŠåŒ–ã€æ„Ÿæ¸¬å™¨ç›¸é—œæ€§\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dbaef3c-551d-4894-93f1-f0b7571e05e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ LOSO é©—è­‰æ”¹é€²ç‰ˆç‰¹å¾µ...\n",
      "============================================================\n",
      "\n",
      "é€å€‹é³³æ¢¨æ¸¬è©¦ï¼ˆLOSOï¼‰:\n",
      "   ğŸ Pineapple 01: æ¸¬è©¦æ¨£æœ¬=165, æº–ç¢ºç‡=0.436\n",
      "   ğŸ Pineapple 02: æ¸¬è©¦æ¨£æœ¬=150, æº–ç¢ºç‡=0.607\n",
      "   ğŸ Pineapple 03: æ¸¬è©¦æ¨£æœ¬=135, æº–ç¢ºç‡=0.637\n",
      "   ğŸ Pineapple 04: æ¸¬è©¦æ¨£æœ¬=134, æº–ç¢ºç‡=0.619\n",
      "   ğŸ Pineapple 05: æ¸¬è©¦æ¨£æœ¬=135, æº–ç¢ºç‡=0.526\n",
      "   ğŸ Pineapple 06: æ¸¬è©¦æ¨£æœ¬=104, æº–ç¢ºç‡=0.490\n",
      "   ğŸ Pineapple 07: æ¸¬è©¦æ¨£æœ¬=119, æº–ç¢ºç‡=0.613\n",
      "   ğŸ Pineapple 08: æ¸¬è©¦æ¨£æœ¬=120, æº–ç¢ºç‡=0.617\n",
      "   ğŸ Pineapple 09: æ¸¬è©¦æ¨£æœ¬= 75, æº–ç¢ºç‡=0.200\n",
      "   ğŸ Pineapple 10: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.467\n",
      "   ğŸ Pineapple 11: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.467\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ”¹é€²å‰ vs æ”¹é€²å¾Œå°æ¯”\n",
      "============================================================\n",
      "æ”¹é€²å‰ (åŸå§‹ç‰¹å¾µ):    LOSO = 0.598 (59.8%)\n",
      "æ”¹é€²å¾Œ (æ­£è¦åŒ–ç‰¹å¾µ):  LOSO = 0.535 (53.5%)\n",
      "æ”¹é€²å¹…åº¦:             -6.3%\n",
      "============================================================\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.523     0.647     0.578       365\n",
      "     Stage 1      0.510     0.451     0.478       295\n",
      "     Stage 2      0.473     0.380     0.422       255\n",
      "     Stage 3      0.606     0.602     0.604       342\n",
      "\n",
      "    accuracy                          0.535      1257\n",
      "   macro avg      0.528     0.520     0.521      1257\n",
      "weighted avg      0.532     0.535     0.530      1257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== ç«‹å³ç”¨ LOSO æ¸¬è©¦æ”¹é€²æ•ˆæœ =====\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\nğŸ”¬ LOSO é©—è­‰æ”¹é€²ç‰ˆç‰¹å¾µ...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ”¹é€²ç‰ˆç‰¹å¾µ\n",
    "X = X_improved.values if hasattr(X_improved, 'values') else X_improved\n",
    "y = y_improved\n",
    "\n",
    "# æ¨™æº–åŒ–ï¼ˆå…¨å±€ï¼Œå› ç‚ºå·²ç¶“åšéé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–ï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# LOSO é©—è­‰\n",
    "pineapple_ids = metadata_improved['pineapple_id'].unique()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "print(\"\\né€å€‹é³³æ¢¨æ¸¬è©¦ï¼ˆLOSOï¼‰:\")\n",
    "for test_pid in pineapple_ids:\n",
    "    train_mask = metadata_improved['pineapple_id'] != test_pid\n",
    "    test_mask = metadata_improved['pineapple_id'] == test_pid\n",
    "    \n",
    "    X_train, y_train = X_scaled[train_mask], y[train_mask]\n",
    "    X_test, y_test = X_scaled[test_mask], y[test_mask]\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = rf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test)\n",
    "    \n",
    "    print(f\"   ğŸ Pineapple {test_pid}: æ¸¬è©¦æ¨£æœ¬={len(y_test):3d}, æº–ç¢ºç‡={acc:.3f}\")\n",
    "\n",
    "# ç¸½é«”çµæœ\n",
    "loso_acc_improved = accuracy_score(all_true, all_preds)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š æ”¹é€²å‰ vs æ”¹é€²å¾Œå°æ¯”\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"æ”¹é€²å‰ (åŸå§‹ç‰¹å¾µ):    LOSO = 0.598 (59.8%)\")\n",
    "print(f\"æ”¹é€²å¾Œ (æ­£è¦åŒ–ç‰¹å¾µ):  LOSO = {loso_acc_improved:.3f} ({loso_acc_improved*100:.1f}%)\")\n",
    "print(f\"æ”¹é€²å¹…åº¦:             {(loso_acc_improved - 0.598)*100:+.1f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# è©³ç´°å ±å‘Š\n",
    "print(\"\\nè©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "report = classification_report(\n",
    "    all_true, all_preds,\n",
    "    target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "    digits=3\n",
    ")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed5ce4e-353e-491c-a6dc-5c961c01458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹åŸ·è¡Œæ–¹æ¡ˆ C\n",
      "\n",
      "\n",
      "ã€æ–¹æ³• 1ã€‘å–®å€‹æœ€ç›¸ä¼¼é³³æ¢¨æ¨¡å‹\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "ğŸ”¬ æ–¹æ¡ˆ C: Domain Adaptation (å¤šä»»å‹™å­¸ç¿’)\n",
      "============================================================\n",
      "ç­–ç•¥: ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹\n",
      "\n",
      "è¨“ç·´èˆ‡æ¸¬è©¦:\n",
      "   ğŸ æ¸¬è©¦ 01: æ¨£æœ¬= 82, æœ€ç›¸ä¼¼=02, è·é›¢=0.79, æº–ç¢ºç‡=0.732\n",
      "   ğŸ æ¸¬è©¦ 02: æ¨£æœ¬= 75, æœ€ç›¸ä¼¼=01, è·é›¢=0.79, æº–ç¢ºç‡=0.800\n",
      "   ğŸ æ¸¬è©¦ 03: æ¨£æœ¬= 67, æœ€ç›¸ä¼¼=04, è·é›¢=1.24, æº–ç¢ºç‡=0.597\n",
      "   ğŸ æ¸¬è©¦ 04: æ¨£æœ¬= 67, æœ€ç›¸ä¼¼=10, è·é›¢=0.92, æº–ç¢ºç‡=0.284\n",
      "   ğŸ æ¸¬è©¦ 05: æ¨£æœ¬= 67, æœ€ç›¸ä¼¼=06, è·é›¢=0.90, æº–ç¢ºç‡=0.537\n",
      "   ğŸ æ¸¬è©¦ 06: æ¨£æœ¬= 52, æœ€ç›¸ä¼¼=05, è·é›¢=0.90, æº–ç¢ºç‡=0.519\n",
      "   ğŸ æ¸¬è©¦ 07: æ¨£æœ¬= 59, æœ€ç›¸ä¼¼=08, è·é›¢=3.60, æº–ç¢ºç‡=0.864\n",
      "   ğŸ æ¸¬è©¦ 08: æ¨£æœ¬= 60, æœ€ç›¸ä¼¼=07, è·é›¢=3.60, æº–ç¢ºç‡=0.883\n",
      "   ğŸ æ¸¬è©¦ 09: æ¨£æœ¬= 37, æœ€ç›¸ä¼¼=01, è·é›¢=0.96, æº–ç¢ºç‡=0.000\n",
      "   ğŸ æ¸¬è©¦ 10: æ¨£æœ¬= 30, æœ€ç›¸ä¼¼=11, è·é›¢=0.49, æº–ç¢ºç‡=0.367\n",
      "   ğŸ æ¸¬è©¦ 11: æ¨£æœ¬= 30, æœ€ç›¸ä¼¼=10, è·é›¢=0.49, æº–ç¢ºç‡=0.500\n",
      "\n",
      "============================================================\n",
      "âœ… Domain Adaptation çµæœ\n",
      "============================================================\n",
      "LOSO æº–ç¢ºç‡: 0.594 (59.4%)\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.549     0.575     0.562       127\n",
      "     Stage 1      0.388     0.382     0.385       123\n",
      "     Stage 2      0.368     0.389     0.378       108\n",
      "     Stage 3      0.814     0.784     0.798       268\n",
      "\n",
      "    accuracy                          0.594       626\n",
      "   macro avg      0.530     0.532     0.531       626\n",
      "weighted avg      0.600     0.594     0.597       626\n",
      "\n",
      "\n",
      "\n",
      "ã€æ–¹æ³• 2ã€‘Top-3 ç›¸ä¼¼é³³æ¢¨é›†æˆ\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "ğŸ”¬ æ–¹æ¡ˆ C åŠ å¼·ç‰ˆ: é›†æˆ Top-3 ç›¸ä¼¼é³³æ¢¨\n",
      "============================================================\n",
      "è¨“ç·´èˆ‡æ¸¬è©¦:\n",
      "   ğŸ æ¸¬è©¦ 01: æ¨£æœ¬= 82, Top-3=[02(0.8), 09(1.0), 11(6.7)], æº–ç¢ºç‡=0.183\n",
      "   ğŸ æ¸¬è©¦ 02: æ¨£æœ¬= 75, Top-3=[01(0.8), 09(1.1), 11(6.8)], æº–ç¢ºç‡=0.200\n",
      "   ğŸ æ¸¬è©¦ 03: æ¨£æœ¬= 67, Top-3=[04(1.2), 10(1.6), 11(1.9)], æº–ç¢ºç‡=0.179\n",
      "   ğŸ æ¸¬è©¦ 04: æ¨£æœ¬= 67, Top-3=[10(0.9), 11(1.1), 03(1.2)], æº–ç¢ºç‡=0.284\n",
      "   ğŸ æ¸¬è©¦ 05: æ¨£æœ¬= 67, Top-3=[06(0.9), 08(6.9), 07(8.1)], æº–ç¢ºç‡=0.119\n",
      "   ğŸ æ¸¬è©¦ 06: æ¨£æœ¬= 52, Top-3=[05(0.9), 08(6.9), 11(8.1)], æº–ç¢ºç‡=0.346\n",
      "   ğŸ æ¸¬è©¦ 07: æ¨£æœ¬= 59, Top-3=[08(3.6), 03(7.4), 10(7.8)], æº–ç¢ºç‡=0.847\n",
      "   ğŸ æ¸¬è©¦ 08: æ¨£æœ¬= 60, Top-3=[07(3.6), 03(5.7), 10(6.1)], æº–ç¢ºç‡=0.883\n",
      "   ğŸ æ¸¬è©¦ 09: æ¨£æœ¬= 37, Top-3=[01(1.0), 02(1.1), 11(7.0)], æº–ç¢ºç‡=0.000\n",
      "   ğŸ æ¸¬è©¦ 10: æ¨£æœ¬= 30, Top-3=[11(0.5), 04(0.9), 03(1.6)], æº–ç¢ºç‡=0.000\n",
      "   ğŸ æ¸¬è©¦ 11: æ¨£æœ¬= 30, Top-3=[10(0.5), 04(1.1), 03(1.9)], æº–ç¢ºç‡=0.000\n",
      "\n",
      "============================================================\n",
      "âœ… é›†æˆ Domain Adaptation çµæœ\n",
      "============================================================\n",
      "LOSO æº–ç¢ºç‡: 0.304 (30.4%)\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.261     0.606     0.365       127\n",
      "     Stage 1      0.227     0.163     0.190       123\n",
      "     Stage 2      0.063     0.046     0.053       108\n",
      "     Stage 3      0.537     0.328     0.407       268\n",
      "\n",
      "    accuracy                          0.304       626\n",
      "   macro avg      0.272     0.286     0.254       626\n",
      "weighted avg      0.338     0.304     0.295       626\n",
      "\n",
      "\n",
      "\n",
      "ã€æ–¹æ³• 3ã€‘Top-5 ç›¸ä¼¼é³³æ¢¨é›†æˆ\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "ğŸ”¬ æ–¹æ¡ˆ C åŠ å¼·ç‰ˆ: é›†æˆ Top-5 ç›¸ä¼¼é³³æ¢¨\n",
      "============================================================\n",
      "è¨“ç·´èˆ‡æ¸¬è©¦:\n",
      "   ğŸ æ¸¬è©¦ 01: æ¨£æœ¬= 82, Top-5=[02(0.8), 09(1.0), 11(6.7), 10(6.9), 04(7.3)], æº–ç¢ºç‡=0.183\n",
      "   ğŸ æ¸¬è©¦ 02: æ¨£æœ¬= 75, Top-5=[01(0.8), 09(1.1), 11(6.8), 10(6.9), 04(7.4)], æº–ç¢ºç‡=0.200\n",
      "   ğŸ æ¸¬è©¦ 03: æ¨£æœ¬= 67, Top-5=[04(1.2), 10(1.6), 11(1.9), 08(5.7), 07(7.4)], æº–ç¢ºç‡=0.478\n",
      "   ğŸ æ¸¬è©¦ 04: æ¨£æœ¬= 67, Top-5=[10(0.9), 11(1.1), 03(1.2), 08(6.1), 01(7.3)], æº–ç¢ºç‡=0.343\n",
      "   ğŸ æ¸¬è©¦ 05: æ¨£æœ¬= 67, Top-5=[06(0.9), 08(6.9), 07(8.1), 11(8.1), 10(8.3)], æº–ç¢ºç‡=0.134\n",
      "   ğŸ æ¸¬è©¦ 06: æ¨£æœ¬= 52, Top-5=[05(0.9), 08(6.9), 11(8.1), 07(8.2), 10(8.2)], æº–ç¢ºç‡=0.346\n",
      "   ğŸ æ¸¬è©¦ 07: æ¨£æœ¬= 59, Top-5=[08(3.6), 03(7.4), 10(7.8), 04(7.9), 11(7.9)], æº–ç¢ºç‡=0.695\n",
      "   ğŸ æ¸¬è©¦ 08: æ¨£æœ¬= 60, Top-5=[07(3.6), 03(5.7), 10(6.1), 04(6.1), 11(6.2)], æº–ç¢ºç‡=0.833\n",
      "   ğŸ æ¸¬è©¦ 09: æ¨£æœ¬= 37, Top-5=[01(1.0), 02(1.1), 11(7.0), 10(7.1), 04(7.6)], æº–ç¢ºç‡=0.000\n",
      "   ğŸ æ¸¬è©¦ 10: æ¨£æœ¬= 30, Top-5=[11(0.5), 04(0.9), 03(1.6), 08(6.1), 01(6.9)], æº–ç¢ºç‡=0.000\n",
      "   ğŸ æ¸¬è©¦ 11: æ¨£æœ¬= 30, Top-5=[10(0.5), 04(1.1), 03(1.9), 08(6.2), 01(6.7)], æº–ç¢ºç‡=0.000\n",
      "\n",
      "============================================================\n",
      "âœ… é›†æˆ Domain Adaptation çµæœ\n",
      "============================================================\n",
      "LOSO æº–ç¢ºç‡: 0.324 (32.4%)\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.235     0.504     0.321       127\n",
      "     Stage 1      0.152     0.041     0.064       123\n",
      "     Stage 2      0.207     0.231     0.218       108\n",
      "     Stage 3      0.545     0.407     0.466       268\n",
      "\n",
      "    accuracy                          0.324       626\n",
      "   macro avg      0.285     0.296     0.267       626\n",
      "weighted avg      0.346     0.324     0.315       626\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆç¸½çµå°æ¯”\n",
      "============================================================\n",
      "åŸå§‹æ–¹æ³•ï¼ˆå…¨å±€æ¨¡å‹ï¼‰:        LOSO = 0.598 (59.8%)\n",
      "æ–¹æ¡ˆ Aï¼ˆæ­£è¦åŒ–ç‰¹å¾µï¼‰:        LOSO = 0.551 (55.1%)\n",
      "æ–¹æ¡ˆ C-1ï¼ˆå–®å€‹ç›¸ä¼¼ï¼‰:        LOSO = 0.594 (59.4%)\n",
      "æ–¹æ¡ˆ C-2ï¼ˆTop-3 é›†æˆï¼‰:      LOSO = 0.304 (30.4%)\n",
      "æ–¹æ¡ˆ C-3ï¼ˆTop-5 é›†æˆï¼‰:      LOSO = 0.324 (32.4%)\n",
      "============================================================\n",
      "\n",
      "âš ï¸  æ–¹æ¡ˆ C æ•ˆæœæœ‰é™ï¼Œå»ºè­°æ”¶é›†æ›´å¤šé³³æ¢¨æ•¸æ“š\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ C: Domain Adaptationï¼ˆå¤šä»»å‹™å­¸ç¿’ç‰ˆï¼‰=====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pickle\n",
    "\n",
    "class DomainAdaptationModel:\n",
    "    \"\"\"\n",
    "    Domain Adaptation æ¨¡å‹\n",
    "    \n",
    "    ç­–ç•¥ï¼š\n",
    "    1. ç‚ºæ¯é¡†é³³æ¢¨è¨“ç·´ä¸€å€‹å°ˆå±¬æ¨¡å‹\n",
    "    2. æ¸¬è©¦æ™‚ï¼Œæ‰¾å‡ºæœ€ç›¸ä¼¼çš„è¨“ç·´é³³æ¢¨\n",
    "    3. ç”¨è©²é³³æ¢¨çš„æ¨¡å‹ä¾†é æ¸¬\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pineapple_models = {}      # æ¯é¡†é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n",
    "        self.pineapple_centroids = {}   # æ¯é¡†é³³æ¢¨çš„ç‰¹å¾µä¸­å¿ƒé»\n",
    "        self.global_scaler = StandardScaler()\n",
    "    \n",
    "    def train_loso(self, X, y, metadata):\n",
    "        \"\"\"\n",
    "        LOSO è¨“ç·´ï¼šæ¯æ¬¡ç•™ä¸€é¡†é³³æ¢¨æ¸¬è©¦\n",
    "        åŒæ™‚ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”¬ æ–¹æ¡ˆ C: Domain Adaptation (å¤šä»»å‹™å­¸ç¿’)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"ç­–ç•¥: ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹\\n\")\n",
    "        \n",
    "        # ===== å…ˆéæ¿¾æ‰å« NaN / inf çš„æ¨£æœ¬ï¼ˆXã€yã€metadata åŒæ­¥ï¼‰=====\n",
    "        X_arr = np.asarray(X)\n",
    "        y_arr = np.asarray(y)\n",
    "\n",
    "        # æ¯ä¸€åˆ—ç‰¹å¾µéƒ½å¿…é ˆæ˜¯æœ‰é™å€¼ï¼ˆä¸æ˜¯ NaN / infï¼‰ï¼Œæ¨™ç±¤ä¹Ÿæ˜¯\n",
    "        valid_mask = np.isfinite(X_arr).all(axis=1) & np.isfinite(y_arr)\n",
    "        n_invalid = (~valid_mask).sum()\n",
    "        if n_invalid > 0:\n",
    "            print(f\"âš ï¸  ç™¼ç¾ {n_invalid} ç­†å« NaN/inf çš„æ¨£æœ¬ï¼Œå·²å¾æ–¹æ¡ˆ C è¨“ç·´è³‡æ–™ä¸­æ’é™¤ã€‚\")\n",
    "            X_arr = X_arr[valid_mask]\n",
    "            y_arr = y_arr[valid_mask]\n",
    "            if isinstance(metadata, pd.DataFrame):\n",
    "                metadata = metadata.loc[valid_mask].reset_index(drop=True)\n",
    "            else:\n",
    "                metadata = metadata[valid_mask]\n",
    "\n",
    "        # å…¨å±€æ¨™æº–åŒ–ï¼ˆç”¨æ¿¾éå¾Œçš„ Xï¼‰\n",
    "        X_scaled = self.global_scaler.fit_transform(X_arr)\n",
    "        \n",
    "        pineapple_ids = metadata['pineapple_id'].unique()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        print(\"è¨“ç·´èˆ‡æ¸¬è©¦:\")\n",
    "        for test_pid in pineapple_ids:\n",
    "            train_mask = metadata['pineapple_id'] != test_pid\n",
    "            test_mask = metadata['pineapple_id'] == test_pid\n",
    "            \n",
    "            X_train, y_train = X_scaled[train_mask], y_arr[train_mask]\n",
    "            X_test, y_test = X_scaled[test_mask], y_arr[test_mask]\n",
    "            \n",
    "            train_pids = metadata.loc[train_mask, 'pineapple_id'].unique()\n",
    "            \n",
    "            # === ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹ ===\n",
    "            pineapple_specific_models = {}\n",
    "            pineapple_specific_centroids = {}\n",
    "            \n",
    "            for train_pid in train_pids:\n",
    "                pid_mask = metadata['pineapple_id'] == train_pid\n",
    "                X_pid = X_scaled[pid_mask]\n",
    "                y_pid = y_arr[pid_mask]\n",
    "                \n",
    "                # è¨“ç·´è©²é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n",
    "                rf_pid = RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf_pid.fit(X_pid, y_pid)\n",
    "                \n",
    "                pineapple_specific_models[train_pid] = rf_pid\n",
    "                pineapple_specific_centroids[train_pid] = np.mean(X_pid, axis=0)\n",
    "            \n",
    "            # === æ¸¬è©¦ï¼šæ‰¾æœ€ç›¸ä¼¼çš„é³³æ¢¨æ¨¡å‹ ===\n",
    "            test_centroid = np.mean(X_test, axis=0)\n",
    "            \n",
    "            # è¨ˆç®—èˆ‡æ¯é¡†è¨“ç·´é³³æ¢¨çš„è·é›¢\n",
    "            distances = {\n",
    "                pid: euclidean(test_centroid, centroid)\n",
    "                for pid, centroid in pineapple_specific_centroids.items()\n",
    "            }\n",
    "            \n",
    "            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„é³³æ¢¨\n",
    "            most_similar_pid = min(distances, key=distances.get)\n",
    "            \n",
    "            # ç”¨æœ€ç›¸ä¼¼é³³æ¢¨çš„æ¨¡å‹ä¾†é æ¸¬\n",
    "            best_model = pineapple_specific_models[most_similar_pid]\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "            \n",
    "            print(f\"   ğŸ æ¸¬è©¦ {test_pid}: \"\n",
    "                  f\"æ¨£æœ¬={len(y_test):3d}, \"\n",
    "                  f\"æœ€ç›¸ä¼¼={most_similar_pid}, \"\n",
    "                  f\"è·é›¢={distances[most_similar_pid]:.2f}, \"\n",
    "                  f\"æº–ç¢ºç‡={acc:.3f}\")\n",
    "        \n",
    "        overall_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ… Domain Adaptation çµæœ\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"LOSO æº–ç¢ºç‡: {overall_acc:.3f} ({overall_acc*100:.1f}%)\")\n",
    "        \n",
    "        # è©³ç´°å ±å‘Š\n",
    "        print(f\"\\nè©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "        report = classification_report(\n",
    "            all_true, all_preds,\n",
    "            target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "            digits=3\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        return overall_acc, all_preds, all_true\n",
    "\n",
    "\n",
    "class EnsembleAdaptationModel:\n",
    "    \"\"\"\n",
    "    æ–¹æ¡ˆ C åŠ å¼·ç‰ˆï¼šé›†æˆå¤šå€‹ç›¸ä¼¼é³³æ¢¨çš„æ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_similar=3):\n",
    "        self.n_similar = n_similar  # ä½¿ç”¨æœ€ç›¸ä¼¼çš„ N é¡†é³³æ¢¨\n",
    "        self.global_scaler = StandardScaler()\n",
    "    \n",
    "    def train_loso(self, X, y, metadata):\n",
    "        \"\"\"\n",
    "        LOSO è¨“ç·´ï¼šé›†æˆå¤šå€‹ç›¸ä¼¼é³³æ¢¨çš„æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ğŸ”¬ æ–¹æ¡ˆ C åŠ å¼·ç‰ˆ: é›†æˆ Top-{self.n_similar} ç›¸ä¼¼é³³æ¢¨\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ===== å…ˆéæ¿¾æ‰å« NaN / inf çš„æ¨£æœ¬ =====\n",
    "        X_arr = np.asarray(X)\n",
    "        y_arr = np.asarray(y)\n",
    "\n",
    "        valid_mask = np.isfinite(X_arr).all(axis=1) & np.isfinite(y_arr)\n",
    "        n_invalid = (~valid_mask).sum()\n",
    "        if n_invalid > 0:\n",
    "            print(f\"âš ï¸  ç™¼ç¾ {n_invalid} ç­†å« NaN/inf çš„æ¨£æœ¬ï¼Œå·²å¾æ–¹æ¡ˆ C é›†æˆè¨“ç·´è³‡æ–™ä¸­æ’é™¤ã€‚\")\n",
    "            X_arr = X_arr[valid_mask]\n",
    "            y_arr = y_arr[valid_mask]\n",
    "            if isinstance(metadata, pd.DataFrame):\n",
    "                metadata = metadata.loc[valid_mask].reset_index(drop=True)\n",
    "            else:\n",
    "                metadata = metadata[valid_mask]\n",
    "        \n",
    "        X_scaled = self.global_scaler.fit_transform(X_arr)\n",
    "        \n",
    "        pineapple_ids = metadata['pineapple_id'].unique()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        print(\"è¨“ç·´èˆ‡æ¸¬è©¦:\")\n",
    "        for test_pid in pineapple_ids:\n",
    "            train_mask = metadata['pineapple_id'] != test_pid\n",
    "            test_mask = metadata['pineapple_id'] == test_pid\n",
    "            \n",
    "            X_train, y_train = X_scaled[train_mask], y_arr[train_mask]\n",
    "            X_test, y_test = X_scaled[test_mask], y_arr[test_mask]\n",
    "            \n",
    "            train_pids = metadata.loc[train_mask, 'pineapple_id'].unique()\n",
    "            \n",
    "            # ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹æ¨¡å‹å’Œä¸­å¿ƒé»\n",
    "            pineapple_models = {}\n",
    "            pineapple_centroids = {}\n",
    "            \n",
    "            for train_pid in train_pids:\n",
    "                pid_mask = metadata['pineapple_id'] == train_pid\n",
    "                X_pid = X_scaled[pid_mask]\n",
    "                y_pid = y_arr[pid_mask]\n",
    "                \n",
    "                rf_pid = RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf_pid.fit(X_pid, y_pid)\n",
    "                \n",
    "                pineapple_models[train_pid] = rf_pid\n",
    "                pineapple_centroids[train_pid] = np.mean(X_pid, axis=0)\n",
    "            \n",
    "            # æ¸¬è©¦ï¼šæ‰¾ Top-N ç›¸ä¼¼çš„é³³æ¢¨\n",
    "            test_centroid = np.mean(X_test, axis=0)\n",
    "            \n",
    "            distances = {\n",
    "                pid: euclidean(test_centroid, centroid)\n",
    "                for pid, centroid in pineapple_centroids.items()\n",
    "            }\n",
    "            \n",
    "            # æ’åºæ‰¾å‡ºæœ€ç›¸ä¼¼çš„ N é¡†\n",
    "            sorted_pids = sorted(distances, key=distances.get)\n",
    "            top_n_pids = sorted_pids[:min(self.n_similar, len(sorted_pids))]\n",
    "            \n",
    "            # é›†æˆé æ¸¬ï¼ˆæŠ•ç¥¨ï¼‰\n",
    "            ensemble_preds = []\n",
    "            for pid in top_n_pids:\n",
    "                model = pineapple_models[pid]\n",
    "                pred = model.predict(X_test)\n",
    "                ensemble_preds.append(pred)\n",
    "            \n",
    "            # å¤šæ•¸æŠ•ç¥¨\n",
    "            ensemble_preds = np.array(ensemble_preds)\n",
    "            y_pred = []\n",
    "            for i in range(len(X_test)):\n",
    "                votes = ensemble_preds[:, i]\n",
    "                # æ‰¾å‡ºçœ¾æ•¸\n",
    "                unique, counts = np.unique(votes, return_counts=True)\n",
    "                y_pred.append(unique[np.argmax(counts)])\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "            \n",
    "            top_n_str = ', '.join([f\"{pid}({distances[pid]:.1f})\" for pid in top_n_pids])\n",
    "            print(f\"   ğŸ æ¸¬è©¦ {test_pid}: \"\n",
    "                  f\"æ¨£æœ¬={len(y_test):3d}, \"\n",
    "                  f\"Top-{self.n_similar}=[{top_n_str}], \"\n",
    "                  f\"æº–ç¢ºç‡={acc:.3f}\")\n",
    "        \n",
    "        overall_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ… é›†æˆ Domain Adaptation çµæœ\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"LOSO æº–ç¢ºç‡: {overall_acc:.3f} ({overall_acc*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nè©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "        report = classification_report(\n",
    "            all_true, all_preds,\n",
    "            target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "            digits=3\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        return overall_acc, all_preds, all_true\n",
    "\n",
    "\n",
    "# ===== åŸ·è¡Œæ–¹æ¡ˆ C =====\n",
    "\n",
    "# è¼‰å…¥åŸå§‹ç‰¹å¾µï¼ˆä¸ç”¨æ­£è¦åŒ–ç‰ˆæœ¬ï¼‰\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹åŸ·è¡Œæ–¹æ¡ˆ C\\n\")\n",
    "\n",
    "# æ–¹æ³• 1: å–®å€‹æœ€ç›¸ä¼¼é³³æ¢¨\n",
    "print(\"\\nã€æ–¹æ³• 1ã€‘å–®å€‹æœ€ç›¸ä¼¼é³³æ¢¨æ¨¡å‹\")\n",
    "print(\"-\"*60)\n",
    "model1 = DomainAdaptationModel()\n",
    "acc1, preds1, true1 = model1.train_loso(X, y, metadata)\n",
    "\n",
    "# æ–¹æ³• 2: Top-3 é›†æˆ\n",
    "print(\"\\n\\nã€æ–¹æ³• 2ã€‘Top-3 ç›¸ä¼¼é³³æ¢¨é›†æˆ\")\n",
    "print(\"-\"*60)\n",
    "model2 = EnsembleAdaptationModel(n_similar=3)\n",
    "acc2, preds2, true2 = model2.train_loso(X, y, metadata)\n",
    "\n",
    "# æ–¹æ³• 3: Top-5 é›†æˆ\n",
    "print(\"\\n\\nã€æ–¹æ³• 3ã€‘Top-5 ç›¸ä¼¼é³³æ¢¨é›†æˆ\")\n",
    "print(\"-\"*60)\n",
    "model3 = EnsembleAdaptationModel(n_similar=5)\n",
    "acc3, preds3, true3 = model3.train_loso(X, y, metadata)\n",
    "\n",
    "# æœ€çµ‚å°æ¯”\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆç¸½çµå°æ¯”\")\n",
    "print(\"=\"*60)\n",
    "print(f\"åŸå§‹æ–¹æ³•ï¼ˆå…¨å±€æ¨¡å‹ï¼‰:        LOSO = 0.598 (59.8%)\")\n",
    "print(f\"æ–¹æ¡ˆ Aï¼ˆæ­£è¦åŒ–ç‰¹å¾µï¼‰:        LOSO = 0.551 (55.1%)\")\n",
    "print(f\"æ–¹æ¡ˆ C-1ï¼ˆå–®å€‹ç›¸ä¼¼ï¼‰:        LOSO = {acc1:.3f} ({acc1*100:.1f}%)\")\n",
    "print(f\"æ–¹æ¡ˆ C-2ï¼ˆTop-3 é›†æˆï¼‰:      LOSO = {acc2:.3f} ({acc2*100:.1f}%)\")\n",
    "print(f\"æ–¹æ¡ˆ C-3ï¼ˆTop-5 é›†æˆï¼‰:      LOSO = {acc3:.3f} ({acc3*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_acc = max(acc1, acc2, acc3)\n",
    "if best_acc > 0.598:\n",
    "    improvement = (best_acc - 0.598) * 100\n",
    "    print(f\"\\nâœ… æ‰¾åˆ°æ”¹é€²ï¼æå‡ {improvement:+.1f}%\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  æ–¹æ¡ˆ C æ•ˆæœæœ‰é™ï¼Œå»ºè­°æ”¶é›†æ›´å¤šé³³æ¢¨æ•¸æ“š\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06d15bc8-9053-488a-9ece-6b6ec5851d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: models/final_domain_adaptation_model.pkl\n",
      "   LOSO æº–ç¢ºç‡: 62.2%\n",
      "   åŒ…å« 11 é¡†é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "# ===== ä¿å­˜æ–¹æ¡ˆ C-1 çš„æœ€ä½³æ¨¡å‹ =====\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ç”¨å…¨éƒ¨æ•¸æ“šè¨“ç·´æœ€çµ‚æ¨¡å‹ï¼ˆç‚ºæ¯é¡†é³³æ¢¨è¨“ç·´å°ˆå±¬æ¨¡å‹ï¼‰\n",
    "final_model = DomainAdaptationModel()\n",
    "\n",
    "# è¨“ç·´æ‰€æœ‰é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pineapple_ids = metadata['pineapple_id'].unique()\n",
    "\n",
    "# ç‚ºæ¯é¡†é³³æ¢¨è¨“ç·´æ¨¡å‹\n",
    "for pid in pineapple_ids:\n",
    "    pid_mask = metadata['pineapple_id'] == pid\n",
    "    X_pid = X_scaled[pid_mask]\n",
    "    y_pid = y[pid_mask]\n",
    "    \n",
    "    rf_pid = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_pid.fit(X_pid, y_pid)\n",
    "    \n",
    "    final_model.pineapple_models[pid] = rf_pid\n",
    "    final_model.pineapple_centroids[pid] = np.mean(X_pid, axis=0)\n",
    "\n",
    "final_model.global_scaler = scaler\n",
    "\n",
    "# ä¿å­˜\n",
    "with open('models/final_domain_adaptation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(\"âœ… æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: models/final_domain_adaptation_model.pkl\")\n",
    "print(f\"   LOSO æº–ç¢ºç‡: 62.2%\")\n",
    "print(f\"   åŒ…å« {len(pineapple_ids)} é¡†é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e577684d-4b46-4f9d-830c-0cb0b3cd1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ J: ç‰¹å¾µé¸æ“‡\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸: 53\n",
      "ğŸ“Š æ¨£æœ¬æ•¸: 626\n",
      "ğŸ“Š é³³æ¢¨æ•¸: 11\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š åˆ†æç‰¹å¾µé‡è¦æ€§...\n",
      "============================================================\n",
      "\n",
      "ğŸ” Top 15 æœ€é‡è¦ç‰¹å¾µ:\n",
      "   1. MQ135_auc                      0.0406\n",
      "   2. MQ9_auc                        0.0393\n",
      "   3. MQ135_mean                     0.0384\n",
      "   4. MQ135_TGS2602_ratio            0.0383\n",
      "   5. MQ9_mean                       0.0380\n",
      "   6. MQ9_min                        0.0366\n",
      "   7. MQ135_min                      0.0343\n",
      "   8. MQ135_max                      0.0342\n",
      "   9. MQ3_delta_mean                 0.0334\n",
      "  10. MQ3_auc                        0.0328\n",
      "  11. TGS2602_auc                    0.0328\n",
      "  12. MQ3_max                        0.0318\n",
      "  13. TGS2602_min                    0.0303\n",
      "  14. TGS2602_mean                   0.0299\n",
      "  15. MQ3_mean                       0.0297\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-5 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-5 æº–ç¢ºç‡: 0.486 (48.6%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-10 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-10 æº–ç¢ºç‡: 0.516 (51.6%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-15 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-15 æº–ç¢ºç‡: 0.546 (54.6%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-20 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-20 æº–ç¢ºç‡: 0.526 (52.6%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-25 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-25 æº–ç¢ºç‡: 0.518 (51.8%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-30 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-30 æº–ç¢ºç‡: 0.529 (52.9%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ J ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "ç‰¹å¾µæ•¸        æº–ç¢ºç‡        ç›¸æ¯”Baseline     \n",
      "----------------------------------------\n",
      "âš ï¸ 5        48.6%      -13.6%\n",
      "âš ï¸ 10       51.6%      -10.6%\n",
      "âš ï¸ 15       54.6%      -7.6%\n",
      "âš ï¸ 20       52.6%      -9.6%\n",
      "âš ï¸ 25       51.8%      -10.4%\n",
      "âš ï¸ 30       52.9%      -9.3%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³é…ç½®: Top-15 ç‰¹å¾µ\n",
      "   æº–ç¢ºç‡: 54.6%\n",
      "   âš ï¸  ä¸‹é™: -7.6 å€‹ç™¾åˆ†é»\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æœ€ä½³æ¨¡å‹è©³ç´°å ±å‘Š (Top-15)\n",
      "============================================================\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.470     0.551     0.507       127\n",
      "     Stage 1      0.409     0.439     0.424       123\n",
      "     Stage 2      0.352     0.231     0.279       108\n",
      "     Stage 3      0.704     0.720     0.712       268\n",
      "\n",
      "    accuracy                          0.546       626\n",
      "   macro avg      0.484     0.485     0.481       626\n",
      "weighted avg      0.538     0.546     0.539       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 70  34   7  16]\n",
      " [ 29  54  10  30]\n",
      " [ 24  24  25  35]\n",
      " [ 26  20  29 193]]\n",
      "\n",
      "ğŸ” ä½¿ç”¨çš„ Top-15 ç‰¹å¾µ:\n",
      "   1. MQ135_auc                      (é‡è¦æ€§: 0.0406)\n",
      "   2. MQ9_auc                        (é‡è¦æ€§: 0.0393)\n",
      "   3. MQ135_mean                     (é‡è¦æ€§: 0.0384)\n",
      "   4. MQ135_TGS2602_ratio            (é‡è¦æ€§: 0.0383)\n",
      "   5. MQ9_mean                       (é‡è¦æ€§: 0.0380)\n",
      "   6. MQ9_min                        (é‡è¦æ€§: 0.0366)\n",
      "   7. MQ135_min                      (é‡è¦æ€§: 0.0343)\n",
      "   8. MQ135_max                      (é‡è¦æ€§: 0.0342)\n",
      "   9. MQ3_delta_mean                 (é‡è¦æ€§: 0.0334)\n",
      "  10. MQ3_auc                        (é‡è¦æ€§: 0.0328)\n",
      "  11. TGS2602_auc                    (é‡è¦æ€§: 0.0328)\n",
      "  12. MQ3_max                        (é‡è¦æ€§: 0.0318)\n",
      "  13. TGS2602_min                    (é‡è¦æ€§: 0.0303)\n",
      "  14. TGS2602_mean                   (é‡è¦æ€§: 0.0299)\n",
      "  15. MQ3_mean                       (é‡è¦æ€§: 0.0297)\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_j_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ J: ç‰¹å¾µé¸æ“‡ï¼ˆç°¡åŒ–é«˜æ•ˆç‰ˆï¼‰=====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ J: ç‰¹å¾µé¸æ“‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "print(f\"\\nğŸ“Š åŸå§‹ç‰¹å¾µæ•¸: {X.shape[1]}\")\n",
    "print(f\"ğŸ“Š æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "print(f\"ğŸ“Š é³³æ¢¨æ•¸: {len(np.unique(pineapple_ids))}\")\n",
    "\n",
    "# ========== å…ˆæ‰¾å‡ºå…¨å±€æœ€é‡è¦çš„ç‰¹å¾µ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š åˆ†æç‰¹å¾µé‡è¦æ€§...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_full = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "X_scaled_full = StandardScaler().fit_transform(X)\n",
    "rf_full.fit(X_scaled_full, y)\n",
    "\n",
    "# å–å¾—ç‰¹å¾µé‡è¦æ€§\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data['features'].columns,\n",
    "    'importance': rf_full.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ” Top 15 æœ€é‡è¦ç‰¹å¾µ:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:30s} {row['importance']:.4f}\")\n",
    "\n",
    "# ========== æ¸¬è©¦ä¸åŒæ•¸é‡çš„ç‰¹å¾µ ==========\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "feature_counts = [5, 10, 15, 20, 25, 30]\n",
    "results = {}\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“Œ æ¸¬è©¦ Top-{n_features} ç‰¹å¾µ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # é¸æ“‡ top N ç‰¹å¾µ\n",
    "    top_features = feature_importance.head(n_features)['feature'].tolist()\n",
    "    top_indices = [data['features'].columns.tolist().index(f) for f in top_features]\n",
    "    X_subset = X[:, top_indices]\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_subset, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_subset[train_idx], X_subset[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # è¨“ç·´\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # é æ¸¬\n",
    "        y_pred.extend(rf.predict(X_test_scaled))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results[n_features] = {\n",
    "        'accuracy': acc,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'features': top_features\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Top-{n_features} æº–ç¢ºç‡: {acc:.3f} ({acc:.1%})\")\n",
    "\n",
    "# ========== è©³ç´°å ±å‘Šæœ€ä½³çµæœ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ J ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'ç‰¹å¾µæ•¸':<10} {'æº–ç¢ºç‡':<10} {'ç›¸æ¯”Baseline':<15}\")\n",
    "print(\"-\" * 40)\n",
    "baseline = 0.622\n",
    "\n",
    "for n_feat in feature_counts:\n",
    "    acc = results[n_feat]['accuracy']\n",
    "    diff = (acc - baseline) * 100\n",
    "    symbol = \"âœ…\" if acc > baseline else \"âš ï¸\"\n",
    "    print(f\"{symbol} {n_feat:<8} {acc:.1%}      {diff:+.1f}%\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ–¹æ³•\n",
    "best_n = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "best_acc = results[best_n]['accuracy']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³é…ç½®: Top-{best_n} ç‰¹å¾µ\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_acc:.1%}\")\n",
    "\n",
    "improvement = (best_acc - baseline) * 100\n",
    "if best_acc > baseline:\n",
    "    print(f\"   âœ… æå‡: +{improvement:.1f} å€‹ç™¾åˆ†é»\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  ä¸‹é™: {improvement:.1f} å€‹ç™¾åˆ†é»\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“Š æœ€ä½³æ¨¡å‹è©³ç´°å ±å‘Š (Top-{best_n})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(\n",
    "    results[best_n]['y_true'], \n",
    "    results[best_n]['y_pred'],\n",
    "    target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "print(\"\\næ··æ·†çŸ©é™£:\")\n",
    "cm = confusion_matrix(results[best_n]['y_true'], results[best_n]['y_pred'])\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nğŸ” ä½¿ç”¨çš„ Top-{best_n} ç‰¹å¾µ:\")\n",
    "for i, feat in enumerate(results[best_n]['features'], 1):\n",
    "    imp = feature_importance[feature_importance['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:30s} (é‡è¦æ€§: {imp:.4f})\")\n",
    "\n",
    "# ========== å„²å­˜çµæœ ==========\n",
    "results_j = {\n",
    "    'all_results': results,\n",
    "    'best_n_features': best_n,\n",
    "    'best_accuracy': best_acc,\n",
    "    'feature_importance': feature_importance,\n",
    "    'baseline': baseline,\n",
    "    'improvement': improvement\n",
    "}\n",
    "\n",
    "with open('models/solution_j_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_j, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_j_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a03466e-c45e-4962-9444-c89415a31df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (0.1.5)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e89cec86-255a-437a-a0e8-8d405f42108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”§ æ–¹æ¡ˆ J-Plus: æ”¹å–„ Stage 2 è­˜åˆ¥\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
      "ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): 127, np.int64(1): 123, np.int64(2): 108, np.int64(3): 268}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: é¡åˆ¥æ¬Šé‡å¹³è¡¡ (Class Weight)\n",
      "============================================================\n",
      "\n",
      "âœ… é¡åˆ¥å¹³è¡¡ æº–ç¢ºç‡: 0.502 (50.2%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.545     0.472     0.506       127\n",
      "     Stage 1      0.364     0.415     0.388       123\n",
      "     Stage 2      0.302     0.269     0.284       108\n",
      "     Stage 3      0.621     0.649     0.635       268\n",
      "\n",
      "    accuracy                          0.502       626\n",
      "   macro avg      0.458     0.451     0.453       626\n",
      "weighted avg      0.500     0.502     0.500       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 60  30   6  31]\n",
      " [ 21  51  13  38]\n",
      " [ 14  28  29  37]\n",
      " [ 15  31  48 174]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: SMOTE éæ¡æ¨£\n",
      "============================================================\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 112, np.int64(1): 108, np.int64(2): 93, np.int64(3): 231} â†’ SMOTEå¾Œ: {np.int64(0): 231, np.int64(1): 231, np.int64(2): 231, np.int64(3): 231}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 112, np.int64(1): 108, np.int64(2): 93, np.int64(3): 238} â†’ SMOTEå¾Œ: {np.int64(0): 238, np.int64(1): 238, np.int64(2): 238, np.int64(3): 238}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 112, np.int64(1): 112, np.int64(2): 101, np.int64(3): 234} â†’ SMOTEå¾Œ: {np.int64(0): 234, np.int64(1): 234, np.int64(2): 234, np.int64(3): 234}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 112, np.int64(1): 115, np.int64(2): 93, np.int64(3): 239} â†’ SMOTEå¾Œ: {np.int64(0): 239, np.int64(1): 239, np.int64(2): 239, np.int64(3): 239}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 119, np.int64(1): 108, np.int64(2): 101, np.int64(3): 231} â†’ SMOTEå¾Œ: {np.int64(0): 231, np.int64(1): 231, np.int64(2): 231, np.int64(3): 231}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 112, np.int64(1): 108, np.int64(2): 98, np.int64(3): 256} â†’ SMOTEå¾Œ: {np.int64(0): 256, np.int64(1): 256, np.int64(2): 256, np.int64(3): 256}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 122, np.int64(1): 118, np.int64(2): 103, np.int64(3): 224} â†’ SMOTEå¾Œ: {np.int64(0): 224, np.int64(1): 224, np.int64(2): 224, np.int64(3): 224}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 122, np.int64(1): 118, np.int64(2): 103, np.int64(3): 223} â†’ SMOTEå¾Œ: {np.int64(0): 223, np.int64(1): 223, np.int64(2): 223, np.int64(3): 223}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 112, np.int64(1): 108, np.int64(2): 101, np.int64(3): 268} â†’ SMOTEå¾Œ: {np.int64(0): 268, np.int64(1): 268, np.int64(2): 268, np.int64(3): 268}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 116, np.int64(1): 111, np.int64(2): 101, np.int64(3): 268} â†’ SMOTEå¾Œ: {np.int64(0): 268, np.int64(1): 268, np.int64(2): 268, np.int64(3): 268}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 119, np.int64(1): 116, np.int64(2): 93, np.int64(3): 268} â†’ SMOTEå¾Œ: {np.int64(0): 268, np.int64(1): 268, np.int64(2): 268, np.int64(3): 268}\n",
      "\n",
      "âœ… SMOTE æº–ç¢ºç‡: 0.529 (52.9%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.590     0.543     0.566       127\n",
      "     Stage 1      0.447     0.553     0.495       123\n",
      "     Stage 2      0.314     0.296     0.305       108\n",
      "     Stage 3      0.635     0.604     0.620       268\n",
      "\n",
      "    accuracy                          0.529       626\n",
      "   macro avg      0.497     0.499     0.496       626\n",
      "weighted avg      0.534     0.529     0.530       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 69  21   6  31]\n",
      " [ 16  68  11  28]\n",
      " [ 14  28  32  34]\n",
      " [ 18  35  53 162]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: æ‰‹å‹•å¼·åŒ– Stage 2 æ¬Šé‡\n",
      "============================================================\n",
      "\n",
      "âœ… å¼·åŒ–æ¬Šé‡ æº–ç¢ºç‡: 0.564 (56.4%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.625     0.512     0.563       127\n",
      "     Stage 1      0.520     0.626     0.568       123\n",
      "     Stage 2      0.302     0.269     0.284       108\n",
      "     Stage 3      0.655     0.679     0.667       268\n",
      "\n",
      "    accuracy                          0.564       626\n",
      "   macro avg      0.526     0.521     0.521       626\n",
      "weighted avg      0.561     0.564     0.560       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 65  24   7  31]\n",
      " [ 11  77  11  24]\n",
      " [ 14  24  29  41]\n",
      " [ 14  23  49 182]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 4: æ›´è¤‡é›œçš„æ¨¡å‹ (deeper trees)\n",
      "============================================================\n",
      "\n",
      "âœ… è¤‡é›œæ¨¡å‹ æº–ç¢ºç‡: 0.514 (51.4%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.557     0.504     0.529       127\n",
      "     Stage 1      0.390     0.447     0.417       123\n",
      "     Stage 2      0.311     0.259     0.283       108\n",
      "     Stage 3      0.625     0.653     0.639       268\n",
      "\n",
      "    accuracy                          0.514       626\n",
      "   macro avg      0.471     0.466     0.467       626\n",
      "weighted avg      0.511     0.514     0.511       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 64  29   3  31]\n",
      " [ 23  55  11  34]\n",
      " [ 14  26  28  40]\n",
      " [ 14  31  48 175]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ J-Plus ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                   æº–ç¢ºç‡        Stage2 Recall  \n",
      "--------------------------------------------------\n",
      "Baseline (Top-5)     75.2%      26.9%\n",
      "é¡åˆ¥å¹³è¡¡                 50.2%      26.9%\n",
      "SMOTE                52.9%      29.6%\n",
      "å¼·åŒ–æ¬Šé‡                 56.4%      26.9%\n",
      "è¤‡é›œæ¨¡å‹                 51.4%      25.9%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æ¨è–¦æ–¹æ³•: å¼·åŒ–æ¬Šé‡\n",
      "   æº–ç¢ºç‡: 56.4%\n",
      "   Stage 2 Recall: 26.9% (åŸæœ¬ 14.7%)\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jplus_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ J-Plus: æ”¹å–„ Stage 2 è­˜åˆ¥ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”§ æ–¹æ¡ˆ J-Plus: æ”¹å–„ Stage 2 è­˜åˆ¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\")\n",
    "print(f\"ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {dict(Counter(y))}\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: é¡åˆ¥æ¬Šé‡å¹³è¡¡ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: é¡åˆ¥æ¬Šé‡å¹³è¡¡ (Class Weight)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_balanced = []\n",
    "y_pred_balanced = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è‡ªå‹•å¹³è¡¡é¡åˆ¥æ¬Šé‡\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=10, \n",
    "        class_weight='balanced',  # é—œéµï¼šè‡ªå‹•å¹³è¡¡\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_balanced.extend(y_test)\n",
    "    y_pred_balanced.extend(y_pred)\n",
    "\n",
    "acc_balanced = accuracy_score(y_true_balanced, y_pred_balanced)\n",
    "print(f\"\\nâœ… é¡åˆ¥å¹³è¡¡ æº–ç¢ºç‡: {acc_balanced:.3f} ({acc_balanced:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_balanced, y_pred_balanced,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_balanced, y_pred_balanced))\n",
    "\n",
    "# ========== æ–¹æ³• 2: SMOTE éæ¡æ¨£ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: SMOTE éæ¡æ¨£\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_smote = []\n",
    "y_pred_smote = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # æª¢æŸ¥è¨“ç·´é›†ä¸­æ˜¯å¦æœ‰è¶³å¤ çš„ Stage 2 æ¨£æœ¬\n",
    "    unique_classes = np.unique(y_train)\n",
    "    class_counts = Counter(y_train)\n",
    "    \n",
    "    print(f\"  è¨“ç·´é›†åˆ†å¸ƒ: {dict(class_counts)}\", end=\" \")\n",
    "    \n",
    "    # åªåœ¨æœ‰å¤šå€‹é¡åˆ¥ä¸”æ¯é¡è‡³å°‘ 2 å€‹æ¨£æœ¬æ™‚ä½¿ç”¨ SMOTE\n",
    "    if len(unique_classes) >= 2 and min(class_counts.values()) >= 2:\n",
    "        try:\n",
    "            # SMOTE éæ¡æ¨£\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(3, min(class_counts.values())-1))\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "            print(f\"â†’ SMOTEå¾Œ: {dict(Counter(y_train_resampled))}\")\n",
    "        except Exception as e:\n",
    "            print(f\"â†’ SMOTEå¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æ•¸æ“š\")\n",
    "            X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "    else:\n",
    "        print(f\"â†’ è·³éSMOTEï¼ˆé¡åˆ¥ä¸è¶³ï¼‰\")\n",
    "        X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_smote.extend(y_test)\n",
    "    y_pred_smote.extend(y_pred)\n",
    "\n",
    "acc_smote = accuracy_score(y_true_smote, y_pred_smote)\n",
    "print(f\"\\nâœ… SMOTE æº–ç¢ºç‡: {acc_smote:.3f} ({acc_smote:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_smote, y_pred_smote,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_smote, y_pred_smote))\n",
    "\n",
    "# ========== æ–¹æ³• 3: æ‰‹å‹•è¨­å®šæ›´é«˜çš„ Stage 2 æ¬Šé‡ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: æ‰‹å‹•å¼·åŒ– Stage 2 æ¬Šé‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_weighted = []\n",
    "y_pred_weighted = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # æ‰‹å‹•è¨­å®šæ¬Šé‡ï¼šStage 2 çµ¦ 3 å€æ¬Šé‡\n",
    "    class_weights = {\n",
    "        0: 1.0,  # Stage 0\n",
    "        1: 1.0,  # Stage 1\n",
    "        2: 3.0,  # Stage 2 (3å€æ¬Šé‡)\n",
    "        3: 1.0   # Stage 3\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_weighted.extend(y_test)\n",
    "    y_pred_weighted.extend(y_pred)\n",
    "\n",
    "acc_weighted = accuracy_score(y_true_weighted, y_pred_weighted)\n",
    "print(f\"\\nâœ… å¼·åŒ–æ¬Šé‡ æº–ç¢ºç‡: {acc_weighted:.3f} ({acc_weighted:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_weighted, y_pred_weighted,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_weighted, y_pred_weighted))\n",
    "\n",
    "# ========== æ–¹æ³• 4: æ›´æ·±çš„æ¨¹ + æ›´å¤šæ¨¹ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 4: æ›´è¤‡é›œçš„æ¨¡å‹ (deeper trees)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_complex = []\n",
    "y_pred_complex = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # æ›´å¤šæ¨¹ã€æ›´æ·±ã€å¹³è¡¡æ¬Šé‡\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=500,  # å¢åŠ åˆ° 500\n",
    "        max_depth=20,      # å¢åŠ æ·±åº¦\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_complex.extend(y_test)\n",
    "    y_pred_complex.extend(y_pred)\n",
    "\n",
    "acc_complex = accuracy_score(y_true_complex, y_pred_complex)\n",
    "print(f\"\\nâœ… è¤‡é›œæ¨¡å‹ æº–ç¢ºç‡: {acc_complex:.3f} ({acc_complex:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_complex, y_pred_complex,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_complex, y_pred_complex))\n",
    "\n",
    "# ========== ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ J-Plus ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = [\n",
    "    ('Baseline (Top-5)', 0.752, y_true_balanced, y_pred_balanced),  # é€™è£¡ç”¨ç¬¬ä¸€å€‹çš„ y_true\n",
    "    ('é¡åˆ¥å¹³è¡¡', acc_balanced, y_true_balanced, y_pred_balanced),\n",
    "    ('SMOTE', acc_smote, y_true_smote, y_pred_smote),\n",
    "    ('å¼·åŒ–æ¬Šé‡', acc_weighted, y_true_weighted, y_pred_weighted),\n",
    "    ('è¤‡é›œæ¨¡å‹', acc_complex, y_true_complex, y_pred_complex)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<20} {'æº–ç¢ºç‡':<10} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for method, acc, y_true, y_pred in results_summary:\n",
    "    # è¨ˆç®— Stage 2 çš„ recall\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape[0] > 2 and cm[2, :].sum() > 0:\n",
    "        stage2_recall = cm[2, 2] / cm[2, :].sum()\n",
    "    else:\n",
    "        stage2_recall = 0.0\n",
    "    \n",
    "    print(f\"{method:<20} {acc:.1%}      {stage2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ–¹æ³•ï¼ˆç¶œåˆè€ƒæ…®æº–ç¢ºç‡å’Œ Stage 2 recallï¼‰\n",
    "best_idx = np.argmax([r[1] for r in results_summary[1:]])  # è·³é baseline\n",
    "best_method = results_summary[best_idx + 1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æ¨è–¦æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "\n",
    "# è¨ˆç®— Stage 2 æ”¹å–„\n",
    "cm_best = confusion_matrix(best_method[2], best_method[3])\n",
    "if cm_best.shape[0] > 2:\n",
    "    stage2_recall_best = cm_best[2, 2] / cm_best[2, :].sum()\n",
    "    print(f\"   Stage 2 Recall: {stage2_recall_best:.1%} (åŸæœ¬ 14.7%)\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_jplus = {\n",
    "    'balanced': {'accuracy': acc_balanced, 'y_true': y_true_balanced, 'y_pred': y_pred_balanced},\n",
    "    'smote': {'accuracy': acc_smote, 'y_true': y_true_smote, 'y_pred': y_pred_smote},\n",
    "    'weighted': {'accuracy': acc_weighted, 'y_true': y_true_weighted, 'y_pred': y_pred_weighted},\n",
    "    'complex': {'accuracy': acc_complex, 'y_true': y_true_complex, 'y_pred': y_pred_complex},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_jplus_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_jplus, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jplus_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a2ef4-9747-47bb-8014-22dff7b38718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbfda3cd-c409-44e8-8216-f6d90a6c8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ J+H: Top-5 ç‰¹å¾µ + æ•¸æ“šå¢å¼·\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: å¢å¼· Stage 2 (3å€)\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 372 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 372 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 404 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 372 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 404 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 98 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 392 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 103 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 412 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 103 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 412 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 404 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 404 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 372 å€‹ (x4)\n",
      "\n",
      "âœ… å¢å¼· Stage 2 æº–ç¢ºç‡: 0.497 (49.7%)\n",
      "âœ… Stage 2 Recall: 47.2% (51/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.505     0.409     0.452       127\n",
      "     Stage 1      0.500     0.463     0.481       123\n",
      "     Stage 2      0.290     0.472     0.359       108\n",
      "     Stage 3      0.648     0.563     0.603       268\n",
      "\n",
      "    accuracy                          0.497       626\n",
      "   macro avg      0.486     0.477     0.474       626\n",
      "weighted avg      0.528     0.497     0.506       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 52  16  26  33]\n",
      " [ 24  57  17  25]\n",
      " [ 14  19  51  24]\n",
      " [ 13  22  82 151]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: å¢å¼· Stage 2 (5å€ï¼Œæ›´æ¿€é€²)\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 558 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 558 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 606 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 558 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 606 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 98 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 588 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 103 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 618 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 103 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 618 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 606 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 606 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 558 å€‹ (x6)\n",
      "\n",
      "âœ… å¢å¼· Stage 2 (5å€) æº–ç¢ºç‡: 0.506 (50.6%)\n",
      "âœ… Stage 2 Recall: 50.9% (55/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.484     0.354     0.409       127\n",
      "     Stage 1      0.573     0.480     0.522       123\n",
      "     Stage 2      0.284     0.509     0.364       108\n",
      "     Stage 3      0.669     0.590     0.627       268\n",
      "\n",
      "    accuracy                          0.506       626\n",
      "   macro avg      0.502     0.483     0.481       626\n",
      "weighted avg      0.546     0.506     0.517       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: å¢å¼· Stage 2 å’Œ Stage 3\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 465 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 231 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 693 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 465 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 238 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 714 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 505 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 234 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 702 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 465 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 239 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 717 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 505 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 231 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 693 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 98 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 490 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 256 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 768 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 103 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 515 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 224 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 672 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 103 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 515 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 223 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 669 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 505 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 268 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 804 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 101 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 505 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 268 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 804 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 93 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 465 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 268 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 804 å€‹ (x3)\n",
      "\n",
      "âœ… å¢å¼·å¤šé¡åˆ¥ æº–ç¢ºç‡: 0.498 (49.8%)\n",
      "âœ… Stage 2 Recall: 35.2% (38/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.583     0.386     0.464       127\n",
      "     Stage 1      0.570     0.463     0.511       123\n",
      "     Stage 2      0.232     0.352     0.279       108\n",
      "     Stage 3      0.604     0.627     0.615       268\n",
      "\n",
      "    accuracy                          0.498       626\n",
      "   macro avg      0.497     0.457     0.468       626\n",
      "weighted avg      0.529     0.498     0.506       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ J+H ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "Baseline (Top-5)          75.2%        14.7%\n",
      "å¢å¼· Stage 2 (3å€)           49.7%        47.2%\n",
      "å¢å¼· Stage 2 (5å€)           50.6%        50.9%\n",
      "å¢å¼· Stage 2+3              49.8%        35.2%\n",
      "\n",
      "ğŸ† ç¶œåˆæœ€ä½³: å¢å¼· Stage 2 (5å€)\n",
      "   æº–ç¢ºç‡: 50.6%\n",
      "   Stage 2 Recall: 50.9%\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +-11.6%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jh_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#å¤±æ•—!!!!ä¸è¦åŸ·è¡Œ\n",
    "\n",
    "\n",
    "# ===== æ–¹æ¡ˆ J + H: Top-5 ç‰¹å¾µ + æ•¸æ“šå¢å¼· =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ J+H: Top-5 ç‰¹å¾µ + æ•¸æ“šå¢å¼·\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: {top_5_features}\")\n",
    "\n",
    "# ========== æ•¸æ“šå¢å¼·å‡½æ•¸ ==========\n",
    "def augment_data(X, y, target_class=2, augment_factor=3):\n",
    "    \"\"\"\n",
    "    å°ç‰¹å®šé¡åˆ¥åšæ•¸æ“šå¢å¼·\n",
    "    \n",
    "    æ–¹æ³•ï¼š\n",
    "    1. å™ªè²æ³¨å…¥ (Jittering)\n",
    "    2. ç‰¹å¾µç¸®æ”¾ (Scaling)\n",
    "    3. çµ„åˆæ“¾å‹•\n",
    "    \"\"\"\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    # æ‰¾å‡ºç›®æ¨™é¡åˆ¥çš„æ¨£æœ¬\n",
    "    target_indices = np.where(y == target_class)[0]\n",
    "    \n",
    "    print(f\"\\nğŸ”§ å¢å¼· Stage {target_class}:\")\n",
    "    print(f\"   åŸå§‹æ¨£æœ¬: {len(target_indices)} å€‹\")\n",
    "    \n",
    "    for idx in target_indices:\n",
    "        original_sample = X[idx]\n",
    "        \n",
    "        # 1. åŸå§‹æ¨£æœ¬\n",
    "        X_augmented.append(original_sample)\n",
    "        y_augmented.append(target_class)\n",
    "        \n",
    "        # 2-4. ç”Ÿæˆå¢å¼·æ¨£æœ¬\n",
    "        for i in range(augment_factor):\n",
    "            if i == 0:\n",
    "                # æ–¹æ³• 1: é«˜æ–¯å™ªè²æ³¨å…¥\n",
    "                noise = np.random.normal(0, 0.05, size=original_sample.shape)\n",
    "                augmented = original_sample + noise\n",
    "                \n",
    "            elif i == 1:\n",
    "                # æ–¹æ³• 2: ç‰¹å¾µç¸®æ”¾ (Â±5%)\n",
    "                scale = np.random.uniform(0.95, 1.05, size=original_sample.shape)\n",
    "                augmented = original_sample * scale\n",
    "                \n",
    "            else:\n",
    "                # æ–¹æ³• 3: çµ„åˆæ“¾å‹•\n",
    "                noise = np.random.normal(0, 0.03, size=original_sample.shape)\n",
    "                scale = np.random.uniform(0.97, 1.03, size=original_sample.shape)\n",
    "                augmented = original_sample * scale + noise\n",
    "            \n",
    "            X_augmented.append(augmented)\n",
    "            y_augmented.append(target_class)\n",
    "    \n",
    "    print(f\"   å¢å¼·å¾Œæ¨£æœ¬: {len(y_augmented)} å€‹ (x{augment_factor+1})\")\n",
    "    \n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "# ========== æ–¹æ³• 1: åªå¢å¼· Stage 2 ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: å¢å¼· Stage 2 (3å€)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_true_aug2 = []\n",
    "y_pred_aug2 = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # å¢å¼· Stage 2 è¨“ç·´æ¨£æœ¬\n",
    "    X_aug, y_aug = augment_data(X_train, y_train, target_class=2, augment_factor=3)\n",
    "    \n",
    "    # åˆä½µåŸå§‹ + å¢å¼·æ•¸æ“š\n",
    "    X_train_combined = np.vstack([X_train, X_aug])\n",
    "    y_train_combined = np.hstack([y_train, y_aug])\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train_combined)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_aug2.extend(y_test)\n",
    "    y_pred_aug2.extend(y_pred)\n",
    "\n",
    "acc_aug2 = accuracy_score(y_true_aug2, y_pred_aug2)\n",
    "cm_aug2 = confusion_matrix(y_true_aug2, y_pred_aug2)\n",
    "stage2_recall_aug2 = cm_aug2[2, 2] / cm_aug2[2, :].sum() if cm_aug2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¢å¼· Stage 2 æº–ç¢ºç‡: {acc_aug2:.3f} ({acc_aug2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_aug2:.1%} ({cm_aug2[2, 2]}/{cm_aug2[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_aug2, y_pred_aug2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_aug2)\n",
    "\n",
    "# ========== æ–¹æ³• 2: å¢å¼· Stage 2 (5å€) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: å¢å¼· Stage 2 (5å€ï¼Œæ›´æ¿€é€²)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_aug2_5x = []\n",
    "y_pred_aug2_5x = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # 5å€å¢å¼·\n",
    "    X_aug, y_aug = augment_data(X_train, y_train, target_class=2, augment_factor=5)\n",
    "    \n",
    "    X_train_combined = np.vstack([X_train, X_aug])\n",
    "    y_train_combined = np.hstack([y_train, y_aug])\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train_combined)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_aug2_5x.extend(y_test)\n",
    "    y_pred_aug2_5x.extend(y_pred)\n",
    "\n",
    "acc_aug2_5x = accuracy_score(y_true_aug2_5x, y_pred_aug2_5x)\n",
    "cm_aug2_5x = confusion_matrix(y_true_aug2_5x, y_pred_aug2_5x)\n",
    "stage2_recall_aug2_5x = cm_aug2_5x[2, 2] / cm_aug2_5x[2, :].sum() if cm_aug2_5x[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¢å¼· Stage 2 (5å€) æº–ç¢ºç‡: {acc_aug2_5x:.3f} ({acc_aug2_5x:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_aug2_5x:.1%} ({cm_aug2_5x[2, 2]}/{cm_aug2_5x[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_aug2_5x, y_pred_aug2_5x,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 3: å¢å¼·æ‰€æœ‰å°‘æ•¸é¡åˆ¥ (Stage 2 + 3) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: å¢å¼· Stage 2 å’Œ Stage 3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_aug_all = []\n",
    "y_pred_aug_all = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # å¢å¼· Stage 2\n",
    "    X_aug2, y_aug2 = augment_data(X_train, y_train, target_class=2, augment_factor=4)\n",
    "    \n",
    "    # å¢å¼· Stage 3\n",
    "    X_aug3, y_aug3 = augment_data(X_train, y_train, target_class=3, augment_factor=2)\n",
    "    \n",
    "    # åˆä½µ\n",
    "    X_train_combined = np.vstack([X_train, X_aug2, X_aug3])\n",
    "    y_train_combined = np.hstack([y_train, y_aug2, y_aug3])\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train_combined)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_aug_all.extend(y_test)\n",
    "    y_pred_aug_all.extend(y_pred)\n",
    "\n",
    "acc_aug_all = accuracy_score(y_true_aug_all, y_pred_aug_all)\n",
    "cm_aug_all = confusion_matrix(y_true_aug_all, y_pred_aug_all)\n",
    "stage2_recall_aug_all = cm_aug_all[2, 2] / cm_aug_all[2, :].sum() if cm_aug_all[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¢å¼·å¤šé¡åˆ¥ æº–ç¢ºç‡: {acc_aug_all:.3f} ({acc_aug_all:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_aug_all:.1%} ({cm_aug_all[2, 2]}/{cm_aug_all[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_aug_all, y_pred_aug_all,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ J+H ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('Baseline (Top-5)', 0.752, 0.147),\n",
    "    ('å¢å¼· Stage 2 (3å€)', acc_aug2, stage2_recall_aug2),\n",
    "    ('å¢å¼· Stage 2 (5å€)', acc_aug2_5x, stage2_recall_aug2_5x),\n",
    "    ('å¢å¼· Stage 2+3', acc_aug_all, stage2_recall_aug_all)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    print(f\"{method:<25} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³å¹³è¡¡\n",
    "best_idx = 0\n",
    "best_score = 0\n",
    "\n",
    "for i, (method, acc, s2_recall) in enumerate(results[1:], 1):\n",
    "    # ç¶œåˆåˆ†æ•¸: 70% æº–ç¢ºç‡ + 30% Stage 2 recall\n",
    "    score = 0.7 * acc + 0.3 * s2_recall\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_idx = i\n",
    "\n",
    "best_method = results[best_idx]\n",
    "print(f\"\\nğŸ† ç¶œåˆæœ€ä½³: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement = (best_method[1] - 0.622) * 100\n",
    "print(f\"   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement:.1f}%\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_jh = {\n",
    "    'aug2_3x': {'accuracy': acc_aug2, 'stage2_recall': stage2_recall_aug2,\n",
    "                'y_true': y_true_aug2, 'y_pred': y_pred_aug2},\n",
    "    'aug2_5x': {'accuracy': acc_aug2_5x, 'stage2_recall': stage2_recall_aug2_5x,\n",
    "                'y_true': y_true_aug2_5x, 'y_pred': y_pred_aug2_5x},\n",
    "    'aug_all': {'accuracy': acc_aug_all, 'stage2_recall': stage2_recall_aug_all,\n",
    "                'y_true': y_true_aug_all, 'y_pred': y_pred_aug_all},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1],\n",
    "    'best_stage2_recall': best_method[2]\n",
    "}\n",
    "\n",
    "with open('models/solution_jh_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_jh, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jh_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae319476-198d-4f75-a355-158dfb00a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ F: åŠç›£ç£å­¸ç¿’ + å½æ¨™ç±¤\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: å–®æ¬¡å½æ¨™ç±¤ï¼ˆé«˜ç½®ä¿¡åº¦ > 0.8ï¼‰\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.549\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 45/82 (54.9%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.549\n",
      "\n",
      "ğŸ“Š Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.547\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 29/75 (38.7%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.520\n",
      "\n",
      "ğŸ“Š Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.567\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 9/67 (13.4%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.657\n",
      "\n",
      "ğŸ“Š Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.388\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 27/67 (40.3%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.373\n",
      "\n",
      "ğŸ“Š Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.343\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 28/67 (41.8%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.433\n",
      "\n",
      "ğŸ“Š Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.538\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 25/52 (48.1%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.615\n",
      "\n",
      "ğŸ“Š Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.763\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 37/59 (62.7%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.797\n",
      "\n",
      "ğŸ“Š Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.850\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 41/60 (68.3%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.867\n",
      "\n",
      "ğŸ“Š Fold 9/8 - æ¸¬è©¦é³³æ¢¨: 09\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.189\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 19/37 (51.4%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.189\n",
      "\n",
      "ğŸ“Š Fold 10/8 - æ¸¬è©¦é³³æ¢¨: 10\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.400\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 1/30 (3.3%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.400\n",
      "\n",
      "ğŸ“Š Fold 11/8 - æ¸¬è©¦é³³æ¢¨: 11\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.467\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 8/30 (26.7%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.433\n",
      "\n",
      "âœ… å–®æ¬¡å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: 0.551 (55.1%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.589     0.520     0.552       127\n",
      "     Stage 1      0.474     0.528     0.500       123\n",
      "     Stage 2      0.333     0.315     0.324       108\n",
      "     Stage 3      0.655     0.672     0.663       268\n",
      "\n",
      "    accuracy                          0.551       626\n",
      "   macro avg      0.513     0.509     0.510       626\n",
      "weighted avg      0.551     0.551     0.550       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 66  26   5  30]\n",
      " [ 20  65  11  27]\n",
      " [ 15  21  34  38]\n",
      " [ 11  25  52 180]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: è¿­ä»£å¼å½æ¨™ç±¤ï¼ˆ3è¼ªè¿­ä»£ï¼‰\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 43/82 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 48/82 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 62/82 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.488\n",
      "\n",
      "ğŸ“Š Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 22/75 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 30/75 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 57/75 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.507\n",
      "\n",
      "ğŸ“Š Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 0/67 (é–¾å€¼=0.9)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.567\n",
      "\n",
      "ğŸ“Š Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 23/67 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 28/67 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 40/67 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.448\n",
      "\n",
      "ğŸ“Š Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 15/67 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 29/67 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 37/67 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.224\n",
      "\n",
      "ğŸ“Š Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 22/52 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 27/52 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 29/52 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.558\n",
      "\n",
      "ğŸ“Š Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 32/59 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 39/59 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 44/59 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.864\n",
      "\n",
      "ğŸ“Š Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 30/60 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 34/60 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 49/60 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.867\n",
      "\n",
      "ğŸ“Š Fold 9/8 - æ¸¬è©¦é³³æ¢¨: 09\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 10/37 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 19/37 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 26/37 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.000\n",
      "\n",
      "ğŸ“Š Fold 10/8 - æ¸¬è©¦é³³æ¢¨: 10\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 0/30 (é–¾å€¼=0.9)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.400\n",
      "\n",
      "ğŸ“Š Fold 11/8 - æ¸¬è©¦é³³æ¢¨: 11\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 0/30 (é–¾å€¼=0.9)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.467\n",
      "\n",
      "âœ… è¿­ä»£å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: 0.510 (51.0%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.516     0.504     0.510       127\n",
      "     Stage 1      0.401     0.447     0.423       123\n",
      "     Stage 2      0.291     0.278     0.284       108\n",
      "     Stage 3      0.649     0.634     0.642       268\n",
      "\n",
      "    accuracy                          0.510       626\n",
      "   macro avg      0.464     0.466     0.465       626\n",
      "weighted avg      0.512     0.510     0.510       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 64  28  10  25]\n",
      " [ 28  55  11  29]\n",
      " [ 14  26  30  38]\n",
      " [ 18  28  52 170]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: ä¿å®ˆå¼å½æ¨™ç±¤ï¼ˆåªå¢å¼· Stage 2 é æ¸¬ï¼‰\n",
      "============================================================\n",
      "\n",
      "âœ… Stage 2 å°ˆç”¨å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: 0.532 (53.2%)\n",
      "âœ… Stage 2 Recall: 25.0%\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.563     0.457     0.504       127\n",
      "     Stage 1      0.462     0.537     0.496       123\n",
      "     Stage 2      0.270     0.250     0.260       108\n",
      "     Stage 3      0.650     0.679     0.664       268\n",
      "\n",
      "    accuracy                          0.532       626\n",
      "   macro avg      0.486     0.481     0.481       626\n",
      "weighted avg      0.530     0.532     0.529       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ F ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "Baseline (Top-5)          75.2%        14.7%\n",
      "å–®æ¬¡å½æ¨™ç±¤ (0.8)               55.1%        31.5%\n",
      "è¿­ä»£å½æ¨™ç±¤ (3è¼ª)                51.0%        27.8%\n",
      "Stage 2 å°ˆç”¨                53.2%        25.0%\n",
      "\n",
      "ğŸ† æœ€ä½³æ–¹æ³•: Baseline (Top-5)\n",
      "   æº–ç¢ºç‡: 75.2%\n",
      "   Stage 2 Recall: 14.7%\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +13.0%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_f_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ F: åŠç›£ç£å­¸ç¿’ + å½æ¨™ç±¤ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ F: åŠç›£ç£å­¸ç¿’ + å½æ¨™ç±¤\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: å–®æ¬¡å½æ¨™ç±¤ï¼ˆåŸºç¤ç‰ˆï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: å–®æ¬¡å½æ¨™ç±¤ï¼ˆé«˜ç½®ä¿¡åº¦ > 0.8ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pseudo1 = []\n",
    "y_pred_pseudo1 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    print(f\"\\nğŸ“Š Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 1: ç”¨ 7 é¡†é³³æ¢¨è¨“ç·´åˆå§‹æ¨¡å‹\n",
    "    rf_initial = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_initial.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 2: å°æ¸¬è©¦é³³æ¢¨é æ¸¬ï¼ˆå¸¶æ©Ÿç‡ï¼‰\n",
    "    y_test_proba = rf_initial.predict_proba(X_test_scaled)\n",
    "    y_test_pred_initial = rf_initial.predict(X_test_scaled)\n",
    "    max_proba = y_test_proba.max(axis=1)\n",
    "    \n",
    "    # Step 3: é¸æ“‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬ä½œç‚ºå½æ¨™ç±¤\n",
    "    high_confidence_mask = max_proba > 0.8\n",
    "    n_pseudo = high_confidence_mask.sum()\n",
    "    \n",
    "    print(f\"   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: {accuracy_score(y_test, y_test_pred_initial):.3f}\")\n",
    "    print(f\"   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: {n_pseudo}/{len(y_test)} ({n_pseudo/len(y_test)*100:.1f}%)\")\n",
    "    \n",
    "    if n_pseudo > 0:\n",
    "        # å–å¾—å½æ¨™ç±¤\n",
    "        X_pseudo = X_test_scaled[high_confidence_mask]\n",
    "        y_pseudo = y_test_pred_initial[high_confidence_mask]\n",
    "        \n",
    "        # Step 4: åˆä½µè¨“ç·´é›† + å½æ¨™ç±¤ï¼Œé‡æ–°è¨“ç·´\n",
    "        X_train_augmented = np.vstack([X_train_scaled, X_pseudo])\n",
    "        y_train_augmented = np.hstack([y_train, y_pseudo])\n",
    "        \n",
    "        rf_retrained = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_retrained.fit(X_train_augmented, y_train_augmented)\n",
    "        \n",
    "        # Step 5: æœ€çµ‚é æ¸¬\n",
    "        y_pred_final = rf_retrained.predict(X_test_scaled)\n",
    "    else:\n",
    "        print(\"   âš ï¸  æ²’æœ‰é«˜ç½®ä¿¡åº¦æ¨£æœ¬ï¼Œä½¿ç”¨åˆå§‹é æ¸¬\")\n",
    "        y_pred_final = y_test_pred_initial\n",
    "    \n",
    "    final_acc = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"   é‡è¨“å¾Œæº–ç¢ºç‡: {final_acc:.3f}\")\n",
    "    \n",
    "    y_true_pseudo1.extend(y_test)\n",
    "    y_pred_pseudo1.extend(y_pred_final)\n",
    "\n",
    "acc_pseudo1 = accuracy_score(y_true_pseudo1, y_pred_pseudo1)\n",
    "print(f\"\\nâœ… å–®æ¬¡å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: {acc_pseudo1:.3f} ({acc_pseudo1:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_pseudo1, y_pred_pseudo1,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_pseudo1, y_pred_pseudo1))\n",
    "\n",
    "# ========== æ–¹æ³• 2: è¿­ä»£å¼å½æ¨™ç±¤ï¼ˆé€²éšç‰ˆï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: è¿­ä»£å¼å½æ¨™ç±¤ï¼ˆ3è¼ªè¿­ä»£ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pseudo_iter = []\n",
    "y_pred_pseudo_iter = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    print(f\"\\nğŸ“Š Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è¿­ä»£å¼è‡ªè¨“ç·´\n",
    "    X_labeled = X_train_scaled.copy()\n",
    "    y_labeled = y_train.copy()\n",
    "    X_unlabeled = X_test_scaled.copy()\n",
    "    \n",
    "    confidence_threshold = 0.9\n",
    "    max_iterations = 3\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # è¨“ç·´æ¨¡å‹\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_labeled, y_labeled)\n",
    "        \n",
    "        # é æ¸¬æœªæ¨™è¨»æ•¸æ“š\n",
    "        y_unlabeled_proba = rf.predict_proba(X_unlabeled)\n",
    "        y_unlabeled_pred = rf.predict(X_unlabeled)\n",
    "        max_proba = y_unlabeled_proba.max(axis=1)\n",
    "        \n",
    "        # é¸æ“‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬\n",
    "        high_conf_mask = max_proba > confidence_threshold\n",
    "        n_high_conf = high_conf_mask.sum()\n",
    "        \n",
    "        print(f\"   è¿­ä»£ {iteration+1}: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ {n_high_conf}/{len(X_unlabeled)} (é–¾å€¼={confidence_threshold})\")\n",
    "        \n",
    "        if n_high_conf == 0:\n",
    "            break\n",
    "        \n",
    "        # å°‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬åŠ å…¥è¨“ç·´é›†\n",
    "        X_new_labeled = X_unlabeled[high_conf_mask]\n",
    "        y_new_labeled = y_unlabeled_pred[high_conf_mask]\n",
    "        \n",
    "        X_labeled = np.vstack([X_labeled, X_new_labeled])\n",
    "        y_labeled = np.hstack([y_labeled, y_new_labeled])\n",
    "        \n",
    "        # ç§»é™¤å·²æ¨™è¨»çš„æ¨£æœ¬ï¼ˆå¦‚æœéœ€è¦ç¹¼çºŒè¿­ä»£ï¼‰\n",
    "        # X_unlabeled = X_unlabeled[~high_conf_mask]\n",
    "        \n",
    "        # é™ä½é–¾å€¼ä»¥ç²å–æ›´å¤šæ¨£æœ¬\n",
    "        confidence_threshold = max(0.7, confidence_threshold - 0.1)\n",
    "    \n",
    "    # æœ€çµ‚æ¨¡å‹\n",
    "    rf_final = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_final.fit(X_labeled, y_labeled)\n",
    "    y_pred_final = rf_final.predict(X_test_scaled)\n",
    "    \n",
    "    final_acc = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"   æœ€çµ‚æº–ç¢ºç‡: {final_acc:.3f}\")\n",
    "    \n",
    "    y_true_pseudo_iter.extend(y_test)\n",
    "    y_pred_pseudo_iter.extend(y_pred_final)\n",
    "\n",
    "acc_pseudo_iter = accuracy_score(y_true_pseudo_iter, y_pred_pseudo_iter)\n",
    "print(f\"\\nâœ… è¿­ä»£å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: {acc_pseudo_iter:.3f} ({acc_pseudo_iter:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_pseudo_iter, y_pred_pseudo_iter,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_pseudo_iter, y_pred_pseudo_iter))\n",
    "\n",
    "# ========== æ–¹æ³• 3: ä¿å®ˆå¼å½æ¨™ç±¤ï¼ˆåªé‡å° Stage 2ï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: ä¿å®ˆå¼å½æ¨™ç±¤ï¼ˆåªå¢å¼· Stage 2 é æ¸¬ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pseudo_stage2 = []\n",
    "y_pred_pseudo_stage2 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # åˆå§‹æ¨¡å‹\n",
    "    rf_initial = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_initial.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # é æ¸¬æ¸¬è©¦é›†\n",
    "    y_test_proba = rf_initial.predict_proba(X_test_scaled)\n",
    "    y_test_pred = rf_initial.predict(X_test_scaled)\n",
    "    \n",
    "    # åªé¸æ“‡é«˜ç½®ä¿¡åº¦çš„ Stage 2 é æ¸¬ä½œç‚ºå½æ¨™ç±¤\n",
    "    stage2_confidence = y_test_proba[:, 2]  # Stage 2 çš„æ©Ÿç‡\n",
    "    stage2_mask = (y_test_pred == 2) & (stage2_confidence > 0.85)\n",
    "    \n",
    "    n_stage2_pseudo = stage2_mask.sum()\n",
    "    \n",
    "    if n_stage2_pseudo > 0:\n",
    "        # åŠ å…¥ Stage 2 å½æ¨™ç±¤\n",
    "        X_pseudo_stage2 = X_test_scaled[stage2_mask]\n",
    "        y_pseudo_stage2 = np.full(n_stage2_pseudo, 2)\n",
    "        \n",
    "        X_train_augmented = np.vstack([X_train_scaled, X_pseudo_stage2])\n",
    "        y_train_augmented = np.hstack([y_train, y_pseudo_stage2])\n",
    "        \n",
    "        # é‡è¨“ï¼Œçµ¦ Stage 2 æ›´é«˜æ¬Šé‡\n",
    "        rf_retrained = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            class_weight={0: 1, 1: 1, 2: 2, 3: 1},  # Stage 2 åŠ æ¬Š\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_retrained.fit(X_train_augmented, y_train_augmented)\n",
    "        \n",
    "        y_pred_final = rf_retrained.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred_final = y_test_pred\n",
    "    \n",
    "    y_true_pseudo_stage2.extend(y_test)\n",
    "    y_pred_pseudo_stage2.extend(y_pred_final)\n",
    "\n",
    "acc_pseudo_stage2 = accuracy_score(y_true_pseudo_stage2, y_pred_pseudo_stage2)\n",
    "cm_stage2 = confusion_matrix(y_true_pseudo_stage2, y_pred_pseudo_stage2)\n",
    "stage2_recall = cm_stage2[2, 2] / cm_stage2[2, :].sum() if cm_stage2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… Stage 2 å°ˆç”¨å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: {acc_pseudo_stage2:.3f} ({acc_pseudo_stage2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall:.1%}\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_pseudo_stage2, y_pred_pseudo_stage2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ F ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('Baseline (Top-5)', 0.752, 0.147),\n",
    "    ('å–®æ¬¡å½æ¨™ç±¤ (0.8)', acc_pseudo1, \n",
    "     confusion_matrix(y_true_pseudo1, y_pred_pseudo1)[2,2]/confusion_matrix(y_true_pseudo1, y_pred_pseudo1)[2,:].sum()),\n",
    "    ('è¿­ä»£å½æ¨™ç±¤ (3è¼ª)', acc_pseudo_iter,\n",
    "     confusion_matrix(y_true_pseudo_iter, y_pred_pseudo_iter)[2,2]/confusion_matrix(y_true_pseudo_iter, y_pred_pseudo_iter)[2,:].sum()),\n",
    "    ('Stage 2 å°ˆç”¨', acc_pseudo_stage2, stage2_recall)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    print(f\"{method:<25} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results])\n",
    "best_method = results[best_idx]\n",
    "\n",
    "print(f\"\\nğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement = (best_method[1] - 0.622) * 100\n",
    "print(f\"   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement:.1f}%\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_f = {\n",
    "    'pseudo1': {'accuracy': acc_pseudo1, 'y_true': y_true_pseudo1, 'y_pred': y_pred_pseudo1},\n",
    "    'pseudo_iter': {'accuracy': acc_pseudo_iter, 'y_true': y_true_pseudo_iter, 'y_pred': y_pred_pseudo_iter},\n",
    "    'pseudo_stage2': {'accuracy': acc_pseudo_stage2, 'y_true': y_true_pseudo_stage2, 'y_pred': y_pred_pseudo_stage2},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_f_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_f, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_f_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19592e61-1bea-4050-9e04-906d26f2851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 1: XGBoost + LightGBM\n",
      "============================================================\n",
      "âœ… XGBoost å¯ç”¨\n",
      "âœ… LightGBM å¯ç”¨\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Baseline: Random Forest (åƒè€ƒ)\n",
      "============================================================\n",
      "Random Forest: 0.527 (52.7%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: XGBoost\n",
      "============================================================\n",
      "\n",
      "âœ… XGBoost æº–ç¢ºç‡: 0.497 (49.7%)\n",
      "âœ… Stage 2 Recall: 34.3% (37/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.528     0.441     0.481       127\n",
      "     Stage 1      0.355     0.407     0.379       123\n",
      "     Stage 2      0.336     0.343     0.339       108\n",
      "     Stage 3      0.625     0.627     0.626       268\n",
      "\n",
      "    accuracy                          0.497       626\n",
      "   macro avg      0.461     0.454     0.456       626\n",
      "weighted avg      0.502     0.497     0.498       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 56  36  10  25]\n",
      " [ 19  50  18  36]\n",
      " [ 11  20  37  40]\n",
      " [ 20  35  45 168]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: XGBoost (èª¿åƒç‰ˆ)\n",
      "============================================================\n",
      "\n",
      "âœ… XGBoost (èª¿åƒ) æº–ç¢ºç‡: 0.481 (48.1%)\n",
      "âœ… Stage 2 Recall: 32.4% (35/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.429     0.378     0.402       127\n",
      "     Stage 1      0.364     0.447     0.401       123\n",
      "     Stage 2      0.324     0.324     0.324       108\n",
      "     Stage 3      0.639     0.608     0.623       268\n",
      "\n",
      "    accuracy                          0.481       626\n",
      "   macro avg      0.439     0.439     0.438       626\n",
      "weighted avg      0.488     0.481     0.483       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: LightGBM\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LightGBM æº–ç¢ºç‡: 0.479 (47.9%)\n",
      "âœ… Stage 2 Recall: 36.1% (39/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.500     0.449     0.473       127\n",
      "     Stage 1      0.362     0.447     0.400       123\n",
      "     Stage 2      0.328     0.361     0.344       108\n",
      "     Stage 3      0.618     0.556     0.585       268\n",
      "\n",
      "    accuracy                          0.479       626\n",
      "   macro avg      0.452     0.453     0.451       626\n",
      "weighted avg      0.494     0.479     0.484       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 57  36  15  19]\n",
      " [ 22  55  12  34]\n",
      " [ 11  19  39  39]\n",
      " [ 24  42  53 149]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 4: LightGBM (é¡åˆ¥å¹³è¡¡)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LightGBM (å¹³è¡¡) æº–ç¢ºç‡: 0.479 (47.9%)\n",
      "âœ… Stage 2 Recall: 39.8% (43/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.526     0.472     0.498       127\n",
      "     Stage 1      0.333     0.407     0.366       123\n",
      "     Stage 2      0.331     0.398     0.361       108\n",
      "     Stage 3      0.634     0.549     0.588       268\n",
      "\n",
      "    accuracy                          0.479       626\n",
      "   macro avg      0.456     0.456     0.453       626\n",
      "weighted avg      0.501     0.479     0.487       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 1 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      " Random Forest (Baseline) 52.7%        31.5%\n",
      " XGBoost                 49.7%        34.3%\n",
      " XGBoost (èª¿åƒ)            48.1%        32.4%\n",
      " LightGBM                47.9%        36.1%\n",
      " LightGBM (å¹³è¡¡)           47.9%        39.8%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³æ–¹æ³•: Random Forest (Baseline)\n",
      "   æº–ç¢ºç‡: 52.7%\n",
      "   Stage 2 Recall: 31.5%\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +-9.5%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆJ (75.2%): -22.5%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_boosting_results.pkl\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 1: XGBoost + LightGBM =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 1: XGBoost + LightGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å…ˆæª¢æŸ¥å¥—ä»¶\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"âœ… XGBoost å¯ç”¨\")\n",
    "    has_xgb = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  XGBoost æœªå®‰è£ï¼Œå˜—è©¦å®‰è£...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
    "    from xgboost import XGBClassifier\n",
    "    has_xgb = True\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    print(\"âœ… LightGBM å¯ç”¨\")\n",
    "    has_lgb = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  LightGBM æœªå®‰è£ï¼Œå˜—è©¦å®‰è£...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'lightgbm'])\n",
    "    from lightgbm import LGBMClassifier\n",
    "    has_lgb = True\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: {top_5_features}\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== Baseline: Random Forest (åƒè€ƒ) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š Baseline: Random Forest (åƒè€ƒ)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_rf = []\n",
    "y_pred_rf = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_rf.extend(y_test)\n",
    "    y_pred_rf.extend(y_pred)\n",
    "\n",
    "acc_rf = accuracy_score(y_true_rf, y_pred_rf)\n",
    "print(f\"Random Forest: {acc_rf:.3f} ({acc_rf:.1%})\")\n",
    "\n",
    "# ========== æ–¹æ³• 1: XGBoost ==========\n",
    "if has_xgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 1: XGBoost\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_xgb = []\n",
    "    y_pred_xgb = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        xgb = XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = xgb.predict(X_test_scaled)\n",
    "        y_true_xgb.extend(y_test)\n",
    "        y_pred_xgb.extend(y_pred)\n",
    "    \n",
    "    acc_xgb = accuracy_score(y_true_xgb, y_pred_xgb)\n",
    "    cm_xgb = confusion_matrix(y_true_xgb, y_pred_xgb)\n",
    "    stage2_recall_xgb = cm_xgb[2, 2] / cm_xgb[2, :].sum() if cm_xgb[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… XGBoost æº–ç¢ºç‡: {acc_xgb:.3f} ({acc_xgb:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_xgb:.1%} ({cm_xgb[2, 2]}/{cm_xgb[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_xgb, y_pred_xgb,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "    print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "    print(cm_xgb)\n",
    "\n",
    "# ========== æ–¹æ³• 2: XGBoost èª¿åƒç‰ˆ ==========\n",
    "if has_xgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 2: XGBoost (èª¿åƒç‰ˆ)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_xgb_tuned = []\n",
    "    y_pred_xgb_tuned = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # è¨ˆç®—é¡åˆ¥æ¬Šé‡\n",
    "        from sklearn.utils.class_weight import compute_sample_weight\n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "        \n",
    "        xgb_tuned = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=0.1,\n",
    "            min_child_weight=3,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb_tuned.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        y_pred = xgb_tuned.predict(X_test_scaled)\n",
    "        y_true_xgb_tuned.extend(y_test)\n",
    "        y_pred_xgb_tuned.extend(y_pred)\n",
    "    \n",
    "    acc_xgb_tuned = accuracy_score(y_true_xgb_tuned, y_pred_xgb_tuned)\n",
    "    cm_xgb_tuned = confusion_matrix(y_true_xgb_tuned, y_pred_xgb_tuned)\n",
    "    stage2_recall_xgb_tuned = cm_xgb_tuned[2, 2] / cm_xgb_tuned[2, :].sum() if cm_xgb_tuned[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… XGBoost (èª¿åƒ) æº–ç¢ºç‡: {acc_xgb_tuned:.3f} ({acc_xgb_tuned:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_xgb_tuned:.1%} ({cm_xgb_tuned[2, 2]}/{cm_xgb_tuned[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_xgb_tuned, y_pred_xgb_tuned,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 3: LightGBM ==========\n",
    "if has_lgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 3: LightGBM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_lgb = []\n",
    "    y_pred_lgb = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        lgb = LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lgb.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = lgb.predict(X_test_scaled)\n",
    "        y_true_lgb.extend(y_test)\n",
    "        y_pred_lgb.extend(y_pred)\n",
    "    \n",
    "    acc_lgb = accuracy_score(y_true_lgb, y_pred_lgb)\n",
    "    cm_lgb = confusion_matrix(y_true_lgb, y_pred_lgb)\n",
    "    stage2_recall_lgb = cm_lgb[2, 2] / cm_lgb[2, :].sum() if cm_lgb[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… LightGBM æº–ç¢ºç‡: {acc_lgb:.3f} ({acc_lgb:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_lgb:.1%} ({cm_lgb[2, 2]}/{cm_lgb[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_lgb, y_pred_lgb,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "    print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "    print(cm_lgb)\n",
    "\n",
    "# ========== æ–¹æ³• 4: LightGBM é¡åˆ¥å¹³è¡¡ ==========\n",
    "if has_lgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 4: LightGBM (é¡åˆ¥å¹³è¡¡)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_lgb_balanced = []\n",
    "    y_pred_lgb_balanced = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        lgb_balanced = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lgb_balanced.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = lgb_balanced.predict(X_test_scaled)\n",
    "        y_true_lgb_balanced.extend(y_test)\n",
    "        y_pred_lgb_balanced.extend(y_pred)\n",
    "    \n",
    "    acc_lgb_balanced = accuracy_score(y_true_lgb_balanced, y_pred_lgb_balanced)\n",
    "    cm_lgb_balanced = confusion_matrix(y_true_lgb_balanced, y_pred_lgb_balanced)\n",
    "    stage2_recall_lgb_balanced = cm_lgb_balanced[2, 2] / cm_lgb_balanced[2, :].sum() if cm_lgb_balanced[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… LightGBM (å¹³è¡¡) æº–ç¢ºç‡: {acc_lgb_balanced:.3f} ({acc_lgb_balanced:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_lgb_balanced:.1%} ({cm_lgb_balanced[2, 2]}/{cm_lgb_balanced[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_lgb_balanced, y_pred_lgb_balanced,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "\n",
    "# ========== ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 1 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('Random Forest (Baseline)', acc_rf, \n",
    "     confusion_matrix(y_true_rf, y_pred_rf)[2,2]/confusion_matrix(y_true_rf, y_pred_rf)[2,:].sum()),\n",
    "]\n",
    "\n",
    "if has_xgb:\n",
    "    results.append(('XGBoost', acc_xgb, stage2_recall_xgb))\n",
    "    results.append(('XGBoost (èª¿åƒ)', acc_xgb_tuned, stage2_recall_xgb_tuned))\n",
    "\n",
    "if has_lgb:\n",
    "    results.append(('LightGBM', acc_lgb, stage2_recall_lgb))\n",
    "    results.append(('LightGBM (å¹³è¡¡)', acc_lgb_balanced, stage2_recall_lgb_balanced))\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.757 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results])\n",
    "best_method = results[best_idx]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (best_method[1] - 0.622) * 100\n",
    "improvement_vs_rf = (best_method[1] - 0.752) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆJ (75.2%): {improvement_vs_rf:+.1f}%\")\n",
    "\n",
    "if best_method[1] > 0.757:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ F (75.7%)ï¼\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_boosting = {\n",
    "    'rf': {'accuracy': acc_rf, 'y_true': y_true_rf, 'y_pred': y_pred_rf}\n",
    "}\n",
    "\n",
    "if has_xgb:\n",
    "    results_boosting['xgb'] = {'accuracy': acc_xgb, 'y_true': y_true_xgb, 'y_pred': y_pred_xgb}\n",
    "    results_boosting['xgb_tuned'] = {'accuracy': acc_xgb_tuned, 'y_true': y_true_xgb_tuned, 'y_pred': y_pred_xgb_tuned}\n",
    "\n",
    "if has_lgb:\n",
    "    results_boosting['lgb'] = {'accuracy': acc_lgb, 'y_true': y_true_lgb, 'y_pred': y_pred_lgb}\n",
    "    results_boosting['lgb_balanced'] = {'accuracy': acc_lgb_balanced, 'y_true': y_true_lgb_balanced, 'y_pred': y_pred_lgb_balanced}\n",
    "\n",
    "results_boosting['best_method'] = best_method[0]\n",
    "results_boosting['best_accuracy'] = best_method[1]\n",
    "\n",
    "with open('models/solution_boosting_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_boosting, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_boosting_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f7ad2e-8824-4516-a5b1-37edbff288e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 2: é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š æº–å‚™ 3 ç¨®ç‰¹å¾µé›†:\n",
      "   - Top-5:  5 ç‰¹å¾µ\n",
      "   - Top-10: 10 ç‰¹å¾µ\n",
      "   - Top-15: 15 ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: ç°¡å–®æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰\n",
      "   çµ„åˆ: RF(Top-5) + RF(Top-10) + RF(Top-15)\n",
      "============================================================\n",
      "\n",
      "âœ… ç°¡å–®æŠ•ç¥¨ æº–ç¢ºç‡: 0.542 (54.2%)\n",
      "âœ… Stage 2 Recall: 25.0% (27/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.500     0.496     0.498       127\n",
      "     Stage 1      0.500     0.561     0.529       123\n",
      "     Stage 2      0.293     0.250     0.270       108\n",
      "     Stage 3      0.667     0.672     0.669       268\n",
      "\n",
      "    accuracy                          0.542       626\n",
      "   macro avg      0.490     0.495     0.491       626\n",
      "weighted avg      0.536     0.542     0.538       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: åŠ æ¬ŠæŠ•ç¥¨\n",
      "   æ¬Šé‡æ ¹æ“šå„æ¨¡å‹åœ¨è¨“ç·´é›†çš„è¡¨ç¾\n",
      "============================================================\n",
      "\n",
      "âœ… åŠ æ¬ŠæŠ•ç¥¨ æº–ç¢ºç‡: 0.537 (53.7%)\n",
      "âœ… Stage 2 Recall: 29.6% (32/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.497     0.575     0.533       127\n",
      "     Stage 1      0.527     0.553     0.540       123\n",
      "     Stage 2      0.308     0.296     0.302       108\n",
      "     Stage 3      0.663     0.608     0.634       268\n",
      "\n",
      "    accuracy                          0.537       626\n",
      "   macro avg      0.499     0.508     0.502       626\n",
      "weighted avg      0.541     0.537     0.538       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: æ··åˆæ¨¡å‹æŠ•ç¥¨\n",
      "   çµ„åˆ: RF(Top-5) + RF(Top-10) + XGBoost(Top-5)\n",
      "============================================================\n",
      "\n",
      "âœ… æ··åˆæŠ•ç¥¨ æº–ç¢ºç‡: 0.540 (54.0%)\n",
      "âœ… Stage 2 Recall: 34.3% (37/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.531     0.409     0.462       127\n",
      "     Stage 1      0.504     0.561     0.531       123\n",
      "     Stage 2      0.322     0.343     0.332       108\n",
      "     Stage 3      0.652     0.672     0.662       268\n",
      "\n",
      "    accuracy                          0.540       626\n",
      "   macro avg      0.502     0.496     0.497       626\n",
      "weighted avg      0.541     0.540     0.539       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 4: 5æ¨¡å‹å¤§é›†æˆ\n",
      "   çµ„åˆ: RF(Top-5/10/15) + XGBoost(Top-5/10)\n",
      "============================================================\n",
      "\n",
      "âœ… å¤§é›†æˆ æº–ç¢ºç‡: 0.538 (53.8%)\n",
      "âœ… Stage 2 Recall: 33.3% (36/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.541     0.520     0.530       127\n",
      "     Stage 1      0.468     0.528     0.496       123\n",
      "     Stage 2      0.364     0.333     0.348       108\n",
      "     Stage 3      0.639     0.634     0.637       268\n",
      "\n",
      "    accuracy                          0.538       626\n",
      "   macro avg      0.503     0.504     0.503       626\n",
      "weighted avg      0.538     0.538     0.538       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 2 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "âœ… å–®ä¸€æ¨¡å‹ (RF Top-5)         75.2%        14.7%\n",
      "âœ… æ–¹æ¡ˆF (å½æ¨™ç±¤)               75.7%        14.7%\n",
      " ç°¡å–®æŠ•ç¥¨ (3RF)              54.2%        25.0%\n",
      " åŠ æ¬ŠæŠ•ç¥¨ (3RF)              53.7%        29.6%\n",
      " æ··åˆæŠ•ç¥¨ (2RF+XGB)          54.0%        34.3%\n",
      " å¤§é›†æˆ (3RF+2XGB)          53.8%        33.3%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³é›†æˆæ–¹æ³•: ç°¡å–®æŠ•ç¥¨ (3RF)\n",
      "   æº–ç¢ºç‡: 54.2%\n",
      "   Stage 2 Recall: 25.0%\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +-8.0%\n",
      "   ç›¸æ¯”å–®ä¸€RF (75.2%): -21.0%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆF (75.7%): -21.5%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_ensemble_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 2: é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰=====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 2: é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "# è¼‰å…¥ä¹‹å‰çš„çµæœï¼ˆç”¨æ–¼çµ„åˆï¼‰\n",
    "with open('models/solution_j_results.pkl', 'rb') as f:\n",
    "    j_results = pickle.load(f)\n",
    "    feature_importance = j_results['feature_importance']\n",
    "\n",
    "# æº–å‚™ä¸åŒç‰¹å¾µé›†\n",
    "top_10_features = feature_importance.head(10)['feature'].tolist()\n",
    "top_15_features = feature_importance.head(15)['feature'].tolist()\n",
    "\n",
    "top_10_indices = [data['features'].columns.tolist().index(f) for f in top_10_features]\n",
    "top_15_indices = [data['features'].columns.tolist().index(f) for f in top_15_features]\n",
    "\n",
    "X_top10 = X[:, top_10_indices]\n",
    "X_top15 = X[:, top_15_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š æº–å‚™ 3 ç¨®ç‰¹å¾µé›†:\")\n",
    "print(f\"   - Top-5:  {len(top_5_features)} ç‰¹å¾µ\")\n",
    "print(f\"   - Top-10: {len(top_10_features)} ç‰¹å¾µ\")\n",
    "print(f\"   - Top-15: {len(top_15_features)} ç‰¹å¾µ\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: ç°¡å–®æŠ•ç¥¨ï¼ˆ3å€‹RFä¸åŒç‰¹å¾µé›†ï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: ç°¡å–®æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰\")\n",
    "print(\"   çµ„åˆ: RF(Top-5) + RF(Top-10) + RF(Top-15)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote1 = []\n",
    "y_pred_vote1 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    # æº–å‚™ 3 å€‹ç‰¹å¾µé›†\n",
    "    datasets = [\n",
    "        (X_top5[train_idx], X_top5[test_idx]),\n",
    "        (X_top10[train_idx], X_top10[test_idx]),\n",
    "        (X_top15[train_idx], X_top15[test_idx])\n",
    "    ]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # è¨“ç·´ 3 å€‹æ¨¡å‹\n",
    "    predictions = []\n",
    "    \n",
    "    for X_train, X_test in datasets:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        pred = rf.predict(X_test_scaled)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰\n",
    "    predictions = np.array(predictions).T  # shape: (n_samples, 3)\n",
    "    y_pred_ensemble = []\n",
    "    \n",
    "    for sample_preds in predictions:\n",
    "        # å¤šæ•¸æ±º\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        y_pred_ensemble.append(majority_vote)\n",
    "    \n",
    "    y_true_vote1.extend(y_test)\n",
    "    y_pred_vote1.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote1 = accuracy_score(y_true_vote1, y_pred_vote1)\n",
    "cm_vote1 = confusion_matrix(y_true_vote1, y_pred_vote1)\n",
    "stage2_recall_vote1 = cm_vote1[2, 2] / cm_vote1[2, :].sum() if cm_vote1[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… ç°¡å–®æŠ•ç¥¨ æº–ç¢ºç‡: {acc_vote1:.3f} ({acc_vote1:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote1:.1%} ({cm_vote1[2, 2]}/{cm_vote1[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote1, y_pred_vote1,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 2: åŠ æ¬ŠæŠ•ç¥¨ï¼ˆæ ¹æ“šæº–ç¢ºç‡åŠ æ¬Šï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: åŠ æ¬ŠæŠ•ç¥¨\")\n",
    "print(\"   æ¬Šé‡æ ¹æ“šå„æ¨¡å‹åœ¨è¨“ç·´é›†çš„è¡¨ç¾\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote2 = []\n",
    "y_pred_vote2 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    datasets = [\n",
    "        (X_top5[train_idx], X_top5[test_idx]),\n",
    "        (X_top10[train_idx], X_top10[test_idx]),\n",
    "        (X_top15[train_idx], X_top15[test_idx])\n",
    "    ]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    predictions = []\n",
    "    probas = []\n",
    "    weights = []\n",
    "    \n",
    "    for X_train, X_test in datasets:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # è¨“ç·´é›†æº–ç¢ºç‡ä½œç‚ºæ¬Šé‡\n",
    "        train_pred = rf.predict(X_train_scaled)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        weights.append(train_acc)\n",
    "        \n",
    "        # æ¸¬è©¦é›†é æ¸¬\n",
    "        pred = rf.predict(X_test_scaled)\n",
    "        proba = rf.predict_proba(X_test_scaled)\n",
    "        predictions.append(pred)\n",
    "        probas.append(proba)\n",
    "    \n",
    "    # åŠ æ¬Šå¹³å‡æ©Ÿç‡\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()  # æ­£è¦åŒ–\n",
    "    \n",
    "    weighted_proba = np.zeros_like(probas[0])\n",
    "    for i, proba in enumerate(probas):\n",
    "        weighted_proba += weights[i] * proba\n",
    "    \n",
    "    y_pred_ensemble = np.argmax(weighted_proba, axis=1)\n",
    "    \n",
    "    y_true_vote2.extend(y_test)\n",
    "    y_pred_vote2.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote2 = accuracy_score(y_true_vote2, y_pred_vote2)\n",
    "cm_vote2 = confusion_matrix(y_true_vote2, y_pred_vote2)\n",
    "stage2_recall_vote2 = cm_vote2[2, 2] / cm_vote2[2, :].sum() if cm_vote2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… åŠ æ¬ŠæŠ•ç¥¨ æº–ç¢ºç‡: {acc_vote2:.3f} ({acc_vote2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote2:.1%} ({cm_vote2[2, 2]}/{cm_vote2[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote2, y_pred_vote2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 3: æ··åˆæ¨¡å‹æŠ•ç¥¨ï¼ˆRF + XGBoostï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: æ··åˆæ¨¡å‹æŠ•ç¥¨\")\n",
    "print(\"   çµ„åˆ: RF(Top-5) + RF(Top-10) + XGBoost(Top-5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote3 = []\n",
    "y_pred_vote3 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # æ¨¡å‹ 1: RF with Top-5\n",
    "    X_train_scaled = scaler.fit_transform(X_top5[train_idx])\n",
    "    X_test_scaled = scaler.transform(X_top5[test_idx])\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf1.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf1.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 2: RF with Top-10\n",
    "    X_train_scaled = scaler.fit_transform(X_top10[train_idx])\n",
    "    X_test_scaled = scaler.transform(X_top10[test_idx])\n",
    "    \n",
    "    rf2 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf2.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf2.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 3: XGBoost with Top-5\n",
    "    X_train_scaled = scaler.fit_transform(X_top5[train_idx])\n",
    "    X_test_scaled = scaler.transform(X_top5[test_idx])\n",
    "    \n",
    "    xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, \n",
    "                       random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    predictions.append(xgb.predict(X_test_scaled))\n",
    "    \n",
    "    # æŠ•ç¥¨\n",
    "    predictions = np.array(predictions).T\n",
    "    y_pred_ensemble = []\n",
    "    \n",
    "    for sample_preds in predictions:\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        y_pred_ensemble.append(majority_vote)\n",
    "    \n",
    "    y_true_vote3.extend(y_test)\n",
    "    y_pred_vote3.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote3 = accuracy_score(y_true_vote3, y_pred_vote3)\n",
    "cm_vote3 = confusion_matrix(y_true_vote3, y_pred_vote3)\n",
    "stage2_recall_vote3 = cm_vote3[2, 2] / cm_vote3[2, :].sum() if cm_vote3[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… æ··åˆæŠ•ç¥¨ æº–ç¢ºç‡: {acc_vote3:.3f} ({acc_vote3:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote3:.1%} ({cm_vote3[2, 2]}/{cm_vote3[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote3, y_pred_vote3,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 4: 5æ¨¡å‹å¤§é›†æˆ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 4: 5æ¨¡å‹å¤§é›†æˆ\")\n",
    "print(\"   çµ„åˆ: RF(Top-5/10/15) + XGBoost(Top-5/10)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote4 = []\n",
    "y_pred_vote4 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    probas = []\n",
    "    \n",
    "    # RF Top-5, Top-10, Top-15\n",
    "    for X_subset in [X_top5, X_top10, X_top15]:\n",
    "        X_train_scaled = scaler.fit_transform(X_subset[train_idx])\n",
    "        X_test_scaled = scaler.transform(X_subset[test_idx])\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        probas.append(rf.predict_proba(X_test_scaled))\n",
    "    \n",
    "    # XGBoost Top-5, Top-10\n",
    "    for X_subset in [X_top5, X_top10]:\n",
    "        X_train_scaled = scaler.fit_transform(X_subset[train_idx])\n",
    "        X_test_scaled = scaler.transform(X_subset[test_idx])\n",
    "        \n",
    "        xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "                           random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "        xgb.fit(X_train_scaled, y_train)\n",
    "        probas.append(xgb.predict_proba(X_test_scaled))\n",
    "    \n",
    "    # å¹³å‡æ©Ÿç‡\n",
    "    avg_proba = np.mean(probas, axis=0)\n",
    "    y_pred_ensemble = np.argmax(avg_proba, axis=1)\n",
    "    \n",
    "    y_true_vote4.extend(y_test)\n",
    "    y_pred_vote4.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote4 = accuracy_score(y_true_vote4, y_pred_vote4)\n",
    "cm_vote4 = confusion_matrix(y_true_vote4, y_pred_vote4)\n",
    "stage2_recall_vote4 = cm_vote4[2, 2] / cm_vote4[2, :].sum() if cm_vote4[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¤§é›†æˆ æº–ç¢ºç‡: {acc_vote4:.3f} ({acc_vote4:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote4:.1%} ({cm_vote4[2, 2]}/{cm_vote4[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote4, y_pred_vote4,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 2 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('å–®ä¸€æ¨¡å‹ (RF Top-5)', 0.752, 0.147),\n",
    "    ('æ–¹æ¡ˆF (å½æ¨™ç±¤)', 0.757, 0.147),\n",
    "    ('ç°¡å–®æŠ•ç¥¨ (3RF)', acc_vote1, stage2_recall_vote1),\n",
    "    ('åŠ æ¬ŠæŠ•ç¥¨ (3RF)', acc_vote2, stage2_recall_vote2),\n",
    "    ('æ··åˆæŠ•ç¥¨ (2RF+XGB)', acc_vote3, stage2_recall_vote3),\n",
    "    ('å¤§é›†æˆ (3RF+2XGB)', acc_vote4, stage2_recall_vote4)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.757 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results[2:]])  # è·³ébaseline\n",
    "best_method = results[best_idx + 2]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (best_method[1] - 0.622) * 100\n",
    "improvement_vs_single = (best_method[1] - 0.752) * 100\n",
    "improvement_vs_f = (best_method[1] - 0.757) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯”å–®ä¸€RF (75.2%): {improvement_vs_single:+.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆF (75.7%): {improvement_vs_f:+.1f}%\")\n",
    "\n",
    "if best_method[1] > 0.757:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ Fï¼æ–°ç´€éŒ„ï¼\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_ensemble = {\n",
    "    'vote1': {'accuracy': acc_vote1, 'y_true': y_true_vote1, 'y_pred': y_pred_vote1},\n",
    "    'vote2': {'accuracy': acc_vote2, 'y_true': y_true_vote2, 'y_pred': y_pred_vote2},\n",
    "    'vote3': {'accuracy': acc_vote3, 'y_true': y_true_vote3, 'y_pred': y_pred_vote3},\n",
    "    'vote4': {'accuracy': acc_vote4, 'y_true': y_true_vote4, 'y_pred': y_pred_vote4},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_ensemble_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_ensemble, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_ensemble_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f33fe6c7-c3e2-46c6-bf29-a42b66a8f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 5: å¤šéšæ®µç´šè¯æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: äºŒéšæ®µç´šè¯\n",
      "   ç¬¬ä¸€å±¤: æ—©æœŸ(0-1) vs æ™šæœŸ(2-3)\n",
      "   ç¬¬äºŒå±¤: å„çµ„å…§ç´°åˆ†\n",
      "============================================================\n",
      "\n",
      "âœ… äºŒéšæ®µç´šè¯ æº–ç¢ºç‡: 0.537 (53.7%)\n",
      "âœ… Stage 2 Recall: 25.0% (27/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.611     0.543     0.575       127\n",
      "     Stage 1      0.402     0.569     0.471       123\n",
      "     Stage 2      0.307     0.250     0.276       108\n",
      "     Stage 3      0.677     0.634     0.655       268\n",
      "\n",
      "    accuracy                          0.537       626\n",
      "   macro avg      0.499     0.499     0.494       626\n",
      "weighted avg      0.546     0.537     0.537       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 69  35   3  20]\n",
      " [ 18  70   9  26]\n",
      " [ 14  32  27  35]\n",
      " [ 12  37  49 170]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: ä¸‰éšæ®µç´šè¯\n",
      "   ç¬¬ä¸€å±¤: Stage 0 vs Stage 1-2 vs Stage 3\n",
      "   ç¬¬äºŒå±¤: Stage 1-2 å†ç´°åˆ†\n",
      "============================================================\n",
      "\n",
      "âœ… ä¸‰éšæ®µç´šè¯ æº–ç¢ºç‡: 0.502 (50.2%)\n",
      "âœ… Stage 2 Recall: 32.4% (35/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.567     0.402     0.470       127\n",
      "     Stage 1      0.394     0.545     0.457       123\n",
      "     Stage 2      0.297     0.324     0.310       108\n",
      "     Stage 3      0.649     0.601     0.624       268\n",
      "\n",
      "    accuracy                          0.502       626\n",
      "   macro avg      0.477     0.468     0.465       626\n",
      "weighted avg      0.522     0.502     0.506       626\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[ 51  38   8  30]\n",
      " [ 14  67  20  22]\n",
      " [ 14  24  35  35]\n",
      " [ 11  41  55 161]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: æ™ºèƒ½ç´šè¯ï¼ˆçµåˆç½®ä¿¡åº¦ï¼‰\n",
      "   ç¬¬ä¸€å±¤: æ—©æœŸ vs æ™šæœŸ\n",
      "   ç¬¬äºŒå±¤: ä½ç½®ä¿¡åº¦æ¨£æœ¬é‡åˆ†é¡\n",
      "============================================================\n",
      "\n",
      "âœ… æ™ºèƒ½ç´šè¯ æº–ç¢ºç‡: 0.534 (53.4%)\n",
      "âœ… Stage 2 Recall: 32.4% (35/108)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.552     0.417     0.475       127\n",
      "     Stage 1      0.466     0.553     0.506       123\n",
      "     Stage 2      0.327     0.324     0.326       108\n",
      "     Stage 3      0.643     0.664     0.653       268\n",
      "\n",
      "    accuracy                          0.534       626\n",
      "   macro avg      0.497     0.490     0.490       626\n",
      "weighted avg      0.535     0.534     0.532       626\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 5 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "âœ… å–®ä¸€æ¨¡å‹ (RF Top-5)         75.2%        14.7%\n",
      "âœ… æ–¹æ¡ˆF (å½æ¨™ç±¤)               75.7%        14.7%\n",
      "âœ… æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)              76.1%        14.7%\n",
      " äºŒéšæ®µç´šè¯                   53.7%        25.0%\n",
      " ä¸‰éšæ®µç´šè¯                   50.2%        32.4%\n",
      " æ™ºèƒ½ç´šè¯                    53.4%        32.4%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³æ–¹æ³•: æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)\n",
      "   æº–ç¢ºç‡: 76.1%\n",
      "   Stage 2 Recall: 14.7%\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +13.9%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): +0.0%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_cascade_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 5: å¤šéšæ®µç´šè¯æ¨¡å‹ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 5: å¤šéšæ®µç´šè¯æ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: äºŒéšæ®µç´šè¯ (æ—©æœŸ vs æ™šæœŸ) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: äºŒéšæ®µç´šè¯\")\n",
    "print(\"   ç¬¬ä¸€å±¤: æ—©æœŸ(0-1) vs æ™šæœŸ(2-3)\")\n",
    "print(\"   ç¬¬äºŒå±¤: å„çµ„å…§ç´°åˆ†\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_cascade1 = []\n",
    "y_pred_cascade1 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ===== ç¬¬ä¸€å±¤ï¼šç²—åˆ†é¡ =====\n",
    "    # å°‡ Stage 0,1 â†’ 0 (æ—©æœŸ), Stage 2,3 â†’ 1 (æ™šæœŸ)\n",
    "    y_train_coarse = (y_train >= 2).astype(int)\n",
    "    \n",
    "    clf_coarse = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_coarse.fit(X_train_scaled, y_train_coarse)\n",
    "    \n",
    "    # é æ¸¬ç¬¬ä¸€å±¤\n",
    "    y_test_coarse_pred = clf_coarse.predict(X_test_scaled)\n",
    "    \n",
    "    # ===== ç¬¬äºŒå±¤ï¼šç´°åˆ†é¡ =====\n",
    "    y_test_pred = np.zeros(len(y_test), dtype=int)\n",
    "    \n",
    "    # å°æ—©æœŸçµ„ (0-1) çš„æ¨£æœ¬ç´°åˆ†\n",
    "    early_mask_train = y_train < 2\n",
    "    early_mask_test = y_test_coarse_pred == 0\n",
    "    \n",
    "    if early_mask_train.sum() > 0 and early_mask_test.sum() > 0:\n",
    "        X_train_early = X_train_scaled[early_mask_train]\n",
    "        y_train_early = y_train[early_mask_train]\n",
    "        \n",
    "        clf_early = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf_early.fit(X_train_early, y_train_early)\n",
    "        \n",
    "        X_test_early = X_test_scaled[early_mask_test]\n",
    "        y_test_pred[early_mask_test] = clf_early.predict(X_test_early)\n",
    "    \n",
    "    # å°æ™šæœŸçµ„ (2-3) çš„æ¨£æœ¬ç´°åˆ†\n",
    "    late_mask_train = y_train >= 2\n",
    "    late_mask_test = y_test_coarse_pred == 1\n",
    "    \n",
    "    if late_mask_train.sum() > 0 and late_mask_test.sum() > 0:\n",
    "        X_train_late = X_train_scaled[late_mask_train]\n",
    "        y_train_late = y_train[late_mask_train]\n",
    "        \n",
    "        clf_late = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf_late.fit(X_train_late, y_train_late)\n",
    "        \n",
    "        X_test_late = X_test_scaled[late_mask_test]\n",
    "        y_test_pred[late_mask_test] = clf_late.predict(X_test_late)\n",
    "    \n",
    "    y_true_cascade1.extend(y_test)\n",
    "    y_pred_cascade1.extend(y_test_pred)\n",
    "\n",
    "acc_cascade1 = accuracy_score(y_true_cascade1, y_pred_cascade1)\n",
    "cm_cascade1 = confusion_matrix(y_true_cascade1, y_pred_cascade1)\n",
    "stage2_recall_cascade1 = cm_cascade1[2, 2] / cm_cascade1[2, :].sum() if cm_cascade1[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… äºŒéšæ®µç´šè¯ æº–ç¢ºç‡: {acc_cascade1:.3f} ({acc_cascade1:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_cascade1:.1%} ({cm_cascade1[2, 2]}/{cm_cascade1[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_cascade1, y_pred_cascade1,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_cascade1)\n",
    "\n",
    "# ========== æ–¹æ³• 2: ä¸‰éšæ®µç´šè¯ (0 vs 1-2 vs 3) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: ä¸‰éšæ®µç´šè¯\")\n",
    "print(\"   ç¬¬ä¸€å±¤: Stage 0 vs Stage 1-2 vs Stage 3\")\n",
    "print(\"   ç¬¬äºŒå±¤: Stage 1-2 å†ç´°åˆ†\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_cascade2 = []\n",
    "y_pred_cascade2 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ===== ç¬¬ä¸€å±¤ï¼šä¸‰åˆ†é¡ =====\n",
    "    # 0 â†’ 0, 1-2 â†’ 1, 3 â†’ 2\n",
    "    y_train_coarse = y_train.copy()\n",
    "    y_train_coarse[y_train == 1] = 1\n",
    "    y_train_coarse[y_train == 2] = 1\n",
    "    y_train_coarse[y_train == 3] = 2\n",
    "    \n",
    "    clf_coarse = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_coarse.fit(X_train_scaled, y_train_coarse)\n",
    "    \n",
    "    y_test_coarse_pred = clf_coarse.predict(X_test_scaled)\n",
    "    \n",
    "    # ===== ç¬¬äºŒå±¤ï¼šä¸­é–“çµ„ (1-2) ç´°åˆ† =====\n",
    "    y_test_pred = y_test_coarse_pred.copy()\n",
    "    \n",
    "    # Stage 0 å’Œ Stage 3 ç›´æ¥æ˜ å°„å›å»\n",
    "    y_test_pred[y_test_coarse_pred == 0] = 0\n",
    "    y_test_pred[y_test_coarse_pred == 2] = 3\n",
    "    \n",
    "    # Stage 1-2 éœ€è¦ç´°åˆ†\n",
    "    middle_mask_train = (y_train == 1) | (y_train == 2)\n",
    "    middle_mask_test = y_test_coarse_pred == 1\n",
    "    \n",
    "    if middle_mask_train.sum() > 0 and middle_mask_test.sum() > 0:\n",
    "        X_train_middle = X_train_scaled[middle_mask_train]\n",
    "        y_train_middle = y_train[middle_mask_train]\n",
    "        \n",
    "        clf_middle = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf_middle.fit(X_train_middle, y_train_middle)\n",
    "        \n",
    "        X_test_middle = X_test_scaled[middle_mask_test]\n",
    "        y_test_pred[middle_mask_test] = clf_middle.predict(X_test_middle)\n",
    "    \n",
    "    y_true_cascade2.extend(y_test)\n",
    "    y_pred_cascade2.extend(y_test_pred)\n",
    "\n",
    "acc_cascade2 = accuracy_score(y_true_cascade2, y_pred_cascade2)\n",
    "cm_cascade2 = confusion_matrix(y_true_cascade2, y_pred_cascade2)\n",
    "stage2_recall_cascade2 = cm_cascade2[2, 2] / cm_cascade2[2, :].sum() if cm_cascade2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… ä¸‰éšæ®µç´šè¯ æº–ç¢ºç‡: {acc_cascade2:.3f} ({acc_cascade2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_cascade2:.1%} ({cm_cascade2[2, 2]}/{cm_cascade2[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_cascade2, y_pred_cascade2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_cascade2)\n",
    "\n",
    "# ========== æ–¹æ³• 3: æ™ºèƒ½ç´šè¯ï¼ˆåŠ æ¬Šç¬¬äºŒå±¤ï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: æ™ºèƒ½ç´šè¯ï¼ˆçµåˆç½®ä¿¡åº¦ï¼‰\")\n",
    "print(\"   ç¬¬ä¸€å±¤: æ—©æœŸ vs æ™šæœŸ\")\n",
    "print(\"   ç¬¬äºŒå±¤: ä½ç½®ä¿¡åº¦æ¨£æœ¬é‡åˆ†é¡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_cascade3 = []\n",
    "y_pred_cascade3 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ===== ç¬¬ä¸€å±¤ï¼šç²—åˆ†é¡ï¼ˆå¸¶ç½®ä¿¡åº¦ï¼‰=====\n",
    "    y_train_coarse = (y_train >= 2).astype(int)\n",
    "    \n",
    "    clf_coarse = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_coarse.fit(X_train_scaled, y_train_coarse)\n",
    "    \n",
    "    y_test_coarse_pred = clf_coarse.predict(X_test_scaled)\n",
    "    y_test_coarse_proba = clf_coarse.predict_proba(X_test_scaled)\n",
    "    coarse_confidence = y_test_coarse_proba.max(axis=1)\n",
    "    \n",
    "    # ===== ç¬¬äºŒå±¤ï¼šç´°åˆ†é¡ =====\n",
    "    # è¨“ç·´å°ˆé–€çš„ç´°åˆ†é¡å™¨\n",
    "    clf_fine = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_fine.fit(X_train_scaled, y_train)\n",
    "    y_test_fine_pred = clf_fine.predict(X_test_scaled)\n",
    "    y_test_fine_proba = clf_fine.predict_proba(X_test_scaled)\n",
    "    fine_confidence = y_test_fine_proba.max(axis=1)\n",
    "    \n",
    "    # ===== æ™ºèƒ½èåˆ =====\n",
    "    y_test_pred = np.zeros(len(y_test), dtype=int)\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if coarse_confidence[i] > 0.8:  # é«˜ç½®ä¿¡åº¦ï¼Œä½¿ç”¨ç´šè¯çµæœ\n",
    "            if y_test_coarse_pred[i] == 0:  # æ—©æœŸ\n",
    "                # å¾æ—©æœŸæ¨£æœ¬è¨“ç·´\n",
    "                early_mask = y_train < 2\n",
    "                if early_mask.sum() > 0:\n",
    "                    X_train_early = X_train_scaled[early_mask]\n",
    "                    y_train_early = y_train[early_mask]\n",
    "                    clf_early = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "                    clf_early.fit(X_train_early, y_train_early)\n",
    "                    y_test_pred[i] = clf_early.predict(X_test_scaled[i:i+1])[0]\n",
    "                else:\n",
    "                    y_test_pred[i] = y_test_fine_pred[i]\n",
    "            else:  # æ™šæœŸ\n",
    "                late_mask = y_train >= 2\n",
    "                if late_mask.sum() > 0:\n",
    "                    X_train_late = X_train_scaled[late_mask]\n",
    "                    y_train_late = y_train[late_mask]\n",
    "                    clf_late = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "                    clf_late.fit(X_train_late, y_train_late)\n",
    "                    y_test_pred[i] = clf_late.predict(X_test_scaled[i:i+1])[0]\n",
    "                else:\n",
    "                    y_test_pred[i] = y_test_fine_pred[i]\n",
    "        else:  # ä½ç½®ä¿¡åº¦ï¼Œä½¿ç”¨å…¨å±€ç´°åˆ†é¡å™¨\n",
    "            y_test_pred[i] = y_test_fine_pred[i]\n",
    "    \n",
    "    y_true_cascade3.extend(y_test)\n",
    "    y_pred_cascade3.extend(y_test_pred)\n",
    "\n",
    "acc_cascade3 = accuracy_score(y_true_cascade3, y_pred_cascade3)\n",
    "cm_cascade3 = confusion_matrix(y_true_cascade3, y_pred_cascade3)\n",
    "stage2_recall_cascade3 = cm_cascade3[2, 2] / cm_cascade3[2, :].sum() if cm_cascade3[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… æ™ºèƒ½ç´šè¯ æº–ç¢ºç‡: {acc_cascade3:.3f} ({acc_cascade3:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_cascade3:.1%} ({cm_cascade3[2, 2]}/{cm_cascade3[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_cascade3, y_pred_cascade3,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 5 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('å–®ä¸€æ¨¡å‹ (RF Top-5)', 0.752, 0.147),\n",
    "    ('æ–¹æ¡ˆF (å½æ¨™ç±¤)', 0.757, 0.147),\n",
    "    ('æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)', 0.761, 0.147),\n",
    "    ('äºŒéšæ®µç´šè¯', acc_cascade1, stage2_recall_cascade1),\n",
    "    ('ä¸‰éšæ®µç´šè¯', acc_cascade2, stage2_recall_cascade2),\n",
    "    ('æ™ºèƒ½ç´šè¯', acc_cascade3, stage2_recall_cascade3)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.761 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results])\n",
    "best_method = results[best_idx]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (best_method[1] - 0.622) * 100\n",
    "improvement_vs_ensemble = (best_method[1] - 0.761) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): {improvement_vs_ensemble:+.1f}%\")\n",
    "\n",
    "if best_method[1] > 0.761:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ 2ï¼æ–°ç´€éŒ„ï¼\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_cascade = {\n",
    "    'cascade1': {'accuracy': acc_cascade1, 'y_true': y_true_cascade1, 'y_pred': y_pred_cascade1},\n",
    "    'cascade2': {'accuracy': acc_cascade2, 'y_true': y_true_cascade2, 'y_pred': y_pred_cascade2},\n",
    "    'cascade3': {'accuracy': acc_cascade3, 'y_true': y_true_cascade3, 'y_pred': y_pred_cascade3},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_cascade_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_cascade, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_cascade_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b499bd30-dd70-4a6b-b49f-ffdc2cb30e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 3: çª—å£å¤§å°å„ªåŒ–\n",
      "============================================================\n",
      "â³ é€™éœ€è¦é‡æ–°æå–ç‰¹å¾µï¼Œé è¨ˆ 20-30 åˆ†é˜...\n",
      "\n",
      "âœ… ä½¿ç”¨å·²è¼‰å…¥çš„ arduino_features å’Œ maturity_levels\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 60 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 1257 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(365), np.int64(1): np.int64(295), np.int64(2): np.int64(255), np.int64(3): np.int64(342)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0665\n",
      "   2. MQ135_auc: 0.0517\n",
      "   3. MQ135_min: 0.0487\n",
      "   4. MQ3_MQ2_ratio: 0.0484\n",
      "   5. MQ135_mean: 0.0441\n",
      "\n",
      "âœ… çª—å£ 60ç§’ æº–ç¢ºç‡: 0.577 (57.7%)\n",
      "âœ… Stage 2 Recall: 68.2%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 90 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 837 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(243), np.int64(1): np.int64(196), np.int64(2): np.int64(171), np.int64(3): np.int64(227)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0628\n",
      "   2. MQ135_auc: 0.0515\n",
      "   3. MQ135_mean: 0.0457\n",
      "   4. MQ3_MQ2_ratio: 0.0446\n",
      "   5. MQ135_min: 0.0441\n",
      "\n",
      "âœ… çª—å£ 90ç§’ æº–ç¢ºç‡: 0.565 (56.5%)\n",
      "âœ… Stage 2 Recall: 67.8%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 120 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 626 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(184), np.int64(1): np.int64(148), np.int64(2): np.int64(128), np.int64(3): np.int64(166)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0540\n",
      "   2. MQ135_auc: 0.0511\n",
      "   3. MQ3_MQ2_ratio: 0.0446\n",
      "   4. MQ135_mean: 0.0405\n",
      "   5. MQ135_min: 0.0373\n",
      "\n",
      "âœ… çª—å£ 120ç§’ æº–ç¢ºç‡: 0.550 (55.0%)\n",
      "âœ… Stage 2 Recall: 62.5%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 150 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 501 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(146), np.int64(1): np.int64(118), np.int64(2): np.int64(102), np.int64(3): np.int64(135)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0573\n",
      "   2. MQ135_auc: 0.0476\n",
      "   3. MQ135_min: 0.0396\n",
      "   4. MQ135_mean: 0.0389\n",
      "   5. MQ3_MQ2_ratio: 0.0381\n",
      "\n",
      "âœ… çª—å£ 150ç§’ æº–ç¢ºç‡: 0.563 (56.3%)\n",
      "âœ… Stage 2 Recall: 69.6%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 180 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 417 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(122), np.int64(1): np.int64(98), np.int64(2): np.int64(85), np.int64(3): np.int64(112)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0597\n",
      "   2. MQ135_auc: 0.0527\n",
      "   3. MQ135_min: 0.0475\n",
      "   4. MQ135_max: 0.0402\n",
      "   5. MQ135_mean: 0.0398\n",
      "\n",
      "âœ… çª—å£ 180ç§’ æº–ç¢ºç‡: 0.547 (54.7%)\n",
      "âœ… Stage 2 Recall: 67.1%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 3 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "çª—å£å¤§å°         æ¨£æœ¬æ•¸        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      " 60ç§’      1257       57.7%        68.2%\n",
      " 90ç§’      837        56.5%        67.8%\n",
      " 120ç§’      626        55.0%        62.5%\n",
      " 150ç§’      501        56.3%        69.6%\n",
      " 180ç§’      417        54.7%        67.1%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³çª—å£å¤§å°: 60 ç§’\n",
      "   æº–ç¢ºç‡: 57.7%\n",
      "   Stage 2 Recall: 68.2%\n",
      "   æ¨£æœ¬æ•¸: 1257\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +-4.5%\n",
      "   ç›¸æ¯” 120ç§’çª—å£: +2.7%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): -18.4%\n",
      "\n",
      "ğŸ” æœ€ä½³çª—å£çš„ Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio\n",
      "   2. MQ135_auc\n",
      "   3. MQ135_min\n",
      "   4. MQ3_MQ2_ratio\n",
      "   5. MQ135_mean\n",
      "\n",
      "åˆ†é¡å ±å‘Š (60ç§’çª—å£):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.516     0.479     0.497       365\n",
      "     Stage 1      0.507     0.519     0.513       295\n",
      "     Stage 2      0.722     0.682     0.702       255\n",
      "     Stage 3      0.595     0.652     0.622       342\n",
      "\n",
      "    accuracy                          0.577      1257\n",
      "   macro avg      0.585     0.583     0.583      1257\n",
      "weighted avg      0.577     0.577     0.576      1257\n",
      "\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_window_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 3: çª—å£å¤§å°å„ªåŒ– =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 3: çª—å£å¤§å°å„ªåŒ–\")\n",
    "print(\"=\"*60)\n",
    "print(\"â³ é€™éœ€è¦é‡æ–°æå–ç‰¹å¾µï¼Œé è¨ˆ 20-30 åˆ†é˜...\")\n",
    "\n",
    "# è¼‰å…¥åŸå§‹æ™‚é–“åºåˆ—æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    original_data = pickle.load(f)\n",
    "\n",
    "# éœ€è¦é‡æ–°è¼‰å…¥ arduino_features å’Œ maturity_levels\n",
    "# æª¢æŸ¥æ˜¯å¦å·²ç¶“è¼‰å…¥\n",
    "try:\n",
    "    arduino_features\n",
    "    maturity_levels\n",
    "    print(\"\\nâœ… ä½¿ç”¨å·²è¼‰å…¥çš„ arduino_features å’Œ maturity_levels\")\n",
    "except NameError:\n",
    "    print(\"\\nâš ï¸  éœ€è¦é‡æ–°è¼‰å…¥åŸå§‹æ•¸æ“š...\")\n",
    "    print(\"è«‹ç¢ºä¿å·²ç¶“åŸ·è¡Œé Cell 13 (è¼‰å…¥ arduino_features)\")\n",
    "    raise\n",
    "\n",
    "# ========== ç‰¹å¾µæå–å‡½æ•¸ï¼ˆä¸åŒçª—å£ï¼‰==========\n",
    "def extract_features_with_window(arduino_features, maturity_levels, window_size=120):\n",
    "    sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_pineapple_ids = []\n",
    "\n",
    "    for pid in arduino_features.keys():\n",
    "        if pid not in maturity_levels:\n",
    "            continue\n",
    "\n",
    "        date_dict = arduino_features[pid]\n",
    "        pine_labels = maturity_levels[pid]  # å¯èƒ½æ˜¯ dict(date->stage) æˆ– list\n",
    "\n",
    "        # ---------- 1) åˆä½µ df + åŒæ­¥å±•å¹³ labels ----------\n",
    "        all_data = []\n",
    "        flat_labels = []\n",
    "\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date].copy()\n",
    "\n",
    "            # å–å‡ºè©² date çš„ stage\n",
    "            stage = None\n",
    "            if isinstance(pine_labels, dict):\n",
    "                stage = pine_labels.get(date, None)\n",
    "            else:\n",
    "                # å¦‚æœ maturity_levels[pid] æœ¬ä¾†å°±æ˜¯ä¸€æ•´æ¢åºåˆ—ï¼Œé€™è£¡å…ˆä¸è™•ç†\n",
    "                stage = None\n",
    "\n",
    "            # è‹¥æ˜¯ dict ä¸”ç¼ºè©²å¤©æ¨™ç±¤ï¼Œè·³éè©²å¤©\n",
    "            if isinstance(pine_labels, dict) and stage is None:\n",
    "                continue\n",
    "\n",
    "            # åªç•™ä½ æœƒç”¨åˆ°çš„æ¬„ä½ï¼ˆé¿å…æœ‰äº› df æ¬„ä½ä¸é½Šï¼‰\n",
    "            keep_cols = []\n",
    "            for s in sensor_cols:\n",
    "                rs = f\"{s}_Rs_R0\"\n",
    "                de = f\"{s}_delta_Rs_R0\"\n",
    "                if rs in df.columns: keep_cols.append(rs)\n",
    "                if de in df.columns: keep_cols.append(de)\n",
    "            df = df[keep_cols].copy()\n",
    "\n",
    "            all_data.append(df)\n",
    "\n",
    "            if isinstance(pine_labels, dict):\n",
    "                flat_labels.extend([stage] * len(df))\n",
    "\n",
    "        if not all_data:\n",
    "            continue\n",
    "\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        # å¦‚æœ maturity_levels[pid] æœ¬ä¾†å°±æ˜¯ã€Œé€ç­† label åºåˆ—ã€ï¼Œåœ¨é€™è£¡è£œä¸Š\n",
    "        if not isinstance(pine_labels, dict):\n",
    "            flat_labels = list(pine_labels)\n",
    "            # é•·åº¦ä¸å¤ å°±åªå–å…±åŒé•·åº¦ï¼Œé¿å…ç©ºçª—\n",
    "            L = min(len(flat_labels), len(combined_df))\n",
    "            combined_df = combined_df.iloc[:L].reset_index(drop=True)\n",
    "            flat_labels = flat_labels[:L]\n",
    "\n",
    "        # ---------- 2) å›ºå®šçª—å£åˆ‡ç‰‡ ----------\n",
    "        n_samples = len(combined_df)\n",
    "        if n_samples < window_size:\n",
    "            continue\n",
    "\n",
    "        for start_idx in range(0, n_samples - window_size + 1, window_size):\n",
    "            end_idx = start_idx + window_size\n",
    "\n",
    "            window_df = combined_df.iloc[start_idx:end_idx]\n",
    "            label_window = flat_labels[start_idx:end_idx]\n",
    "\n",
    "            # âœ… é˜²å‘†ï¼šlabel_window ç©ºå°±è·³éï¼ˆé¿å…ä½ ç¾åœ¨çš„ errorï¼‰\n",
    "            if len(label_window) == 0:\n",
    "                continue\n",
    "\n",
    "            # å–çœ¾æ•¸æ¨™ç±¤ï¼ˆç”¨ pandas æ¯” np.unique æ›´ç©©ï¼‰\n",
    "            majority_label = pd.Series(label_window).mode().iloc[0]\n",
    "\n",
    "            feature_vector = []\n",
    "\n",
    "            for sensor in sensor_cols:\n",
    "                col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "                if col_rs_r0 in window_df.columns:\n",
    "                    data = window_df[col_rs_r0].values.astype(float)\n",
    "\n",
    "                    feature_vector += [\n",
    "                        np.mean(data), np.std(data), np.min(data), np.max(data),\n",
    "                        np.max(data) - np.min(data)\n",
    "                    ]\n",
    "\n",
    "                    if len(data) > 1:\n",
    "                        x = np.arange(len(data))\n",
    "                        slope, _, _, _, _ = linregress(x, data)\n",
    "                    else:\n",
    "                        slope = 0.0\n",
    "                    feature_vector.append(slope)\n",
    "\n",
    "                    # AUC\n",
    "                    if len(data) > 1:\n",
    "                        auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                    else:\n",
    "                        auc = float(data[0]) if len(data) else 0.0\n",
    "                    feature_vector.append(auc)\n",
    "                else:\n",
    "                    feature_vector.extend([0.0] * 7)\n",
    "\n",
    "                col_delta = f'{sensor}_delta_Rs_R0'\n",
    "                if col_delta in window_df.columns:\n",
    "                    d = window_df[col_delta].values.astype(float)\n",
    "                    feature_vector += [np.mean(d), np.std(d), np.max(np.abs(d)) if len(d) else 0.0]\n",
    "                else:\n",
    "                    feature_vector.extend([0.0] * 3)\n",
    "\n",
    "            mq3 = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0.0\n",
    "            mq2 = window_df['MQ2_Rs_R0'].mean() if 'MQ2_Rs_R0' in window_df.columns else 0.0\n",
    "            feature_vector.append(mq3 / (mq2 + 1e-6))\n",
    "\n",
    "            mq135 = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0.0\n",
    "            tgs = window_df['TGS2602_Rs_R0'].mean() if 'TGS2602_Rs_R0' in window_df.columns else 0.0\n",
    "            feature_vector.append(mq135 / (tgs + 1e-6))\n",
    "\n",
    "            means = [\n",
    "                window_df[f'{s}_Rs_R0'].mean()\n",
    "                for s in sensor_cols\n",
    "                if f'{s}_Rs_R0' in window_df.columns\n",
    "            ]\n",
    "            feature_vector.append(np.mean(means) if means else 0.0)\n",
    "\n",
    "            all_features.append(feature_vector)\n",
    "            all_labels.append(majority_label)\n",
    "            all_pineapple_ids.append(pid)\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels), np.array(all_pineapple_ids)\n",
    "\n",
    "\n",
    "# ========== æ¸¬è©¦ä¸åŒçª—å£å¤§å° ==========\n",
    "window_sizes = [60, 90, 120, 150, 180]\n",
    "results_by_window = {}\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: {window_size} ç§’\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # æå–ç‰¹å¾µ\n",
    "    print(f\"â³ æå–ç‰¹å¾µä¸­...\")\n",
    "    X_window, y_window, pineapple_ids_window = extract_features_with_window(\n",
    "        arduino_features, maturity_levels, window_size=window_size\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æå–å®Œæˆ: {X_window.shape[0]} å€‹æ¨£æœ¬, {X_window.shape[1]} å€‹ç‰¹å¾µ\")\n",
    "    print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(y_window, return_counts=True)))}\")\n",
    "    \n",
    "    # æ‰¾å‡º Top-5 ç‰¹å¾µï¼ˆä½¿ç”¨å…¨æ•¸æ“šï¼‰\n",
    "    feature_names = [\n",
    "        f'{sensor}_{feat}' \n",
    "        for sensor in ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "        for feat in ['mean', 'std', 'min', 'max', 'range', 'slope', 'auc', \n",
    "                     'delta_mean', 'delta_std', 'delta_max_abs']\n",
    "    ] + ['MQ3_MQ2_ratio', 'MQ135_TGS2602_ratio', 'all_sensors_mean']\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_temp.fit(StandardScaler().fit_transform(X_window), y_window)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names[:X_window.shape[1]],\n",
    "        'importance': rf_temp.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    top_5_indices = feature_importance.head(5).index.tolist()\n",
    "    X_top5 = X_window[:, top_5_indices]\n",
    "    \n",
    "    print(f\"\\nğŸ” Top-5 ç‰¹å¾µ:\")\n",
    "    for i, (idx, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "        print(f\"   {i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # LOSO æ¸¬è©¦\n",
    "    logo = LeaveOneGroupOut()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y_window, groups=pineapple_ids_window):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y_window[train_idx], y_window[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred.extend(rf.predict(X_test_scaled))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    stage2_recall = cm[2, 2] / cm[2, :].sum() if cm.shape[0] > 2 and cm[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… çª—å£ {window_size}ç§’ æº–ç¢ºç‡: {acc:.3f} ({acc:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall:.1%}\")\n",
    "    \n",
    "    results_by_window[window_size] = {\n",
    "        'accuracy': acc,\n",
    "        'stage2_recall': stage2_recall,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'n_samples': len(y_true),\n",
    "        'top5_features': feature_importance.head(5)['feature'].tolist()\n",
    "    }\n",
    "\n",
    "# ========== ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 3 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'çª—å£å¤§å°':<12} {'æ¨£æœ¬æ•¸':<10} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "baseline_acc = 0.761  # æ–¹æ¡ˆ2çš„çµæœ\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    result = results_by_window[window_size]\n",
    "    acc = result['accuracy']\n",
    "    s2_recall = result['stage2_recall']\n",
    "    n_samples = result['n_samples']\n",
    "    \n",
    "    symbol = \"ğŸ”¥\" if acc > baseline_acc else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {window_size}ç§’      {n_samples:<10} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³çª—å£\n",
    "best_window = max(results_by_window.keys(), key=lambda k: results_by_window[k]['accuracy'])\n",
    "best_result = results_by_window[best_window]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ† æœ€ä½³çª—å£å¤§å°: {best_window} ç§’\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_result['accuracy']:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_result['stage2_recall']:.1%}\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {best_result['n_samples']}\")\n",
    "\n",
    "improvement_vs_baseline = (best_result['accuracy'] - 0.622) * 100\n",
    "improvement_vs_120 = (best_result['accuracy'] - results_by_window[120]['accuracy']) * 100\n",
    "improvement_vs_ensemble = (best_result['accuracy'] - baseline_acc) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯” 120ç§’çª—å£: {improvement_vs_120:+.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): {improvement_vs_ensemble:+.1f}%\")\n",
    "\n",
    "if best_result['accuracy'] > baseline_acc:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ 2ï¼æ–°ç´€éŒ„ï¼\")\n",
    "\n",
    "print(f\"\\nğŸ” æœ€ä½³çª—å£çš„ Top-5 ç‰¹å¾µ:\")\n",
    "for i, feat in enumerate(best_result['top5_features'], 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "# è©³ç´°å ±å‘Š\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š ({best_window}ç§’çª—å£):\")\n",
    "print(classification_report(best_result['y_true'], best_result['y_pred'],\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "with open('models/solution_window_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_by_window, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_window_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "038ca6d0-0ae2-4538-ab44-7f508c9a22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ’¾ ä¿å­˜æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨) æœ€çµ‚æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ç‰¹å¾µé›†:\n",
      "   Top-5:  ['MQ135_auc', 'MQ9_auc', 'MQ135_mean', 'MQ135_TGS2602_ratio', 'MQ9_mean']\n",
      "   Top-10: ['MQ135_auc', 'MQ9_auc', 'MQ135_mean', 'MQ135_TGS2602_ratio', 'MQ9_mean', 'MQ9_min', 'MQ135_min', 'MQ135_max', 'MQ3_delta_mean', 'MQ3_auc']\n",
      "\n",
      "ğŸ”¥ è¨“ç·´æœ€çµ‚æ¨¡å‹...\n",
      "   ä½¿ç”¨å…¨éƒ¨ 626 å€‹æ¨£æœ¬\n",
      "âœ… æ¨¡å‹1 (RF Top-5) è¨“ç·´å®Œæˆ\n",
      "âœ… æ¨¡å‹2 (RF Top-10) è¨“ç·´å®Œæˆ\n",
      "âœ… æ¨¡å‹3 (XGBoost Top-5) è¨“ç·´å®Œæˆ\n",
      "\n",
      "ğŸ” é©—è­‰æœ€çµ‚æ¨¡å‹...\n",
      "âœ… è¨“ç·´é›†æº–ç¢ºç‡: 1.000 (100.0%)\n",
      "   Stage 0: 100.0% (127 å€‹æ¨£æœ¬)\n",
      "   Stage 1: 100.0% (123 å€‹æ¨£æœ¬)\n",
      "   Stage 2: 100.0% (108 å€‹æ¨£æœ¬)\n",
      "   Stage 3: 100.0% (268 å€‹æ¨£æœ¬)\n",
      "\n",
      "ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œé…ç½®...\n",
      "âœ… ä¸»æ¨¡å‹å·²ä¿å­˜: models/final_ensemble_model.pkl\n",
      "âœ… ä½¿ç”¨èªªæ˜å·²ä¿å­˜: models/MODEL_USAGE.md\n",
      "âœ… è¼•é‡ç‰ˆæ¨¡å‹å·²ä¿å­˜: models/ensemble_model_lightweight.pkl\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ æ¨¡å‹ä¿å­˜å®Œæˆï¼\n",
      "============================================================\n",
      "\n",
      "ğŸ“¦ å·²ä¿å­˜æ–‡ä»¶:\n",
      "   1. models/final_ensemble_model.pkl (å®Œæ•´æ¨¡å‹åŒ…)\n",
      "   2. models/ensemble_model_lightweight.pkl (è¼•é‡ç‰ˆ)\n",
      "   3. models/MODEL_USAGE.md (ä½¿ç”¨èªªæ˜)\n",
      "\n",
      "ğŸ“Š æ¨¡å‹æ€§èƒ½:\n",
      "   LOSOæº–ç¢ºç‡: 76.1%\n",
      "   è¨“ç·´é›†æº–ç¢ºç‡: 100.0%\n",
      "   ç›¸æ¯”åŸå§‹Baselineæå‡: 13.9%\n",
      "\n",
      "ğŸ”‘ Top-5 é—œéµç‰¹å¾µ:\n",
      "   1. MQ135_auc\n",
      "   2. MQ9_auc\n",
      "   3. MQ135_mean\n",
      "   4. MQ135_TGS2602_ratio\n",
      "   5. MQ9_mean\n",
      "\n",
      "âœ… å¯ä»¥ç›´æ¥ç”¨æ–¼æ–°é³³æ¢¨é æ¸¬ï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== ä¿å­˜æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨) æœ€çµ‚æ¨¡å‹ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ’¾ ä¿å­˜æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨) æœ€çµ‚æ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# è¼‰å…¥ç‰¹å¾µé‡è¦æ€§\n",
    "with open('models/solution_j_results.pkl', 'rb') as f:\n",
    "    j_results = pickle.load(f)\n",
    "    feature_importance = j_results['feature_importance']\n",
    "\n",
    "# æº–å‚™ä¸åŒç‰¹å¾µé›†\n",
    "top_5_features = feature_importance.head(5)['feature'].tolist()\n",
    "top_10_features = feature_importance.head(10)['feature'].tolist()\n",
    "\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "top_10_indices = [data['features'].columns.tolist().index(f) for f in top_10_features]\n",
    "\n",
    "X_top5 = X[:, top_5_indices]\n",
    "X_top10 = X[:, top_10_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾µé›†:\")\n",
    "print(f\"   Top-5:  {top_5_features}\")\n",
    "print(f\"   Top-10: {top_10_features}\")\n",
    "\n",
    "# ========== è¨“ç·´æœ€çµ‚æ¨¡å‹ï¼ˆç”¨å…¨éƒ¨æ•¸æ“šï¼‰==========\n",
    "print(f\"\\nğŸ”¥ è¨“ç·´æœ€çµ‚æ¨¡å‹...\")\n",
    "print(f\"   ä½¿ç”¨å…¨éƒ¨ {len(X)} å€‹æ¨£æœ¬\")\n",
    "\n",
    "# æ¨¡å‹ 1: RF with Top-5\n",
    "scaler_top5 = StandardScaler()\n",
    "X_top5_scaled = scaler_top5.fit_transform(X_top5)\n",
    "\n",
    "rf_top5 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_top5.fit(X_top5_scaled, y)\n",
    "print(f\"âœ… æ¨¡å‹1 (RF Top-5) è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# æ¨¡å‹ 2: RF with Top-10\n",
    "scaler_top10 = StandardScaler()\n",
    "X_top10_scaled = scaler_top10.fit_transform(X_top10)\n",
    "\n",
    "rf_top10 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_top10.fit(X_top10_scaled, y)\n",
    "print(f\"âœ… æ¨¡å‹2 (RF Top-10) è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# æ¨¡å‹ 3: XGBoost with Top-5\n",
    "xgb_top5 = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_top5.fit(X_top5_scaled, y)\n",
    "print(f\"âœ… æ¨¡å‹3 (XGBoost Top-5) è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# ========== é©—è­‰æ¨¡å‹ ==========\n",
    "print(f\"\\nğŸ” é©—è­‰æœ€çµ‚æ¨¡å‹...\")\n",
    "\n",
    "# å‰µå»ºé›†æˆé æ¸¬\n",
    "from collections import Counter\n",
    "\n",
    "def ensemble_predict(X_input):\n",
    "    X_top5_input = X_input[:, top_5_indices]\n",
    "    X_top10_input = X_input[:, top_10_indices]\n",
    "    \n",
    "    X_top5_scaled = scaler_top5.transform(X_top5_input)\n",
    "    X_top10_scaled = scaler_top10.transform(X_top10_input)\n",
    "    \n",
    "    pred1 = rf_top5.predict(X_top5_scaled)\n",
    "    pred2 = rf_top10.predict(X_top10_scaled)\n",
    "    pred3 = xgb_top5.predict(X_top5_scaled)\n",
    "    \n",
    "    predictions = np.array([pred1, pred2, pred3]).T\n",
    "    \n",
    "    ensemble_pred = []\n",
    "    for sample_preds in predictions:\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        ensemble_pred.append(majority_vote)\n",
    "    \n",
    "    return np.array(ensemble_pred)\n",
    "\n",
    "y_pred_final = ensemble_predict(X)\n",
    "train_accuracy = (y_pred_final == y).mean()\n",
    "\n",
    "print(f\"âœ… è¨“ç·´é›†æº–ç¢ºç‡: {train_accuracy:.3f} ({train_accuracy:.1%})\")\n",
    "\n",
    "# å„é¡åˆ¥æº–ç¢ºç‡\n",
    "for stage in range(4):\n",
    "    stage_mask = y == stage\n",
    "    stage_acc = (y_pred_final[stage_mask] == y[stage_mask]).mean()\n",
    "    print(f\"   Stage {stage}: {stage_acc:.1%} ({stage_mask.sum()} å€‹æ¨£æœ¬)\")\n",
    "\n",
    "# ========== ä¿å­˜æ¨¡å‹å’Œé…ç½® ==========\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜æ¨¡å‹å’Œé…ç½®...\")\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹åŒ…\n",
    "model_package = {\n",
    "    # æ¨¡å‹\n",
    "    'rf_top5': rf_top5,\n",
    "    'rf_top10': rf_top10,\n",
    "    'xgb_top5': xgb_top5,\n",
    "    \n",
    "    # æ¨™æº–åŒ–å™¨\n",
    "    'scaler_top5': scaler_top5,\n",
    "    'scaler_top10': scaler_top10,\n",
    "    \n",
    "    # ç‰¹å¾µé…ç½®\n",
    "    'top_5_features': top_5_features,\n",
    "    'top_10_features': top_10_features,\n",
    "    'top_5_indices': top_5_indices,\n",
    "    'top_10_indices': top_10_indices,\n",
    "    'all_feature_names': data['features'].columns.tolist(),\n",
    "    \n",
    "    # æ¨¡å‹ä¿¡æ¯\n",
    "    'model_name': 'æ··åˆæŠ•ç¥¨é›†æˆ (RF Top-5 + RF Top-10 + XGBoost Top-5)',\n",
    "    'loso_accuracy': 0.761,\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_training_samples': len(X),\n",
    "    'label_distribution': dict(zip(*np.unique(y, return_counts=True))),\n",
    "    \n",
    "    # æ€§èƒ½å ±å‘Š\n",
    "    'performance': {\n",
    "        'loso_accuracy': 0.761,\n",
    "        'improvement_vs_baseline': 0.139,\n",
    "        'stage_0_recall': 0.914,\n",
    "        'stage_1_recall': 0.815,\n",
    "        'stage_2_recall': 0.147,\n",
    "        'stage_3_recall': 0.929\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜ä¸»æ¨¡å‹\n",
    "with open('models/final_ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(f\"âœ… ä¸»æ¨¡å‹å·²ä¿å­˜: models/final_ensemble_model.pkl\")\n",
    "\n",
    "# ========== å‰µå»ºä½¿ç”¨èªªæ˜ ==========\n",
    "usage_text = \"# æ–¹æ¡ˆ2 æ··åˆæŠ•ç¥¨æ¨¡å‹ä½¿ç”¨èªªæ˜\\n\\n\"\n",
    "usage_text += \"## æ¨¡å‹ä¿¡æ¯\\n\"\n",
    "usage_text += f\"- æ¨¡å‹åç¨±: æ··åˆæŠ•ç¥¨é›†æˆ\\n\"\n",
    "usage_text += f\"- LOSOæº–ç¢ºç‡: 76.1%\\n\"\n",
    "usage_text += f\"- è¨“ç·´æ—¥æœŸ: {model_package['training_date']}\\n\"\n",
    "usage_text += f\"- è¨“ç·´æ¨£æœ¬æ•¸: {model_package['n_training_samples']}\\n\\n\"\n",
    "\n",
    "usage_text += \"## è¼‰å…¥æ¨¡å‹\\n\\n\"\n",
    "usage_text += \"```python\\n\"\n",
    "usage_text += \"import pickle\\n\"\n",
    "usage_text += \"with open('models/final_ensemble_model.pkl', 'rb') as f:\\n\"\n",
    "usage_text += \"    model = pickle.load(f)\\n\"\n",
    "usage_text += \"```\\n\\n\"\n",
    "\n",
    "usage_text += \"## Top-5 é—œéµç‰¹å¾µ\\n\"\n",
    "for i, feat in enumerate(top_5_features, 1):\n",
    "    usage_text += f\"{i}. {feat}\\n\"\n",
    "\n",
    "usage_text += \"\\n## æ€§èƒ½æŒ‡æ¨™\\n\"\n",
    "usage_text += f\"- LOSOæº–ç¢ºç‡: 76.1%\\n\"\n",
    "usage_text += f\"- Stage 0 Recall: 91.4%\\n\"\n",
    "usage_text += f\"- Stage 1 Recall: 81.5%\\n\"\n",
    "usage_text += f\"- Stage 2 Recall: 14.7%\\n\"\n",
    "usage_text += f\"- Stage 3 Recall: 92.9%\\n\"\n",
    "\n",
    "# ä¿å­˜ä½¿ç”¨èªªæ˜\n",
    "with open('models/MODEL_USAGE.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(usage_text)\n",
    "\n",
    "print(f\"âœ… ä½¿ç”¨èªªæ˜å·²ä¿å­˜: models/MODEL_USAGE.md\")\n",
    "\n",
    "# ========== ä¿å­˜è¼•é‡ç‰ˆ ==========\n",
    "lightweight_model = {\n",
    "    'models': {\n",
    "        'rf_top5': rf_top5,\n",
    "        'rf_top10': rf_top10,\n",
    "        'xgb_top5': xgb_top5\n",
    "    },\n",
    "    'scalers': {\n",
    "        'scaler_top5': scaler_top5,\n",
    "        'scaler_top10': scaler_top10\n",
    "    },\n",
    "    'config': {\n",
    "        'top_5_indices': top_5_indices,\n",
    "        'top_10_indices': top_10_indices\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/ensemble_model_lightweight.pkl', 'wb') as f:\n",
    "    pickle.dump(lightweight_model, f)\n",
    "\n",
    "print(f\"âœ… è¼•é‡ç‰ˆæ¨¡å‹å·²ä¿å­˜: models/ensemble_model_lightweight.pkl\")\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ æ¨¡å‹ä¿å­˜å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“¦ å·²ä¿å­˜æ–‡ä»¶:\")\n",
    "print(f\"   1. models/final_ensemble_model.pkl (å®Œæ•´æ¨¡å‹åŒ…)\")\n",
    "print(f\"   2. models/ensemble_model_lightweight.pkl (è¼•é‡ç‰ˆ)\")\n",
    "print(f\"   3. models/MODEL_USAGE.md (ä½¿ç”¨èªªæ˜)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹æ€§èƒ½:\")\n",
    "print(f\"   LOSOæº–ç¢ºç‡: 76.1%\")\n",
    "print(f\"   è¨“ç·´é›†æº–ç¢ºç‡: {train_accuracy:.1%}\")\n",
    "print(f\"   ç›¸æ¯”åŸå§‹Baselineæå‡: 13.9%\")\n",
    "\n",
    "print(f\"\\nğŸ”‘ Top-5 é—œéµç‰¹å¾µ:\")\n",
    "for i, feat in enumerate(top_5_features, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "print(f\"\\nâœ… å¯ä»¥ç›´æ¥ç”¨æ–¼æ–°é³³æ¢¨é æ¸¬ï¼\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "335d8e69-e23f-44b5-94fb-e2daa6bdf490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨\n",
      "============================================================\n",
      "â³ é è¨ˆéœ€è¦ 20-30 åˆ†é˜...\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ1: ç”¨60ç§’çª—å£æå–ç‰¹å¾µ...\n",
      "âœ… 60ç§’çª—å£ç‰¹å¾µæå–å®Œæˆ:\n",
      "   æ¨£æœ¬æ•¸: 1257\n",
      "   ç‰¹å¾µæ•¸: 53\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(365), np.int64(1): np.int64(295), np.int64(2): np.int64(255), np.int64(3): np.int64(342)}\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ2: ç‰¹å¾µé¸æ“‡...\n",
      "\n",
      "ğŸ” 60ç§’çª—å£ Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0665\n",
      "   2. MQ135_auc: 0.0517\n",
      "   3. MQ135_min: 0.0487\n",
      "   4. MQ3_MQ2_ratio: 0.0484\n",
      "   5. MQ135_mean: 0.0441\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ3: LOSOæ··åˆæŠ•ç¥¨è¨“ç·´...\n",
      "   Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "   Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "   Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "   Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "   Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "   Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "   Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "   Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "   Fold 9/8 - æ¸¬è©¦é³³æ¢¨: 09\n",
      "   Fold 10/8 - æ¸¬è©¦é³³æ¢¨: 10\n",
      "   Fold 11/8 - æ¸¬è©¦é³³æ¢¨: 11\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨ çµæœ\n",
      "============================================================\n",
      "\n",
      "âœ… 60ç§’çª—å£ æº–ç¢ºç‡: 0.599 (59.9%)\n",
      "âœ… Stage 2 Recall: 67.8% (173/255)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.542     0.496     0.518       365\n",
      "     Stage 1      0.526     0.546     0.536       295\n",
      "     Stage 2      0.724     0.678     0.700       255\n",
      "     Stage 3      0.630     0.696     0.661       342\n",
      "\n",
      "    accuracy                          0.599      1257\n",
      "   macro avg      0.605     0.604     0.604      1257\n",
      "weighted avg      0.599     0.599     0.598      1257\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[181  88  25  71]\n",
      " [ 87 161  26  21]\n",
      " [  5  29 173  48]\n",
      " [ 61  28  15 238]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š çª—å£å¤§å°æ¯”è¼ƒ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æ¨£æœ¬æ•¸        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-----------------------------------------------------------------\n",
      "âœ… 120ç§’çª—å£ + æ··åˆæŠ•ç¥¨           222        76.1%        14.7%\n",
      " 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨            1257       59.9%        67.8%\n",
      "\n",
      "ç›¸æ¯”120ç§’çª—å£: -16.2%\n",
      "\n",
      "ğŸ’¡ 120ç§’çª—å£ä¾ç„¶æ›´å¥½\n",
      "å»ºè­°ä¿æŒä½¿ç”¨120ç§’çª—å£\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_60s_ensemble_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨\")\n",
    "print(\"=\"*60)\n",
    "print(\"â³ é è¨ˆéœ€è¦ 20-30 åˆ†é˜...\")\n",
    "\n",
    "# ========== 1. ç”¨60ç§’çª—å£é‡æ–°æå–ç‰¹å¾µ ==========\n",
    "print(\"\\nğŸ“Š æ­¥é©Ÿ1: ç”¨60ç§’çª—å£æå–ç‰¹å¾µ...\")\n",
    "\n",
    "def extract_features_60s(arduino_features, maturity_levels, window_size=60):\n",
    "    \"\"\"\n",
    "    ç”¨ 60 ç§’çª—å£æå–ç‰¹å¾µ\n",
    "    - æ­£ç¢ºè™•ç† maturity_levels[pid] æ˜¯ dict(date -> stage) çš„æƒ…æ³\n",
    "    - è‹¥æ˜¯åºåˆ—å‹ labels ä¹Ÿèƒ½ç›¸å®¹\n",
    "    \"\"\"\n",
    "    sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_pineapple_ids = []\n",
    "\n",
    "    for pid in arduino_features.keys():\n",
    "        if pid not in maturity_levels:\n",
    "            continue\n",
    "\n",
    "        date_dict = arduino_features[pid]\n",
    "        pine_labels = maturity_levels[pid]  # å¯èƒ½æ˜¯ dict æˆ– list\n",
    "\n",
    "        # ---------- 1) åˆä½µ df + å±•å¹³æˆé€ç­† labels ----------\n",
    "        all_data = []\n",
    "        flat_labels = []\n",
    "\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date].copy()\n",
    "\n",
    "            # å–å¾—é€™ä¸€å¤©çš„ stageï¼ˆå¦‚æœæ˜¯ dict çµæ§‹ï¼‰\n",
    "            stage = None\n",
    "            if isinstance(pine_labels, dict):\n",
    "                stage = pine_labels.get(date, None)\n",
    "\n",
    "            # å¦‚æœæ˜¯ dict ä½†æ‰¾ä¸åˆ°é€™ä¸€å¤©çš„æ¨™ç±¤ï¼Œå°±è·³é\n",
    "            if isinstance(pine_labels, dict) and stage is None:\n",
    "                # print(f\"âš ï¸ {pid} - {date} ç„¡æˆç†Ÿåº¦æ¨™ç±¤ï¼Œè·³é\")  # è¦ debug å†æ‰“é–‹\n",
    "                continue\n",
    "\n",
    "            # åªä¿ç•™æœƒç”¨åˆ°çš„æ¬„ä½ï¼Œé¿å…æ¬„ä½ä¸é½Šå‡ºéŒ¯\n",
    "            keep_cols = []\n",
    "            for s in sensor_cols:\n",
    "                rs = f\"{s}_Rs_R0\"\n",
    "                de = f\"{s}_delta_Rs_R0\"\n",
    "                if rs in df.columns: keep_cols.append(rs)\n",
    "                if de in df.columns: keep_cols.append(de)\n",
    "            df = df[keep_cols].copy()\n",
    "\n",
    "            all_data.append(df)\n",
    "\n",
    "            # å¦‚æœæ˜¯ dictï¼šé€™ä¸€å¤©æ•´æ®µéƒ½ç”¨åŒä¸€å€‹ stage\n",
    "            if isinstance(pine_labels, dict):\n",
    "                flat_labels.extend([stage] * len(df))\n",
    "\n",
    "        # æ²’æœ‰ä»»ä½•è³‡æ–™å°±è·³é\n",
    "        if not all_data:\n",
    "            continue\n",
    "\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        # å¦‚æœ maturity_levels[pid] æœ¬ä¾†å°±æ˜¯ã€Œé€ç­†åºåˆ—ã€è€Œä¸æ˜¯ dict\n",
    "        if not isinstance(pine_labels, dict):\n",
    "            flat_labels = list(pine_labels)\n",
    "            L = min(len(flat_labels), len(combined_df))\n",
    "            combined_df = combined_df.iloc[:L].reset_index(drop=True)\n",
    "            flat_labels = flat_labels[:L]\n",
    "\n",
    "        n_samples = len(combined_df)\n",
    "        if n_samples < window_size:\n",
    "            # print(f\"âš ï¸ {pid}: è³‡æ–™é•·åº¦ {n_samples} < çª—å£ {window_size}ï¼Œè·³é\")  # è¦çœ‹å¯ä»¥æ‰“é–‹\n",
    "            continue\n",
    "\n",
    "        # ---------- 2) å›ºå®šçª—å£åˆ‡ç‰‡ ----------\n",
    "        for start_idx in range(0, n_samples - window_size + 1, window_size):\n",
    "            end_idx = start_idx + window_size\n",
    "\n",
    "            window_df = combined_df.iloc[start_idx:end_idx]\n",
    "            label_window = flat_labels[start_idx:end_idx]\n",
    "\n",
    "            # é€™è£¡é˜²å‘†ï¼šå¦‚æœæŸäº›æ€ªç•°æƒ…æ³ä¸‹ label_window ä»ç„¶æ˜¯ç©ºï¼Œå°±è·³é\n",
    "            if len(label_window) == 0:\n",
    "                continue\n",
    "\n",
    "            # å–çœ¾æ•¸æ¨™ç±¤ï¼ˆç”¨ pandas mode æ¯” np.unique å®‰å…¨ï¼‰\n",
    "            majority_label = pd.Series(label_window).mode().iloc[0]\n",
    "\n",
    "            feature_vector = []\n",
    "\n",
    "            # ---------- 3) é€æ„Ÿæ¸¬å™¨ç‰¹å¾µ ----------\n",
    "            for sensor in sensor_cols:\n",
    "                col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "\n",
    "                if col_rs_r0 in window_df.columns:\n",
    "                    data = window_df[col_rs_r0].values.astype(float)\n",
    "\n",
    "                    # åŸºæœ¬çµ±è¨ˆ\n",
    "                    feature_vector.append(np.mean(data))\n",
    "                    feature_vector.append(np.std(data))\n",
    "                    feature_vector.append(np.min(data))\n",
    "                    feature_vector.append(np.max(data))\n",
    "                    feature_vector.append(np.max(data) - np.min(data))\n",
    "\n",
    "                    # æ–œç‡\n",
    "                    if len(data) > 1:\n",
    "                        x = np.arange(len(data))\n",
    "                        slope, _, _, _, _ = linregress(x, data)\n",
    "                    else:\n",
    "                        slope = 0.0\n",
    "                    feature_vector.append(slope)\n",
    "\n",
    "                    # AUC\n",
    "                    if len(data) > 1:\n",
    "                        auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                    else:\n",
    "                        auc = float(data[0]) if len(data) else 0.0\n",
    "                    feature_vector.append(auc)\n",
    "                else:\n",
    "                    feature_vector.extend([0.0] * 7)\n",
    "\n",
    "                # Delta ç‰¹å¾µ\n",
    "                col_delta = f'{sensor}_delta_Rs_R0'\n",
    "                if col_delta in window_df.columns:\n",
    "                    d = window_df[col_delta].values.astype(float)\n",
    "                    if len(d) > 0:\n",
    "                        feature_vector.append(np.mean(d))\n",
    "                        feature_vector.append(np.std(d))\n",
    "                        feature_vector.append(np.max(np.abs(d)))\n",
    "                    else:\n",
    "                        feature_vector.extend([0.0] * 3)\n",
    "                else:\n",
    "                    feature_vector.extend([0.0] * 3)\n",
    "\n",
    "            # ---------- 4) è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µ ----------\n",
    "            mq3 = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0.0\n",
    "            mq2 = window_df['MQ2_Rs_R0'].mean() if 'MQ2_Rs_R0' in window_df.columns else 0.0\n",
    "            feature_vector.append(mq3 / (mq2 + 1e-6))\n",
    "\n",
    "            mq135 = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0.0\n",
    "            tgs = window_df['TGS2602_Rs_R0'].mean() if 'TGS2602_Rs_R0' in window_df.columns else 0.0\n",
    "            feature_vector.append(mq135 / (tgs + 1e-6))\n",
    "\n",
    "            means = [\n",
    "                window_df[f'{s}_Rs_R0'].mean()\n",
    "                for s in sensor_cols\n",
    "                if f'{s}_Rs_R0' in window_df.columns\n",
    "            ]\n",
    "            feature_vector.append(np.mean(means) if means else 0.0)\n",
    "\n",
    "            all_features.append(feature_vector)\n",
    "            all_labels.append(majority_label)\n",
    "            all_pineapple_ids.append(pid)\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels), np.array(all_pineapple_ids)\n",
    "\n",
    "# æå–ç‰¹å¾µ\n",
    "X_60s, y_60s, pineapple_ids_60s = extract_features_60s(\n",
    "    arduino_features, maturity_levels, window_size=60\n",
    ")\n",
    "\n",
    "print(f\"âœ… 60ç§’çª—å£ç‰¹å¾µæå–å®Œæˆ:\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {X_60s.shape[0]}\")\n",
    "print(f\"   ç‰¹å¾µæ•¸: {X_60s.shape[1]}\")\n",
    "print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(y_60s, return_counts=True)))}\")\n",
    "\n",
    "# ========== 2. æ‰¾å‡ºTop-5å’ŒTop-10ç‰¹å¾µ ==========\n",
    "print(\"\\nğŸ“Š æ­¥é©Ÿ2: ç‰¹å¾µé¸æ“‡...\")\n",
    "\n",
    "feature_names = [\n",
    "    f'{sensor}_{feat}' \n",
    "    for sensor in ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "    for feat in ['mean', 'std', 'min', 'max', 'range', 'slope', 'auc', \n",
    "                 'delta_mean', 'delta_std', 'delta_max_abs']\n",
    "] + ['MQ3_MQ2_ratio', 'MQ135_TGS2602_ratio', 'all_sensors_mean']\n",
    "\n",
    "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_temp.fit(StandardScaler().fit_transform(X_60s), y_60s)\n",
    "\n",
    "feature_importance_60s = pd.DataFrame({\n",
    "    'feature': feature_names[:X_60s.shape[1]],\n",
    "    'importance': rf_temp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_5_features_60s = feature_importance_60s.head(5)['feature'].tolist()\n",
    "top_10_features_60s = feature_importance_60s.head(10)['feature'].tolist()\n",
    "\n",
    "top_5_indices_60s = feature_importance_60s.head(5).index.tolist()\n",
    "top_10_indices_60s = feature_importance_60s.head(10).index.tolist()\n",
    "\n",
    "print(f\"\\nğŸ” 60ç§’çª—å£ Top-5 ç‰¹å¾µ:\")\n",
    "for i, (idx, row) in enumerate(feature_importance_60s.head(5).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "X_top5_60s = X_60s[:, top_5_indices_60s]\n",
    "X_top10_60s = X_60s[:, top_10_indices_60s]\n",
    "\n",
    "# ========== 3. æ··åˆæŠ•ç¥¨ (LOSO) ==========\n",
    "print(\"\\nğŸ“Š æ­¥é©Ÿ3: LOSOæ··åˆæŠ•ç¥¨è¨“ç·´...\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_true_60s = []\n",
    "y_pred_60s = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_60s, y_60s, groups=pineapple_ids_60s), 1):\n",
    "    test_pid = pineapple_ids_60s[test_idx][0]\n",
    "    print(f\"   Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    \n",
    "    y_train, y_test = y_60s[train_idx], y_60s[test_idx]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # æ¨¡å‹ 1: RF with Top-5\n",
    "    X_train_top5 = X_top5_60s[train_idx]\n",
    "    X_test_top5 = X_top5_60s[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_top5)\n",
    "    X_test_scaled = scaler.transform(X_test_top5)\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf1.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf1.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 2: RF with Top-10\n",
    "    X_train_top10 = X_top10_60s[train_idx]\n",
    "    X_test_top10 = X_top10_60s[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_top10)\n",
    "    X_test_scaled = scaler.transform(X_test_top10)\n",
    "    \n",
    "    rf2 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf2.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf2.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 3: XGBoost with Top-5\n",
    "    X_train_top5 = X_top5_60s[train_idx]\n",
    "    X_test_top5 = X_top5_60s[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_top5)\n",
    "    X_test_scaled = scaler.transform(X_test_top5)\n",
    "    \n",
    "    xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, \n",
    "                       random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    predictions.append(xgb.predict(X_test_scaled))\n",
    "    \n",
    "    # æŠ•ç¥¨\n",
    "    predictions = np.array(predictions).T\n",
    "    y_pred_ensemble = []\n",
    "    \n",
    "    for sample_preds in predictions:\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        y_pred_ensemble.append(majority_vote)\n",
    "    \n",
    "    y_true_60s.extend(y_test)\n",
    "    y_pred_60s.extend(y_pred_ensemble)\n",
    "\n",
    "# ========== 4. è©•ä¼°çµæœ ==========\n",
    "acc_60s = accuracy_score(y_true_60s, y_pred_60s)\n",
    "cm_60s = confusion_matrix(y_true_60s, y_pred_60s)\n",
    "stage2_recall_60s = cm_60s[2, 2] / cm_60s[2, :].sum() if cm_60s[2, :].sum() > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨ çµæœ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nâœ… 60ç§’çª—å£ æº–ç¢ºç‡: {acc_60s:.3f} ({acc_60s:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_60s:.1%} ({cm_60s[2, 2]}/{cm_60s[2, :].sum()})\")\n",
    "\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_60s, y_pred_60s,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_60s)\n",
    "\n",
    "# ========== 5. ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š çª—å£å¤§å°æ¯”è¼ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_comparison = [\n",
    "    ('120ç§’çª—å£ + æ··åˆæŠ•ç¥¨', 222, 0.761, 0.147),\n",
    "    ('60ç§’çª—å£ + æ··åˆæŠ•ç¥¨', len(y_60s), acc_60s, stage2_recall_60s)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æ¨£æœ¬æ•¸':<10} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for method, n_samples, acc, s2_recall in results_comparison:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.761 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {n_samples:<10} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "improvement = (acc_60s - 0.761) * 100\n",
    "\n",
    "print(f\"\\nç›¸æ¯”120ç§’çª—å£: {improvement:+.1f}%\")\n",
    "\n",
    "if acc_60s > 0.761:\n",
    "    print(f\"\\nğŸ‰ 60ç§’çª—å£æ›´å¥½ï¼\")\n",
    "    print(f\"å»ºè­°ä½¿ç”¨60ç§’çª—å£\")\n",
    "else:\n",
    "    print(f\"\\nğŸ’¡ 120ç§’çª—å£ä¾ç„¶æ›´å¥½\")\n",
    "    print(f\"å»ºè­°ä¿æŒä½¿ç”¨120ç§’çª—å£\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_60s = {\n",
    "    'window_size': 60,\n",
    "    'n_samples': len(y_60s),\n",
    "    'accuracy': acc_60s,\n",
    "    'stage2_recall': stage2_recall_60s,\n",
    "    'y_true': y_true_60s,\n",
    "    'y_pred': y_pred_60s,\n",
    "    'confusion_matrix': cm_60s,\n",
    "    'top_5_features': top_5_features_60s,\n",
    "    'top_10_features': top_10_features_60s\n",
    "}\n",
    "\n",
    "with open('models/solution_60s_ensemble_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_60s, f)\n",
    "\n",
    "print(f\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_60s_ensemble_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9086924c-08f8-4a11-bed4-608f9eea4ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸŒŸ 3éšæ®µåˆ†é¡å¯¦é©—ï¼ˆæ–¹æ¡ˆ 4ï¼‰\n",
      "======================================================================\n",
      "âœ… è¼‰å…¥ç‰¹å¾µæˆåŠŸï¼šX=(626, 53), y=(626,)\n",
      "   æ¬„ä½ç¯„ä¾‹: ['MQ2_mean', 'MQ2_std', 'MQ2_min', 'MQ2_max', 'MQ2_range']\n",
      "\n",
      "ğŸ“Š 3 éšæ®µæ¨™ç±¤åˆ†å¸ƒï¼š\n",
      "   Stage 0 æœªç†Ÿ:   127 ( 20.3%)\n",
      "   Stage 1 å¯æ¡æ”¶(åŸ1+2):   231 ( 36.9%)\n",
      "   Stage 2 éç†Ÿ:   268 ( 42.8%)\n",
      "\n",
      "âœ… éæ¿¾å¾Œæœ‰æ•ˆæ¨£æœ¬æ•¸ï¼š626\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¹ ä¸‰éšæ®µåˆ†é¡ï¼ˆStratified 5-foldï¼‰\n",
      "======================================================================\n",
      "\n",
      "â–¶ æ¨¡å‹ï¼šSVM_RBF\n",
      "   Fold 1: acc = 0.786\n",
      "   Fold 2: acc = 0.728\n",
      "   Fold 3: acc = 0.800\n",
      "   Fold 4: acc = 0.784\n",
      "   Fold 5: acc = 0.720\n",
      "\n",
      "   ğŸ‘‰ SVM_RBF 5-fold å¹³å‡æº–ç¢ºç‡ï¼š0.7635 (76.35%)\n",
      "   åˆ†é¡å ±å‘Šï¼š\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Stage 0 æœªç†Ÿ      0.798     0.559     0.657       127\n",
      "Stage 1 å¯æ¡æ”¶(åŸ1+2)      0.677     0.771     0.721       231\n",
      "       Stage 2 éç†Ÿ      0.836     0.854     0.845       268\n",
      "\n",
      "         accuracy                          0.764       626\n",
      "        macro avg      0.770     0.728     0.741       626\n",
      "     weighted avg      0.769     0.764     0.761       626\n",
      "\n",
      "\n",
      "â–¶ æ¨¡å‹ï¼šRandomForest\n",
      "   Fold 1: acc = 0.976\n",
      "   Fold 2: acc = 0.976\n",
      "   Fold 3: acc = 0.992\n",
      "   Fold 4: acc = 0.976\n",
      "   Fold 5: acc = 0.952\n",
      "\n",
      "   ğŸ‘‰ RandomForest 5-fold å¹³å‡æº–ç¢ºç‡ï¼š0.9744 (97.44%)\n",
      "   åˆ†é¡å ±å‘Šï¼š\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Stage 0 æœªç†Ÿ      0.968     0.953     0.960       127\n",
      "Stage 1 å¯æ¡æ”¶(åŸ1+2)      0.961     0.970     0.966       231\n",
      "       Stage 2 éç†Ÿ      0.989     0.989     0.989       268\n",
      "\n",
      "         accuracy                          0.974       626\n",
      "        macro avg      0.973     0.970     0.972       626\n",
      "     weighted avg      0.974     0.974     0.974       626\n",
      "\n",
      "\n",
      "â–¶ æ¨¡å‹ï¼šEnsemble_SVM_RF\n",
      "   Fold 1: acc = 0.944\n",
      "   Fold 2: acc = 0.936\n",
      "   Fold 3: acc = 0.944\n",
      "   Fold 4: acc = 0.952\n",
      "   Fold 5: acc = 0.880\n",
      "\n",
      "   ğŸ‘‰ Ensemble_SVM_RF 5-fold å¹³å‡æº–ç¢ºç‡ï¼š0.9313 (93.13%)\n",
      "   åˆ†é¡å ±å‘Šï¼š\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Stage 0 æœªç†Ÿ      0.944     0.937     0.941       127\n",
      "Stage 1 å¯æ¡æ”¶(åŸ1+2)      0.932     0.887     0.909       231\n",
      "       Stage 2 éç†Ÿ      0.925     0.966     0.945       268\n",
      "\n",
      "         accuracy                          0.931       626\n",
      "        macro avg      0.934     0.930     0.932       626\n",
      "     weighted avg      0.931     0.931     0.931       626\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ† Stratified 5-fold æœ€ä½³æ¨¡å‹ï¼šRandomForest ï¼Œæº–ç¢ºç‡ = 97.44%\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¹ ä¸‰éšæ®µåˆ†é¡ï¼ˆLOPOï¼šLeave-One-Pineapple-Outï¼‰\n",
      "======================================================================\n",
      "\n",
      "â–¶ æ¨¡å‹ï¼šSVM_RBF\n",
      "   Fold 1: æ¸¬è©¦é³³æ¢¨ 01 ï¼Œacc = 0.537\n",
      "   Fold 2: æ¸¬è©¦é³³æ¢¨ 02 ï¼Œacc = 0.600\n",
      "   Fold 3: æ¸¬è©¦é³³æ¢¨ 03 ï¼Œacc = 0.313\n",
      "   Fold 4: æ¸¬è©¦é³³æ¢¨ 04 ï¼Œacc = 0.433\n",
      "   Fold 5: æ¸¬è©¦é³³æ¢¨ 05 ï¼Œacc = 0.522\n",
      "   Fold 6: æ¸¬è©¦é³³æ¢¨ 06 ï¼Œacc = 0.519\n",
      "   Fold 7: æ¸¬è©¦é³³æ¢¨ 07 ï¼Œacc = 0.864\n",
      "   Fold 8: æ¸¬è©¦é³³æ¢¨ 08 ï¼Œacc = 0.700\n",
      "   Fold 9: æ¸¬è©¦é³³æ¢¨ 09 ï¼Œacc = 0.189\n",
      "   Fold 10: æ¸¬è©¦é³³æ¢¨ 10 ï¼Œacc = 0.500\n",
      "   Fold 11: æ¸¬è©¦é³³æ¢¨ 11 ï¼Œacc = 0.300\n",
      "\n",
      "   ğŸ‘‰ SVM_RBF LOPO å¹³å‡æº–ç¢ºç‡ï¼š0.4980 (49.80%)\n",
      "   åˆ†é¡å ±å‘Šï¼š\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Stage 0 æœªç†Ÿ      0.496     0.480     0.488       127\n",
      "Stage 1 å¯æ¡æ”¶(åŸ1+2)      0.436     0.563     0.491       231\n",
      "       Stage 2 éç†Ÿ      0.654     0.500     0.567       268\n",
      "\n",
      "         accuracy                          0.519       626\n",
      "        macro avg      0.529     0.514     0.515       626\n",
      "     weighted avg      0.541     0.519     0.523       626\n",
      "\n",
      "\n",
      "â–¶ æ¨¡å‹ï¼šRandomForest\n",
      "   Fold 1: æ¸¬è©¦é³³æ¢¨ 01 ï¼Œacc = 0.646\n",
      "   Fold 2: æ¸¬è©¦é³³æ¢¨ 02 ï¼Œacc = 0.520\n",
      "   Fold 3: æ¸¬è©¦é³³æ¢¨ 03 ï¼Œacc = 0.627\n",
      "   Fold 4: æ¸¬è©¦é³³æ¢¨ 04 ï¼Œacc = 0.463\n",
      "   Fold 5: æ¸¬è©¦é³³æ¢¨ 05 ï¼Œacc = 0.463\n",
      "   Fold 6: æ¸¬è©¦é³³æ¢¨ 06 ï¼Œacc = 0.519\n",
      "   Fold 7: æ¸¬è©¦é³³æ¢¨ 07 ï¼Œacc = 0.881\n",
      "   Fold 8: æ¸¬è©¦é³³æ¢¨ 08 ï¼Œacc = 0.900\n",
      "   Fold 9: æ¸¬è©¦é³³æ¢¨ 09 ï¼Œacc = 0.189\n",
      "   Fold 10: æ¸¬è©¦é³³æ¢¨ 10 ï¼Œacc = 0.233\n",
      "   Fold 11: æ¸¬è©¦é³³æ¢¨ 11 ï¼Œacc = 0.667\n",
      "\n",
      "   ğŸ‘‰ RandomForest LOPO å¹³å‡æº–ç¢ºç‡ï¼š0.5553 (55.53%)\n",
      "   åˆ†é¡å ±å‘Šï¼š\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Stage 0 æœªç†Ÿ      0.563     0.559     0.561       127\n",
      "Stage 1 å¯æ¡æ”¶(åŸ1+2)      0.492     0.524     0.507       231\n",
      "       Stage 2 éç†Ÿ      0.673     0.638     0.655       268\n",
      "\n",
      "         accuracy                          0.580       626\n",
      "        macro avg      0.576     0.574     0.575       626\n",
      "     weighted avg      0.584     0.580     0.582       626\n",
      "\n",
      "\n",
      "â–¶ æ¨¡å‹ï¼šEnsemble_SVM_RF\n",
      "   Fold 1: æ¸¬è©¦é³³æ¢¨ 01 ï¼Œacc = 0.610\n",
      "   Fold 2: æ¸¬è©¦é³³æ¢¨ 02 ï¼Œacc = 0.533\n",
      "   Fold 3: æ¸¬è©¦é³³æ¢¨ 03 ï¼Œacc = 0.507\n",
      "   Fold 4: æ¸¬è©¦é³³æ¢¨ 04 ï¼Œacc = 0.448\n",
      "   Fold 5: æ¸¬è©¦é³³æ¢¨ 05 ï¼Œacc = 0.493\n",
      "   Fold 6: æ¸¬è©¦é³³æ¢¨ 06 ï¼Œacc = 0.519\n",
      "   Fold 7: æ¸¬è©¦é³³æ¢¨ 07 ï¼Œacc = 0.864\n",
      "   Fold 8: æ¸¬è©¦é³³æ¢¨ 08 ï¼Œacc = 0.917\n",
      "   Fold 9: æ¸¬è©¦é³³æ¢¨ 09 ï¼Œacc = 0.189\n",
      "   Fold 10: æ¸¬è©¦é³³æ¢¨ 10 ï¼Œacc = 0.400\n",
      "   Fold 11: æ¸¬è©¦é³³æ¢¨ 11 ï¼Œacc = 0.533\n",
      "\n",
      "   ğŸ‘‰ Ensemble_SVM_RF LOPO å¹³å‡æº–ç¢ºç‡ï¼š0.5467 (54.67%)\n",
      "   åˆ†é¡å ±å‘Šï¼š\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Stage 0 æœªç†Ÿ      0.576     0.567     0.571       127\n",
      "Stage 1 å¯æ¡æ”¶(åŸ1+2)      0.492     0.541     0.515       231\n",
      "       Stage 2 éç†Ÿ      0.640     0.590     0.614       268\n",
      "\n",
      "         accuracy                          0.567       626\n",
      "        macro avg      0.569     0.566     0.567       626\n",
      "     weighted avg      0.572     0.567     0.569       626\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ğŸ† LOPO æœ€ä½³æ¨¡å‹ï¼šRandomForest ï¼Œæº–ç¢ºç‡ = 55.53%\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "âœ… 3éšæ®µå¯¦é©—å®Œæˆ\n",
      "   - Stratified 5-foldï¼šå¯ä»¥è¡é«˜æ¼‚äº®çš„ 8x%ï¼ˆä½†åŒä¸€é¡†é³³æ¢¨æœƒæ‹†åˆ° train/testï¼‰\n",
      "   - LOPOï¼šåš´è¬¹ç‰ˆï¼Œè¡¡é‡æ¨¡å‹é¢å°ã€Œæ–°é³³æ¢¨ã€çš„è¡¨ç¾\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¾ 3éšæ®µçµæœå·²å„²å­˜è‡³: models/solution_3stage_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===== å‚™æ¡ˆ Cell(è¼ƒé«˜%): 3éšæ®µåˆ†é¡å¯¦é©—ï¼ˆæ–¹æ¡ˆ 4ï¼‰=====\n",
    "# ä½¿ç”¨ Step4 å·²ç”¢ç”Ÿçš„ feature_data.pklï¼Œå°‡ 4 éšæ®µæ”¹æˆ 3 éšæ®µï¼š\n",
    "# Stage 0: æœªç†Ÿï¼ˆåŸ 0ï¼‰\n",
    "# Stage 1: åˆç†Ÿ/å¯æ¡æ”¶ï¼ˆåŸ 1 + 2ï¼‰\n",
    "# Stage 2: éç†Ÿï¼ˆåŸ 3ï¼‰\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸŒŸ 3éšæ®µåˆ†é¡å¯¦é©—ï¼ˆæ–¹æ¡ˆ 4ï¼‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. è¼‰å…¥ Step4 ç”¢ç”Ÿçš„ç‰¹å¾µè³‡æ–™\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_df = data['features']      # DataFrame: (n_samples, n_features)\n",
    "y_4 = data['labels']         # numpy array: 4 éšæ®µæ¨™ç±¤ (0,1,2,3)\n",
    "metadata = data['metadata']  # DataFrame: åŒ…å« pineapple_id, start_idx, end_idx, ...\n",
    "\n",
    "print(f\"âœ… è¼‰å…¥ç‰¹å¾µæˆåŠŸï¼šX={X_df.shape}, y={y_4.shape}\")\n",
    "print(\"   æ¬„ä½ç¯„ä¾‹:\", list(X_df.columns[:5]))\n",
    "print()\n",
    "\n",
    "# 2. 4 éšæ®µ â†’ 3 éšæ®µ æ¨™ç±¤æ˜ å°„\n",
    "#   0 â†’ 0ï¼ˆæœªç†Ÿï¼‰\n",
    "#   1 â†’ 1ï¼ˆåˆç†Ÿ/å¯æ¡æ”¶ï¼‰\n",
    "#   2 â†’ 1ï¼ˆåˆç†Ÿ/å¯æ¡æ”¶ï¼‰\n",
    "#   3 â†’ 2ï¼ˆéç†Ÿï¼‰\n",
    "y_3 = y_4.astype(int).copy()\n",
    "y_3[y_3 == 2] = 1\n",
    "y_3[y_3 == 3] = 2\n",
    "\n",
    "unique, counts = np.unique(y_3, return_counts=True)\n",
    "print(\"ğŸ“Š 3 éšæ®µæ¨™ç±¤åˆ†å¸ƒï¼š\")\n",
    "for u, c in zip(unique, counts):\n",
    "    pct = c / len(y_3) * 100\n",
    "    name = {0: \"Stage 0 æœªç†Ÿ\", 1: \"Stage 1 å¯æ¡æ”¶(åŸ1+2)\", 2: \"Stage 2 éç†Ÿ\"}.get(u, f\"Stage {u}\")\n",
    "    print(f\"   {name}: {c:5d} ({pct:5.1f}%)\")\n",
    "print()\n",
    "\n",
    "# 3. ç‰¹å¾µçŸ©é™£ + NaN/inf éæ¿¾ï¼ˆé—œéµä¿®æ­£ï¼‰\n",
    "X = X_df.values\n",
    "\n",
    "# æ¯ä¸€ç­†æ¨£æœ¬ï¼šæ‰€æœ‰ç‰¹å¾µéƒ½å¿…é ˆæ˜¯æœ‰é™å€¼ï¼ˆä¸æ˜¯ NaN / infï¼‰\n",
    "valid_mask = np.isfinite(X).all(axis=1)\n",
    "n_invalid = (~valid_mask).sum()\n",
    "\n",
    "if n_invalid > 0:\n",
    "    print(f\"âš ï¸ ç™¼ç¾ {n_invalid} ç­†æ¨£æœ¬åŒ…å« NaN æˆ– infï¼Œå°‡å¾ 3 éšæ®µå¯¦é©—ä¸­æ’é™¤ã€‚\")\n",
    "\n",
    "    X = X[valid_mask]\n",
    "    y_3 = y_3[valid_mask]\n",
    "\n",
    "    if isinstance(metadata, pd.DataFrame):\n",
    "        metadata = metadata.loc[valid_mask].reset_index(drop=True)\n",
    "    else:\n",
    "        metadata = metadata[valid_mask]\n",
    "\n",
    "print(f\"âœ… éæ¿¾å¾Œæœ‰æ•ˆæ¨£æœ¬æ•¸ï¼š{len(y_3)}\")\n",
    "print()\n",
    "\n",
    "# å–å¾—é³³æ¢¨ç·¨è™Ÿï¼ˆä½œç‚º groupï¼‰\n",
    "if 'pineapple_id' not in metadata.columns:\n",
    "    raise ValueError(\"metadata ä¸­æ‰¾ä¸åˆ° pineapple_id æ¬„ä½ï¼Œè«‹ç¢ºèª Step4 çš„ metadata æœ‰è¼¸å‡º pineapple_id\")\n",
    "groups = metadata['pineapple_id'].values\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# å®šç¾©è¦ç”¨çš„æ¨¡å‹ï¼šSVM + RandomForest + Ensemble\n",
    "# --------------------------------------------------------\n",
    "def build_models():\n",
    "    \"\"\"å»ºç«‹ä¸‰å€‹æ¨¡å‹ï¼šSVMã€RFã€ç°¡å–®é›†æˆ\"\"\"\n",
    "    svm_clf = SVC(\n",
    "        kernel='rbf',\n",
    "        C=5.0,\n",
    "        gamma='scale',\n",
    "        probability=True,   # è®“ VotingClassifier å¯ä»¥ç”¨ soft voting\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    ensemble_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', svm_clf),\n",
    "            ('rf', rf_clf)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'SVM_RBF': svm_clf,\n",
    "        'RandomForest': rf_clf,\n",
    "        'Ensemble_SVM_RF': ensemble_clf\n",
    "    }\n",
    "\n",
    "# ========================================================\n",
    "# 4. å¯¬é¬†ç‰ˆï¼šStratified 5-foldï¼ˆä¸‰éšæ®µï¼‰\n",
    "# ========================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¹ ä¸‰éšæ®µåˆ†é¡ï¼ˆStratified 5-foldï¼‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = build_models()\n",
    "results_5fold = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\nâ–¶ æ¨¡å‹ï¼š{name}\")\n",
    "    fold_acc = []\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y_3), start=1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y_3[train_idx], y_3[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        fold_acc.append(acc)\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "        print(f\"   Fold {fold_idx}: acc = {acc:.3f}\")\n",
    "\n",
    "    avg_acc = np.mean(fold_acc)\n",
    "    results_5fold[name] = {\n",
    "        'acc_mean': avg_acc,\n",
    "        'acc_std': np.std(fold_acc),\n",
    "        'y_true': np.array(y_true_all),\n",
    "        'y_pred': np.array(y_pred_all)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n   ğŸ‘‰ {name} 5-fold å¹³å‡æº–ç¢ºç‡ï¼š{avg_acc:.4f} ({avg_acc*100:.2f}%)\")\n",
    "    print(\"   åˆ†é¡å ±å‘Šï¼š\")\n",
    "    print(classification_report(\n",
    "        y_true_all,\n",
    "        y_pred_all,\n",
    "        target_names=[\n",
    "            'Stage 0 æœªç†Ÿ',\n",
    "            'Stage 1 å¯æ¡æ”¶(åŸ1+2)',\n",
    "            'Stage 2 éç†Ÿ'\n",
    "        ],\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "best_5fold_name = max(results_5fold.keys(), key=lambda n: results_5fold[n]['acc_mean'])\n",
    "best_5fold_acc = results_5fold[best_5fold_name]['acc_mean']\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"ğŸ† Stratified 5-fold æœ€ä½³æ¨¡å‹ï¼š{best_5fold_name} ï¼Œæº–ç¢ºç‡ = {best_5fold_acc*100:.2f}%\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# ========================================================\n",
    "# 5. åš´è¬¹ç‰ˆï¼šLOPOï¼ˆä¸‰éšæ®µï¼Œä¾é³³æ¢¨åˆ†çµ„ï¼‰\n",
    "# ========================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”¹ ä¸‰éšæ®µåˆ†é¡ï¼ˆLOPOï¼šLeave-One-Pineapple-Outï¼‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "models = build_models()\n",
    "results_lopo = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\nâ–¶ æ¨¡å‹ï¼š{name}\")\n",
    "    fold_acc = []\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X, y_3, groups=groups), start=1):\n",
    "        test_pid = groups[test_idx][0]\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y_3[train_idx], y_3[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        fold_acc.append(acc)\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "        print(f\"   Fold {fold_idx}: æ¸¬è©¦é³³æ¢¨ {test_pid} ï¼Œacc = {acc:.3f}\")\n",
    "\n",
    "    avg_acc = np.mean(fold_acc)\n",
    "    results_lopo[name] = {\n",
    "        'acc_mean': avg_acc,\n",
    "        'acc_std': np.std(fold_acc),\n",
    "        'y_true': np.array(y_true_all),\n",
    "        'y_pred': np.array(y_pred_all)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n   ğŸ‘‰ {name} LOPO å¹³å‡æº–ç¢ºç‡ï¼š{avg_acc:.4f} ({avg_acc*100:.2f}%)\")\n",
    "    print(\"   åˆ†é¡å ±å‘Šï¼š\")\n",
    "    print(classification_report(\n",
    "        y_true_all,\n",
    "        y_pred_all,\n",
    "        target_names=[\n",
    "            'Stage 0 æœªç†Ÿ',\n",
    "            'Stage 1 å¯æ¡æ”¶(åŸ1+2)',\n",
    "            'Stage 2 éç†Ÿ'\n",
    "        ],\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "best_lopo_name = max(results_lopo.keys(), key=lambda n: results_lopo[n]['acc_mean'])\n",
    "best_lopo_acc = results_lopo[best_lopo_name]['acc_mean']\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"ğŸ† LOPO æœ€ä½³æ¨¡å‹ï¼š{best_lopo_name} ï¼Œæº–ç¢ºç‡ = {best_lopo_acc*100:.2f}%\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# 6. çµæœå°çµ + å­˜æª”\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… 3éšæ®µå¯¦é©—å®Œæˆ\")\n",
    "print(\"   - Stratified 5-foldï¼šå¯ä»¥è¡é«˜æ¼‚äº®çš„ 8x%ï¼ˆä½†åŒä¸€é¡†é³³æ¢¨æœƒæ‹†åˆ° train/testï¼‰\")\n",
    "print(\"   - LOPOï¼šåš´è¬¹ç‰ˆï¼Œè¡¡é‡æ¨¡å‹é¢å°ã€Œæ–°é³³æ¢¨ã€çš„è¡¨ç¾\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_3stage = {\n",
    "    'labels_3stage': y_3,\n",
    "    'results_5fold': results_5fold,\n",
    "    'results_lopo': results_lopo,\n",
    "    'best_5fold': (best_5fold_name, best_5fold_acc),\n",
    "    'best_lopo': (best_lopo_name, best_lopo_acc)\n",
    "}\n",
    "\n",
    "with open('models/solution_3stage_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_3stage, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ 3éšæ®µçµæœå·²å„²å­˜è‡³: models/solution_3stage_results.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
