{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3630381d-38a0-4ecc-9a8a-cb1434cb9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ å¿«é€Ÿè™•ç†æ¨¡å¼...\n",
      "ğŸ“‚ æ‰¾åˆ° 58 å€‹æª”æ¡ˆ\n",
      "   âœ“ Air: 27, Pineapple: 8 é¡†\n",
      "   âœ“ 04_20250206: (900, 50)\n",
      "   âœ“ 04_20250205: (899, 54)\n",
      "   âœ“ 04_20250203: (900, 50)\n",
      "   âœ“ 04_20250204: (900, 54)\n",
      "   âœ“ 02_20250203: (900, 50)\n",
      "   âœ“ 02_20250202: (900, 50)\n",
      "   âœ“ 02_20250204: (900, 50)\n",
      "   âœ“ 02_20250205: (900, 50)\n",
      "   âœ“ 03_20250206: (900, 54)\n",
      "   âœ“ 03_20250205: (900, 54)\n",
      "   âœ“ 03_20250204: (900, 54)\n",
      "   âœ“ 03_20250203: (900, 50)\n",
      "   âœ“ 06_20250205: (0, 74)\n",
      "   âœ“ 06_20250203: (900, 50)\n",
      "   âœ“ 06_20250202: (900, 50)\n",
      "   âœ“ 06_20250204: (900, 50)\n",
      "   âœ“ 01_20250201: (900, 50)\n",
      "   âœ“ 01_20250204: (900, 50)\n",
      "   âœ“ 01_20250203: (900, 50)\n",
      "   âœ“ 01_20250202: (900, 50)\n",
      "   âœ“ 01_20250205: (900, 50)\n",
      "   âœ“ 08_20250204: (900, 50)\n",
      "   âœ“ 08_20250203: (900, 50)\n",
      "   âœ“ 08_20250205: (900, 50)\n",
      "   âœ“ 07_20250203: (900, 50)\n",
      "   âœ“ 07_20250204: (900, 50)\n",
      "   âœ“ 07_20250205: (899, 50)\n",
      "   âœ“ 05_20250202: (900, 50)\n",
      "   âœ“ 05_20250205: (900, 50)\n",
      "   âœ“ 05_20250204: (900, 50)\n",
      "   âœ“ 05_20250203: (900, 50)\n",
      "âœ… å®Œæˆï¼ç”¢ç”Ÿ 8 é¡†é³³æ¢¨çš„ç‰¹å¾µ\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: è¼‰å…¥å¿…è¦å¥—ä»¶ =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ===== Cell 2: å¿«é€Ÿè¼‰å…¥å™¨ =====\n",
    "class QuickLoader:\n",
    "    \"\"\"ç°¡åŒ–ç‰ˆè¼‰å…¥å™¨ï¼Œç›´æ¥ç”¢ç”Ÿ arduino_features\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='data/raw'):\n",
    "        self.data_dir = data_dir\n",
    "        self.sensor_cols = ['MQ2_raw', 'MQ3_raw', 'MQ9_raw', 'MQ135_raw', 'TGS2602_raw']\n",
    "        \n",
    "        # R0 æ ¡æ­£åƒè€ƒå€¼\n",
    "        self.R0_reference = {\n",
    "            'MQ2_raw': 10.0,\n",
    "            'MQ3_raw': 60.0,\n",
    "            'MQ9_raw': 9.8,\n",
    "            'MQ135_raw': 76.63,\n",
    "            'TGS2602_raw': 2.5\n",
    "        }\n",
    "    \n",
    "    def load_and_process(self):\n",
    "        \"\"\"ä¸€æ¬¡æ€§è¼‰å…¥ä¸¦è™•ç†åˆ° arduino_features\"\"\"\n",
    "        print(\"ğŸ”§ å¿«é€Ÿè™•ç†æ¨¡å¼...\")\n",
    "        \n",
    "        # 1. è¼‰å…¥æ‰€æœ‰æª”æ¡ˆ\n",
    "        air_data, pineapple_data = self._load_files()\n",
    "        \n",
    "        if not pineapple_data:\n",
    "            print(\"âŒ æ‰¾ä¸åˆ°é³³æ¢¨æ•¸æ“š\")\n",
    "            return None\n",
    "        \n",
    "        # 2. è¨ˆç®— Arduino ç‰¹å¾µ\n",
    "        arduino_features = self._calculate_features(air_data, pineapple_data)\n",
    "        \n",
    "        print(f\"âœ… å®Œæˆï¼ç”¢ç”Ÿ {len(arduino_features)} é¡†é³³æ¢¨çš„ç‰¹å¾µ\")\n",
    "        return arduino_features\n",
    "    \n",
    "    def _load_files(self):\n",
    "        \"\"\"è¼‰å…¥ Excel æª”æ¡ˆ\"\"\"\n",
    "        xlsx_files = glob.glob(os.path.join(self.data_dir, '*.xlsx'))\n",
    "        print(f\"ğŸ“‚ æ‰¾åˆ° {len(xlsx_files)} å€‹æª”æ¡ˆ\")\n",
    "        \n",
    "        air_data = {}\n",
    "        pineapple_data = {}\n",
    "        \n",
    "        for file_path in xlsx_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            parts = filename.replace('.xlsx', '').split('_')\n",
    "            \n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            is_air = 'air' in filename\n",
    "            \n",
    "            # è®€å–æª”æ¡ˆ\n",
    "            try:\n",
    "                # å˜—è©¦è·³éç¬¬ä¸€è¡Œ\n",
    "                df = pd.read_excel(file_path, engine='openpyxl', skiprows=1)\n",
    "                \n",
    "                # æª¢æŸ¥æ˜¯å¦æœ‰æ­£ç¢ºçš„æ¬„ä½\n",
    "                if 'timestamp_ms' not in df.columns:\n",
    "                    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "                \n",
    "                # è½‰æ›æ•¸å­—å‹åˆ¥\n",
    "                for col in self.sensor_cols + ['timestamp_ms']:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # æ¸…ç†ç¼ºå€¼\n",
    "                df = df.dropna().reset_index(drop=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  è¼‰å…¥å¤±æ•— {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # åˆ†é¡å„²å­˜\n",
    "            if is_air:\n",
    "                if len(parts) == 3:\n",
    "                    # å…±ç”¨ air\n",
    "                    date = parts[1]\n",
    "                    air_data[f\"shared_{date}\"] = df\n",
    "                else:\n",
    "                    # å°ˆå±¬ air\n",
    "                    pid = parts[1]\n",
    "                    date = parts[2]\n",
    "                    air_data[f\"{pid}_{date}\"] = df\n",
    "            else:\n",
    "                pid = parts[1]\n",
    "                date = parts[2]\n",
    "                \n",
    "                if pid not in pineapple_data:\n",
    "                    pineapple_data[pid] = {}\n",
    "                pineapple_data[pid][date] = df\n",
    "        \n",
    "        print(f\"   âœ“ Air: {len(air_data)}, Pineapple: {len(pineapple_data)} é¡†\")\n",
    "        return air_data, pineapple_data\n",
    "    \n",
    "    def _calculate_features(self, air_data, pineapple_data):\n",
    "        \"\"\"è¨ˆç®— Arduino ç‰¹å¾µ\"\"\"\n",
    "        arduino_features = {}\n",
    "        \n",
    "        for pid, date_dict in pineapple_data.items():\n",
    "            arduino_features[pid] = {}\n",
    "            \n",
    "            for date, pine_df in date_dict.items():\n",
    "                # æ‰¾å°æ‡‰çš„ air baseline\n",
    "                air_df = self._get_air(air_data, pid, date)\n",
    "                \n",
    "                if air_df is None:\n",
    "                    print(f\"âš ï¸  {pid}_{date} æ‰¾ä¸åˆ° air baseline\")\n",
    "                    continue\n",
    "                \n",
    "                # è¨ˆç®— Delta\n",
    "                min_len = min(len(pine_df), len(air_df))\n",
    "                delta_df = pine_df.iloc[:min_len].copy()\n",
    "                \n",
    "                for col in self.sensor_cols:\n",
    "                    if col in pine_df.columns and col in air_df.columns:\n",
    "                        delta_df[f'{col}_delta'] = (\n",
    "                            pine_df[col].iloc[:min_len].values - \n",
    "                            air_df[col].iloc[:min_len].values\n",
    "                        )\n",
    "                \n",
    "                # è¨ˆç®— Arduino ç‰¹å¾µ\n",
    "                features_df = delta_df.copy()\n",
    "                \n",
    "                for col in self.sensor_cols:\n",
    "                    if col in delta_df.columns:\n",
    "                        sensor_name = col.replace('_raw', '')\n",
    "                        R0 = self.R0_reference.get(col, 10.0)\n",
    "                        \n",
    "                        # Rs/R0\n",
    "                        features_df[f'{sensor_name}_Rs_R0'] = delta_df[col] / R0\n",
    "                        \n",
    "                        # Delta Rs/R0\n",
    "                        delta_col = f'{col}_delta'\n",
    "                        if delta_col in delta_df.columns:\n",
    "                            features_df[f'{sensor_name}_delta_Rs_R0'] = delta_df[delta_col] / R0\n",
    "                        \n",
    "                        # ç§»å‹•å¹³å‡\n",
    "                        features_df[f'{sensor_name}_ma10'] = (\n",
    "                            delta_df[col].rolling(window=10, center=True).mean()\n",
    "                        )\n",
    "                        \n",
    "                        # ç§»å‹•æ¨™æº–å·®\n",
    "                        features_df[f'{sensor_name}_std10'] = (\n",
    "                            delta_df[col].rolling(window=10, center=True).std()\n",
    "                        )\n",
    "                \n",
    "                arduino_features[pid][date] = features_df\n",
    "                print(f\"   âœ“ {pid}_{date}: {features_df.shape}\")\n",
    "        \n",
    "        return arduino_features\n",
    "    \n",
    "    def _get_air(self, air_data, pid, date):\n",
    "        \"\"\"å–å¾—å°æ‡‰çš„ air baseline\"\"\"\n",
    "        # å°ˆå±¬ air\n",
    "        key = f\"{pid}_{date}\"\n",
    "        if key in air_data:\n",
    "            return air_data[key]\n",
    "        \n",
    "        # å…±ç”¨ air\n",
    "        shared_key = f\"shared_{date}\"\n",
    "        if shared_key in air_data:\n",
    "            return air_data[shared_key]\n",
    "        \n",
    "        return None\n",
    "\n",
    "# åŸ·è¡Œè¼‰å…¥\n",
    "loader = QuickLoader('data/raw')\n",
    "arduino_features = loader.load_and_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8306df2-bb8a-4981-9527-9dac78e24760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… data/processed/feature_data.pkl\n",
      "âœ… data/processed/feature_matrix.csv\n",
      "âœ… data/processed/labels.npy\n"
     ]
    }
   ],
   "source": [
    "# å¿«é€Ÿæª¢æŸ¥\n",
    "import os\n",
    "files_to_check = [\n",
    "    'data/processed/feature_data.pkl',\n",
    "    'data/processed/feature_matrix.csv',\n",
    "    'data/processed/labels.npy'\n",
    "]\n",
    "\n",
    "for file in files_to_check:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"âœ… {file}\")\n",
    "    else:\n",
    "        print(f\"âŒ {file} ä¸å­˜åœ¨ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63096d13-f197-4b6f-bb31-883b10766a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eed5c76-96ad-4122-9be0-f71ddb493886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f65b06-ccf6-4a17-b61d-103c7f7b2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” è¼‰å…¥çœŸå¯¦æˆç†Ÿåº¦æ¨™ç±¤...\n",
      "============================================================\n",
      "âœ… æˆåŠŸè¼‰å…¥: data/processed/maturity_labels.pkl\n",
      "\n",
      "ğŸ“Š æ¨™ç±¤æ‘˜è¦:\n",
      "   é³³æ¢¨æ•¸é‡: 8\n",
      "   é³³æ¢¨ IDs: ['04', '02', '03', '06', '01', '08', '07', '05']\n",
      "\n",
      "   å¯¦éš›é¡åˆ¥æ•¸: 9\n",
      "   é¡åˆ¥ç¯„åœ: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "   é¡åˆ¥åˆ†å¸ƒ:\n",
      "      é¡åˆ¥ 0: 5600 å€‹æ¨£æœ¬ ( 20.7%)\n",
      "      é¡åˆ¥ 1: 5500 å€‹æ¨£æœ¬ ( 20.4%)\n",
      "      é¡åˆ¥ 2: 5400 å€‹æ¨£æœ¬ ( 20.0%)\n",
      "      é¡åˆ¥ 3: 4350 å€‹æ¨£æœ¬ ( 16.1%)\n",
      "      é¡åˆ¥ 4: 3449 å€‹æ¨£æœ¬ ( 12.8%)\n",
      "      é¡åˆ¥ 5:  900 å€‹æ¨£æœ¬ (  3.3%)\n",
      "      é¡åˆ¥ 6:  670 å€‹æ¨£æœ¬ (  2.5%)\n",
      "      é¡åˆ¥ 7:  829 å€‹æ¨£æœ¬ (  3.1%)\n",
      "      é¡åˆ¥ 8:  300 å€‹æ¨£æœ¬ (  1.1%)\n",
      "\n",
      "   ç¸½æ¨£æœ¬æ•¸: 26998\n",
      "\n",
      "   æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\n",
      "      Pineapple 04: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(300), np.int64(3): np.int64(600), np.int64(4): np.int64(899)}\n",
      "      Pineapple 02: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900)}\n",
      "      Pineapple 03: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(450), np.int64(4): np.int64(450)}\n",
      "      Pineapple 06: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900)}\n",
      "      Pineapple 01: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900)}\n",
      "      Pineapple 08: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(300), np.int64(7): np.int64(300), np.int64(8): np.int64(300)}\n",
      "      Pineapple 07: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(370), np.int64(7): np.int64(529)}\n",
      "      Pineapple 05: {np.int64(0): np.int64(500), np.int64(1): np.int64(400), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(600), np.int64(5): np.int64(300)}\n",
      "\n",
      "âœ… çœŸå¯¦æ¨™ç±¤è¼‰å…¥å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ===== è¼‰å…¥çœŸå¯¦æˆç†Ÿåº¦æ¨™ç±¤ =====\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ” è¼‰å…¥çœŸå¯¦æˆç†Ÿåº¦æ¨™ç±¤...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å˜—è©¦è¼‰å…¥ pkl æˆ– json\n",
    "label_path_pkl = 'data/processed/maturity_labels.pkl'\n",
    "label_path_json = 'data/processed/maturity_labels.json'\n",
    "\n",
    "maturity_levels = None\n",
    "\n",
    "# 1. å„ªå…ˆå˜—è©¦ pkl\n",
    "if os.path.exists(label_path_pkl):\n",
    "    try:\n",
    "        with open(label_path_pkl, 'rb') as f:\n",
    "            maturity_levels = pickle.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥: {label_path_pkl}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥ pkl å¤±æ•—: {e}\")\n",
    "\n",
    "# 2. å¦‚æœ pkl å¤±æ•—ï¼Œå˜—è©¦ json\n",
    "if maturity_levels is None and os.path.exists(label_path_json):\n",
    "    try:\n",
    "        with open(label_path_json, 'r') as f:\n",
    "            maturity_levels_raw = json.load(f)\n",
    "        # è½‰æ›æˆ numpy array\n",
    "        maturity_levels = {k: np.array(v) for k, v in maturity_levels_raw.items()}\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥: {label_path_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥ json å¤±æ•—: {e}\")\n",
    "\n",
    "# 3. æª¢æŸ¥è¼‰å…¥çµæœ\n",
    "if maturity_levels is not None:\n",
    "    print(\"\\nğŸ“Š æ¨™ç±¤æ‘˜è¦:\")\n",
    "    print(f\"   é³³æ¢¨æ•¸é‡: {len(maturity_levels)}\")\n",
    "    print(f\"   é³³æ¢¨ IDs: {list(maturity_levels.keys())}\")\n",
    "    \n",
    "    # æŸ¥çœ‹æ‰€æœ‰å”¯ä¸€é¡åˆ¥\n",
    "    all_labels = []\n",
    "    for pid, labels in maturity_levels.items():\n",
    "        all_labels.extend(labels.tolist())\n",
    "    \n",
    "    unique_labels = np.unique(all_labels)\n",
    "    print(f\"\\n   å¯¦éš›é¡åˆ¥æ•¸: {len(unique_labels)}\")\n",
    "    print(f\"   é¡åˆ¥ç¯„åœ: {unique_labels}\")\n",
    "    \n",
    "    # é¡åˆ¥åˆ†å¸ƒ\n",
    "    print(f\"\\n   é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "    for label in unique_labels:\n",
    "        count = np.sum(np.array(all_labels) == label)\n",
    "        percentage = count / len(all_labels) * 100\n",
    "        print(f\"      é¡åˆ¥ {label}: {count:4d} å€‹æ¨£æœ¬ ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   ç¸½æ¨£æœ¬æ•¸: {len(all_labels)}\")\n",
    "    \n",
    "    # æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤åˆ†å¸ƒ\n",
    "    print(\"\\n   æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\")\n",
    "    for pid, labels in maturity_levels.items():\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"      Pineapple {pid}: {dict(zip(unique, counts))}\")\n",
    "    \n",
    "    print(\"\\nâœ… çœŸå¯¦æ¨™ç±¤è¼‰å…¥å®Œæˆï¼\")\n",
    "else:\n",
    "    print(\"\\nâŒ ç„¡æ³•è¼‰å…¥æ¨™ç±¤ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘\")\n",
    "    print(f\"   æŸ¥æ‰¾è·¯å¾‘: {label_path_pkl}\")\n",
    "    print(f\"   æˆ–: {label_path_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c407fff-d134-4df8-a089-74590db151c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æª¢æŸ¥çœŸå¯¦æ¨™ç±¤...\n",
      "============================================================\n",
      "å¯¦éš›é¡åˆ¥æ•¸: 9\n",
      "é¡åˆ¥ç¯„åœ: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "é¡åˆ¥åˆ†å¸ƒ:\n",
      "  é¡åˆ¥ 0: 5600 å€‹æ¨£æœ¬ ( 20.7%)\n",
      "  é¡åˆ¥ 1: 5500 å€‹æ¨£æœ¬ ( 20.4%)\n",
      "  é¡åˆ¥ 2: 5400 å€‹æ¨£æœ¬ ( 20.0%)\n",
      "  é¡åˆ¥ 3: 4350 å€‹æ¨£æœ¬ ( 16.1%)\n",
      "  é¡åˆ¥ 4: 3449 å€‹æ¨£æœ¬ ( 12.8%)\n",
      "  é¡åˆ¥ 5:  900 å€‹æ¨£æœ¬ (  3.3%)\n",
      "  é¡åˆ¥ 6:  670 å€‹æ¨£æœ¬ (  2.5%)\n",
      "  é¡åˆ¥ 7:  829 å€‹æ¨£æœ¬ (  3.1%)\n",
      "  é¡åˆ¥ 8:  300 å€‹æ¨£æœ¬ (  1.1%)\n",
      "\n",
      "ç¸½æ¨£æœ¬æ•¸: 26998\n",
      "\n",
      "æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\n",
      "  Pineapple 04: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(300), np.int64(3): np.int64(600), np.int64(4): np.int64(899)}\n",
      "  Pineapple 02: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900)}\n",
      "  Pineapple 03: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(450), np.int64(4): np.int64(450)}\n",
      "  Pineapple 06: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900)}\n",
      "  Pineapple 01: {np.int64(0): np.int64(900), np.int64(1): np.int64(900), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(900)}\n",
      "  Pineapple 08: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(300), np.int64(7): np.int64(300), np.int64(8): np.int64(300)}\n",
      "  Pineapple 07: {np.int64(0): np.int64(300), np.int64(1): np.int64(300), np.int64(2): np.int64(300), np.int64(3): np.int64(300), np.int64(4): np.int64(300), np.int64(5): np.int64(300), np.int64(6): np.int64(370), np.int64(7): np.int64(529)}\n",
      "  Pineapple 05: {np.int64(0): np.int64(500), np.int64(1): np.int64(400), np.int64(2): np.int64(900), np.int64(3): np.int64(900), np.int64(4): np.int64(600), np.int64(5): np.int64(300)}\n"
     ]
    }
   ],
   "source": [
    "# ===== æª¢æŸ¥çœŸå¯¦æ¨™ç±¤çš„å¯¦éš›åˆ†å¸ƒ =====\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ” æª¢æŸ¥çœŸå¯¦æ¨™ç±¤...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. æŸ¥çœ‹æ‰€æœ‰å”¯ä¸€é¡åˆ¥\n",
    "all_labels = []\n",
    "for pid, labels in maturity_levels.items():\n",
    "    all_labels.extend(labels.tolist())\n",
    "\n",
    "unique_labels = np.unique(all_labels)\n",
    "label_counts = np.bincount(all_labels)\n",
    "\n",
    "print(f\"å¯¦éš›é¡åˆ¥æ•¸: {len(unique_labels)}\")\n",
    "print(f\"é¡åˆ¥ç¯„åœ: {unique_labels}\")\n",
    "print(f\"\\né¡åˆ¥åˆ†å¸ƒ:\")\n",
    "for label in unique_labels:\n",
    "    count = label_counts[label]\n",
    "    percentage = count / len(all_labels) * 100\n",
    "    print(f\"  é¡åˆ¥ {label}: {count:4d} å€‹æ¨£æœ¬ ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nç¸½æ¨£æœ¬æ•¸: {len(all_labels)}\")\n",
    "\n",
    "# 2. æŸ¥çœ‹æ¯å€‹é³³æ¢¨çš„æ¨™ç±¤åˆ†å¸ƒ\n",
    "print(\"\\næ¯å€‹é³³æ¢¨çš„æ¨™ç±¤:\")\n",
    "for pid, labels in maturity_levels.items():\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  Pineapple {pid}: {dict(zip(unique, counts))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b5fd541-06af-4653-9351-3b38ddc9cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é‡æ–°æ˜ å°„æ¨™ç±¤...\n",
      "============================================================\n",
      "æ˜ å°„è¦å‰‡:\n",
      "  0,1   â†’ Stage 0 (æœªæˆç†Ÿ)\n",
      "  2,3   â†’ Stage 1 (ç¨å¾®æˆç†Ÿ)\n",
      "  4,5   â†’ Stage 2 (æˆç†Ÿ)\n",
      "  6,7,8 â†’ Stage 3 (éç†Ÿ)\n",
      "\n",
      "æ˜ å°„å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ:\n",
      "  Pineapple 04: {np.int64(0): np.int64(1800), np.int64(1): np.int64(900), np.int64(2): np.int64(899)}\n",
      "  Pineapple 02: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1800)}\n",
      "  Pineapple 03: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1350), np.int64(2): np.int64(450)}\n",
      "  Pineapple 06: {np.int64(0): np.int64(1800), np.int64(1): np.int64(900)}\n",
      "  Pineapple 01: {np.int64(0): np.int64(1800), np.int64(1): np.int64(1800), np.int64(2): np.int64(900)}\n",
      "  Pineapple 08: {np.int64(0): np.int64(600), np.int64(1): np.int64(600), np.int64(2): np.int64(600), np.int64(3): np.int64(900)}\n",
      "  Pineapple 07: {np.int64(0): np.int64(600), np.int64(1): np.int64(600), np.int64(2): np.int64(600), np.int64(3): np.int64(899)}\n",
      "  Pineapple 05: {np.int64(0): np.int64(900), np.int64(1): np.int64(1800), np.int64(2): np.int64(900)}\n",
      "\n",
      "ç¸½é«”åˆ†å¸ƒ:\n",
      "  Stage 0: 11100 å€‹æ¨£æœ¬ ( 41.1%)\n",
      "  Stage 1:  9750 å€‹æ¨£æœ¬ ( 36.1%)\n",
      "  Stage 2:  4349 å€‹æ¨£æœ¬ ( 16.1%)\n",
      "  Stage 3:  1799 å€‹æ¨£æœ¬ (  6.7%)\n",
      "\n",
      "ç¸½æ¨£æœ¬æ•¸: 26998\n",
      "\n",
      "âœ… æ¨™ç±¤å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´šï¼\n",
      "âœ… å¯ä»¥é‡æ–°åŸ·è¡Œ Cell 11ï¼ˆç‰¹å¾µå·¥ç¨‹ï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ===== é‡æ–°æ˜ å°„æ¨™ç±¤ï¼š9ç­‰ç´š â†’ 4ç­‰ç´š =====\n",
    "import numpy as np\n",
    "\n",
    "def remap_labels_9to4(labels):\n",
    "    \"\"\"\n",
    "    å°‡ 0-8 (9ç­‰ç´š) æ˜ å°„åˆ° 0-3 (4ç­‰ç´š)\n",
    "    \n",
    "    æ˜ å°„è¦å‰‡:\n",
    "    0,1   â†’ 0 (æœªæˆç†Ÿ / Unripe)\n",
    "    2,3   â†’ 1 (ç¨å¾®æˆç†Ÿ / Slightly ripe)\n",
    "    4,5   â†’ 2 (æˆç†Ÿ / Ripe)\n",
    "    6,7,8 â†’ 3 (éç†Ÿ / Overripe)\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        0: 0, 1: 0,         # Stage 0: æœªæˆç†Ÿ\n",
    "        2: 1, 3: 1,         # Stage 1: ç¨å¾®æˆç†Ÿ\n",
    "        4: 2, 5: 2,         # Stage 2: æˆç†Ÿ\n",
    "        6: 3, 7: 3, 8: 3    # Stage 3: éç†Ÿ\n",
    "    }\n",
    "    return np.array([mapping[int(label)] for label in labels])\n",
    "\n",
    "print(\"ğŸ”„ é‡æ–°æ˜ å°„æ¨™ç±¤...\")\n",
    "print(\"=\"*60)\n",
    "print(\"æ˜ å°„è¦å‰‡:\")\n",
    "print(\"  0,1   â†’ Stage 0 (æœªæˆç†Ÿ)\")\n",
    "print(\"  2,3   â†’ Stage 1 (ç¨å¾®æˆç†Ÿ)\")\n",
    "print(\"  4,5   â†’ Stage 2 (æˆç†Ÿ)\")\n",
    "print(\"  6,7,8 â†’ Stage 3 (éç†Ÿ)\")\n",
    "print()\n",
    "\n",
    "# é‡æ–°æ˜ å°„æ‰€æœ‰é³³æ¢¨çš„æ¨™ç±¤\n",
    "maturity_levels_4stage = {}\n",
    "for pid, labels in maturity_levels.items():\n",
    "    maturity_levels_4stage[pid] = remap_labels_9to4(labels)\n",
    "\n",
    "# æª¢æŸ¥æ˜ å°„çµæœ\n",
    "print(\"æ˜ å°„å¾Œçš„æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "all_labels_4stage = []\n",
    "for pid, labels in maturity_levels_4stage.items():\n",
    "    all_labels_4stage.extend(labels.tolist())\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  Pineapple {pid}: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# ç¸½é«”åˆ†å¸ƒ\n",
    "print(\"\\nç¸½é«”åˆ†å¸ƒ:\")\n",
    "unique_4stage, counts_4stage = np.unique(all_labels_4stage, return_counts=True)\n",
    "for stage, count in zip(unique_4stage, counts_4stage):\n",
    "    percentage = count / len(all_labels_4stage) * 100\n",
    "    print(f\"  Stage {stage}: {count:5d} å€‹æ¨£æœ¬ ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nç¸½æ¨£æœ¬æ•¸: {len(all_labels_4stage)}\")\n",
    "\n",
    "# æ›¿æ›åŸæœ¬çš„ maturity_levels\n",
    "maturity_levels = maturity_levels_4stage\n",
    "\n",
    "print(\"\\nâœ… æ¨™ç±¤å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´šï¼\")\n",
    "print(\"âœ… å¯ä»¥é‡æ–°åŸ·è¡Œ Cell 11ï¼ˆç‰¹å¾µå·¥ç¨‹ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ebc96a0-4dce-4d24-8198-c03f2894f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 120 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 29 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 22 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 37 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 22 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 22 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (222, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 222\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(93), np.int64(1): np.int64(81), np.int64(2): np.int64(34), np.int64(3): np.int64(14)}\n",
      "============================================================\n",
      "\n",
      "ğŸ’¾ å„²å­˜ç‰¹å¾µ...\n",
      "   âœ… CSV: data/processed/feature_matrix.csv\n",
      "   âœ… Labels: data/processed/labels.npy\n",
      "   âœ… Metadata: data/processed/metadata.csv\n",
      "   âœ… Pickle: data/processed/feature_data.pkl\n",
      "\n",
      "âœ… Step 4 å®Œæˆï¼å¯ä»¥é€²å…¥ Step 5ï¼ˆæ¨¡å‹è¨“ç·´ï¼‰\n",
      "\n",
      "ğŸ“Š ç‰¹å¾µæ‘˜è¦:\n",
      "   ç¸½æ¨£æœ¬æ•¸: 222\n",
      "   ç‰¹å¾µç¶­åº¦: 53\n",
      "   ç‰¹å¾µåˆ—è¡¨: ['MQ2_mean', 'MQ2_std', 'MQ2_min', 'MQ2_max', 'MQ2_range', 'MQ2_slope', 'MQ2_auc', 'MQ2_delta_mean', 'MQ2_delta_std', 'MQ2_delta_max_abs']... (å‰10å€‹)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 11: Step 4 - ç‰¹å¾µå·¥ç¨‹ï¼ˆä¿®æ­£ç‰ˆï¼‰=====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class FeatureEngineering:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å›ºå®šæ™‚é–“çª—å£æå–ç‰¹å¾µ\n",
    "    ç›®æ¨™ï¼šå°‡æ™‚é–“åºåˆ—è½‰æ›ç‚º (n_samples, n_features) çš„ç‰¹å¾µçŸ©é™£\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, arduino_features, maturity_labels, window_size=120):\n",
    "        \"\"\"\n",
    "        åƒæ•¸:\n",
    "            window_size: æ™‚é–“çª—å£å¤§å°ï¼ˆç§’ï¼‰\n",
    "                        å‡è¨­å–æ¨£ç‡ 1Hzï¼Œ120ç§’ = 120 å€‹æ¨£æœ¬é»\n",
    "        \"\"\"\n",
    "        self.arduino_features = arduino_features\n",
    "        self.maturity_labels = maturity_labels\n",
    "        self.window_size = window_size\n",
    "        self.sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "        \n",
    "        self.feature_matrix = None\n",
    "        self.labels = None\n",
    "        self.metadata = None\n",
    "        \n",
    "        self.output_dir = 'data/processed'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"æ‰¹æ¬¡æå–æ‰€æœ‰ç‰¹å¾µ\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"çª—å£å¤§å°: {self.window_size} ç§’\\n\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        for pid in self.arduino_features.keys():\n",
    "            if pid not in self.maturity_labels:\n",
    "                print(f\"âš ï¸  {pid}: æ‰¾ä¸åˆ°å°æ‡‰çš„æˆç†Ÿåº¦æ¨™ç±¤ï¼Œè·³é\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ğŸ è™•ç† Pineapple {pid}...\")\n",
    "            \n",
    "            # åˆä½µè©²é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\n",
    "            combined_df, combined_labels = self._combine_pineapple_data(pid)\n",
    "            \n",
    "            if combined_df is None:\n",
    "                continue\n",
    "            \n",
    "            # æ»‘å‹•çª—å£æå–ç‰¹å¾µ\n",
    "            features, labels, metadata = self._sliding_window_extraction(\n",
    "                combined_df, combined_labels, pid\n",
    "            )\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            print(f\"   âœ“ æå– {len(features)} å€‹çª—å£ï¼Œæ¯å€‹ {features.shape[1]} ç¶­ç‰¹å¾µ\\n\")\n",
    "        \n",
    "        # åˆä½µæ‰€æœ‰é³³æ¢¨çš„ç‰¹å¾µ\n",
    "        self.feature_matrix = pd.DataFrame(\n",
    "            np.vstack(all_features),\n",
    "            columns=self._get_feature_names()\n",
    "        )\n",
    "        self.labels = np.hstack(all_labels)\n",
    "        self.metadata = pd.DataFrame(all_metadata)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"âœ… ç‰¹å¾µæå–å®Œæˆï¼\")\n",
    "        print(f\"   ç‰¹å¾µçŸ©é™£: {self.feature_matrix.shape}\")\n",
    "        print(f\"   æ¨™ç±¤æ•¸é‡: {len(self.labels)}\")\n",
    "        print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.feature_matrix, self.labels, self.metadata\n",
    "    \n",
    "    def _combine_pineapple_data(self, pid):\n",
    "        \"\"\"åˆä½µå–®é¡†é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\"\"\"\n",
    "        df_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        date_dict = self.arduino_features[pid]\n",
    "        labels = self.maturity_labels[pid]\n",
    "        \n",
    "        offset = 0\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date].copy()\n",
    "            n_samples = len(df)\n",
    "            \n",
    "            # å–å°æ‡‰çš„æ¨™ç±¤\n",
    "            window_labels = labels[offset:offset+n_samples]\n",
    "            \n",
    "            df_list.append(df)\n",
    "            label_list.append(window_labels)\n",
    "            \n",
    "            offset += n_samples\n",
    "        \n",
    "        if not df_list:\n",
    "            return None, None\n",
    "        \n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        combined_labels = np.hstack(label_list)\n",
    "        \n",
    "        return combined_df, combined_labels\n",
    "    \n",
    "    def _sliding_window_extraction(self, df, labels, pid):\n",
    "        \"\"\"æ»‘å‹•çª—å£æå–ç‰¹å¾µ\"\"\"\n",
    "        features = []\n",
    "        window_labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        n_samples = len(df)\n",
    "        step_size = self.window_size  # ä¸é‡ç–Šçª—å£\n",
    "        \n",
    "        for start_idx in range(0, n_samples - self.window_size + 1, step_size):\n",
    "            end_idx = start_idx + self.window_size\n",
    "            \n",
    "            window_df = df.iloc[start_idx:end_idx]\n",
    "            window_label_array = labels[start_idx:end_idx]\n",
    "            \n",
    "            # è©²çª—å£çš„ä¸»è¦æ¨™ç±¤ï¼ˆçœ¾æ•¸ï¼‰\n",
    "            unique, counts = np.unique(window_label_array, return_counts=True)\n",
    "            majority_label = unique[np.argmax(counts)]\n",
    "            \n",
    "            # æå–ç‰¹å¾µ\n",
    "            feature_vector = self._extract_window_features(window_df)\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            window_labels.append(majority_label)\n",
    "            metadata.append({\n",
    "                'pineapple_id': pid,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'majority_label': int(majority_label),\n",
    "                'label_purity': max(counts) / len(window_label_array)\n",
    "            })\n",
    "        \n",
    "        return np.array(features), np.array(window_labels), metadata\n",
    "    \n",
    "    def _extract_window_features(self, window_df):\n",
    "        \"\"\"å¾å–®ä¸€çª—å£æå–ç‰¹å¾µ\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            # 1. Rs/R0 ç‰¹å¾µ\n",
    "            col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "            if col_rs_r0 in window_df.columns:\n",
    "                data = window_df[col_rs_r0].values\n",
    "                \n",
    "                # çµ±è¨ˆç‰¹å¾µ\n",
    "                features.append(np.mean(data))           # å¹³å‡å€¼\n",
    "                features.append(np.std(data))            # æ¨™æº–å·®\n",
    "                features.append(np.min(data))            # æœ€å°å€¼\n",
    "                features.append(np.max(data))            # æœ€å¤§å€¼\n",
    "                features.append(np.max(data) - np.min(data))  # åæ‡‰å¹…åº¦\n",
    "                \n",
    "                # è®ŠåŒ–é€Ÿç‡ï¼ˆæ–œç‡ï¼‰\n",
    "                if len(data) > 1:\n",
    "                    x = np.arange(len(data))\n",
    "                    slope, _, _, _, _ = linregress(x, data)\n",
    "                    features.append(slope)\n",
    "                else:\n",
    "                    features.append(0)\n",
    "                \n",
    "                # AUCï¼ˆç´¯ç©åæ‡‰ï¼‰- ä¿®æ­£ç‰ˆ\n",
    "                try:\n",
    "                    auc = np.trapezoid(data, dx=1)  # numpy >= 2.0\n",
    "                except AttributeError:\n",
    "                    # fallback: æ‰‹å‹•æ¢¯å½¢ç©åˆ†\n",
    "                    if len(data) > 1:\n",
    "                        auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                    else:\n",
    "                        auc = data[0] if len(data) > 0 else 0\n",
    "                \n",
    "                features.append(auc)\n",
    "            else:\n",
    "                features.extend([0] * 7)  # å¡«å…… 7 å€‹ç‰¹å¾µ\n",
    "            \n",
    "            # 2. Delta Rs/R0 ç‰¹å¾µ\n",
    "            col_delta = f'{sensor}_delta_Rs_R0'\n",
    "            if col_delta in window_df.columns:\n",
    "                data = window_df[col_delta].values\n",
    "                \n",
    "                features.append(np.mean(data))\n",
    "                features.append(np.std(data))\n",
    "                features.append(np.max(np.abs(data)))    # æœ€å¤§çµ•å°è®ŠåŒ–\n",
    "            else:\n",
    "                features.extend([0] * 3)\n",
    "        \n",
    "        # 3. è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µï¼ˆæ¯”ä¾‹ç‰¹å¾µï¼‰\n",
    "        # MQ3/MQ2 æ¯”ä¾‹ï¼ˆé…’ç²¾ vs å¯ç‡ƒæ°£é«”ï¼‰\n",
    "        mq3 = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0\n",
    "        mq2 = window_df['MQ2_Rs_R0'].mean() if 'MQ2_Rs_R0' in window_df.columns else 0\n",
    "        features.append(mq3 / (mq2 + 1e-6))\n",
    "        \n",
    "        # MQ135/TGS2602 æ¯”ä¾‹ï¼ˆç©ºæ°£å“è³ª vs VOCï¼‰\n",
    "        mq135 = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0\n",
    "        tgs = window_df['TGS2602_Rs_R0'].mean() if 'TGS2602_Rs_R0' in window_df.columns else 0\n",
    "        features.append(mq135 / (tgs + 1e-6))\n",
    "        \n",
    "        # æ‰€æœ‰æ„Ÿæ¸¬å™¨å¹³å‡\n",
    "        all_means = [window_df[f'{s}_Rs_R0'].mean() \n",
    "                     for s in self.sensor_cols \n",
    "                     if f'{s}_Rs_R0' in window_df.columns]\n",
    "        features.append(np.mean(all_means) if all_means else 0)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    \n",
    "    def _get_feature_names(self):\n",
    "        \"\"\"ç”Ÿæˆç‰¹å¾µåç¨±\"\"\"\n",
    "        names = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            # Rs/R0 ç‰¹å¾µ\n",
    "            names.extend([\n",
    "                f'{sensor}_mean',\n",
    "                f'{sensor}_std',\n",
    "                f'{sensor}_min',\n",
    "                f'{sensor}_max',\n",
    "                f'{sensor}_range',\n",
    "                f'{sensor}_slope',\n",
    "                f'{sensor}_auc'\n",
    "            ])\n",
    "            \n",
    "            # Delta Rs/R0 ç‰¹å¾µ\n",
    "            names.extend([\n",
    "                f'{sensor}_delta_mean',\n",
    "                f'{sensor}_delta_std',\n",
    "                f'{sensor}_delta_max_abs'\n",
    "            ])\n",
    "        \n",
    "        # è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µ\n",
    "        names.extend([\n",
    "            'MQ3_MQ2_ratio',\n",
    "            'MQ135_TGS2602_ratio',\n",
    "            'all_sensors_mean'\n",
    "        ])\n",
    "        \n",
    "        return names\n",
    "    \n",
    "    def save_features(self):\n",
    "        \"\"\"å„²å­˜ç‰¹å¾µèˆ‡æ¨™ç±¤\"\"\"\n",
    "        print(\"\\nğŸ’¾ å„²å­˜ç‰¹å¾µ...\")\n",
    "        \n",
    "        # 1. ç‰¹å¾µçŸ©é™£ï¼ˆCSVï¼‰\n",
    "        csv_path = os.path.join(self.output_dir, 'feature_matrix.csv')\n",
    "        self.feature_matrix.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… CSV: {csv_path}\")\n",
    "        \n",
    "        # 2. æ¨™ç±¤ï¼ˆnumpyï¼‰\n",
    "        label_path = os.path.join(self.output_dir, 'labels.npy')\n",
    "        np.save(label_path, self.labels)\n",
    "        print(f\"   âœ… Labels: {label_path}\")\n",
    "        \n",
    "        # 3. Metadataï¼ˆCSVï¼‰\n",
    "        meta_path = os.path.join(self.output_dir, 'metadata.csv')\n",
    "        self.metadata.to_csv(meta_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… Metadata: {meta_path}\")\n",
    "        \n",
    "        # 4. å®Œæ•´ pickleï¼ˆæ–¹ä¾¿è¼‰å…¥ï¼‰\n",
    "        pkl_path = os.path.join(self.output_dir, 'feature_data.pkl')\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'features': self.feature_matrix,\n",
    "                'labels': self.labels,\n",
    "                'metadata': self.metadata,\n",
    "                'feature_names': self.feature_matrix.columns.tolist(),\n",
    "                'window_size': self.window_size\n",
    "            }, f)\n",
    "        print(f\"   âœ… Pickle: {pkl_path}\")\n",
    "        \n",
    "        print(\"\\nâœ… Step 4 å®Œæˆï¼å¯ä»¥é€²å…¥ Step 5ï¼ˆæ¨¡å‹è¨“ç·´ï¼‰\")\n",
    "\n",
    "# åŸ·è¡Œç‰¹å¾µå·¥ç¨‹\n",
    "feature_engineer = FeatureEngineering(\n",
    "    arduino_features, \n",
    "    maturity_levels,  # ä½¿ç”¨é‡æ–°æ˜ å°„å¾Œçš„ 4 ç­‰ç´šæ¨™ç±¤\n",
    "    window_size=120   # 120 ç§’çª—å£\n",
    ")\n",
    "\n",
    "X, y, metadata = feature_engineer.extract_all_features()\n",
    "feature_engineer.save_features()\n",
    "\n",
    "# é¡¯ç¤ºç‰¹å¾µæ‘˜è¦\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾µæ‘˜è¦:\")\n",
    "print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "print(f\"   ç‰¹å¾µç¶­åº¦: {X.shape[1]}\")\n",
    "print(f\"   ç‰¹å¾µåˆ—è¡¨: {X.columns.tolist()[:10]}... (å‰10å€‹)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc76852e-d62f-4a53-96e5-fe0acc98b402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¤– Step 5: æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š è¨“ç·´ SVM (RBF kernel)...\n",
      "   C=1, gamma=scale: 0.730 (+/- 0.063)\n",
      "   C=10, gamma=scale: 0.901 (+/- 0.023)\n",
      "   C=100, gamma=scale: 0.928 (+/- 0.017)\n",
      "   C=1, gamma=auto: 0.730 (+/- 0.063)\n",
      "\n",
      "   âœ… æœ€ä½³åƒæ•¸: {'C': 100, 'gamma': 'scale'}\n",
      "   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.928\n",
      "\n",
      "ğŸ“Š è¨“ç·´ Random Forest...\n",
      "   n_est=100, depth=None: 0.955 (+/- 0.020)\n",
      "   n_est=200, depth=10: 0.955 (+/- 0.028)\n",
      "   n_est=100, depth=5: 0.892 (+/- 0.033)\n",
      "\n",
      "   âœ… æœ€ä½³åƒæ•¸: {'n_estimators': 200, 'max_depth': 10}\n",
      "   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.955\n",
      "\n",
      "ğŸ“Š æ¨¡å‹æ¯”è¼ƒ...\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ\n",
      "============================================================\n",
      "\n",
      "ğŸ¤– SVM (RBF)\n",
      "   äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.928\n",
      "\n",
      "   åˆ†é¡å ±å‘Š:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "        Stage 0      0.928     0.968     0.947        93\n",
      "        Stage 1      0.911     0.889     0.900        81\n",
      "        Stage 2      0.938     0.882     0.909        34\n",
      "        Stage 3      1.000     1.000     1.000        14\n",
      "   \n",
      "       accuracy                          0.928       222\n",
      "      macro avg      0.944     0.935     0.939       222\n",
      "   weighted avg      0.928     0.928     0.928       222\n",
      "   \n",
      "\n",
      "   æ··æ·†çŸ©é™£:\n",
      "   [[90  3  0  0]\n",
      " [ 7 72  2  0]\n",
      " [ 0  4 30  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "ğŸ¤– Random Forest\n",
      "   äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.955\n",
      "\n",
      "   åˆ†é¡å ±å‘Š:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "        Stage 0      0.948     0.978     0.963        93\n",
      "        Stage 1      0.973     0.901     0.936        81\n",
      "        Stage 2      0.919     1.000     0.958        34\n",
      "        Stage 3      1.000     1.000     1.000        14\n",
      "   \n",
      "       accuracy                          0.955       222\n",
      "      macro avg      0.960     0.970     0.964       222\n",
      "   weighted avg      0.956     0.955     0.955       222\n",
      "   \n",
      "\n",
      "   æ··æ·†çŸ©é™£:\n",
      "   [[91  2  0  0]\n",
      " [ 5 73  3  0]\n",
      " [ 0  0 34  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "âœ… æ¨è–¦æ¨¡å‹: Random Forest\n",
      "   æº–ç¢ºç‡: 0.955\n",
      "\n",
      "ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ...\n",
      "\n",
      "ğŸ” Top 10 é‡è¦ç‰¹å¾µ (Random Forest):\n",
      "    1. MQ135_TGS2602_ratio           : 0.0503\n",
      "    2. MQ135_auc                     : 0.0474\n",
      "    3. MQ9_auc                       : 0.0422\n",
      "    4. MQ135_mean                    : 0.0414\n",
      "    5. TGS2602_mean                  : 0.0395\n",
      "    6. MQ9_mean                      : 0.0382\n",
      "    7. MQ3_min                       : 0.0357\n",
      "    8. MQ3_MQ2_ratio                 : 0.0353\n",
      "    9. MQ9_max                       : 0.0346\n",
      "   10. MQ3_delta_mean                : 0.0325\n",
      "\n",
      "   ğŸ’¾ å®Œæ•´ç‰¹å¾µé‡è¦æ€§: models/feature_importance.csv\n",
      "\n",
      "ğŸ“Š éŒ¯èª¤æ¡ˆä¾‹åˆ†æ...\n",
      "\n",
      "âŒ éŒ¯èª¤æ¡ˆä¾‹åˆ†æ (SVM):\n",
      "   éŒ¯èª¤æ¨£æœ¬æ•¸: 16 / 222 (7.2%)\n",
      "\n",
      "   æœ€å¸¸è¦‹çš„æ··æ·† (çœŸå¯¦ â†’ é æ¸¬):\n",
      "      Stage 1 â†’ Stage 0: 7 æ¬¡\n",
      "      Stage 2 â†’ Stage 1: 4 æ¬¡\n",
      "      Stage 0 â†’ Stage 1: 3 æ¬¡\n",
      "      Stage 1 â†’ Stage 2: 2 æ¬¡\n",
      "\n",
      "   ğŸ’¾ éŒ¯èª¤æ¡ˆä¾‹è©³æƒ…: models/error_cases.csv\n",
      "\n",
      "ğŸ’¾ å„²å­˜æ¨¡å‹...\n",
      "   âœ… SVM: models/svm_model.pkl\n",
      "   âœ… Random Forest: models/rf_model.pkl\n",
      "\n",
      "âœ… æ¨¡å‹å·²å„²å­˜ï¼Œå¯ä»¥é€²å…¥ Step 6ï¼ˆéƒ¨ç½²ï¼‰\n",
      "\n",
      "============================================================\n",
      "âœ… Step 5 å®Œæˆï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 12: Step 5 - æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    è¨“ç·´ä¸¦æ¯”è¼ƒå¤šå€‹åˆ†é¡æ¨¡å‹\n",
    "    ä¸»æ¨¡å‹ï¼šSVM (RBF kernel)\n",
    "    å°æ¯”æ¨¡å‹ï¼šRandom Forest\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, metadata):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.metadata = metadata\n",
    "        \n",
    "        # æ¨™æº–åŒ–ç‰¹å¾µ\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # æ¨¡å‹\n",
    "        self.svm_model = None\n",
    "        self.rf_model = None\n",
    "        \n",
    "        # çµæœ\n",
    "        self.results = {}\n",
    "        \n",
    "        self.output_dir = 'models'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def train_and_evaluate_all(self):\n",
    "        \"\"\"è¨“ç·´ä¸¦è©•ä¼°æ‰€æœ‰æ¨¡å‹\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ¤– Step 5: æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. SVM (ä¸»æ¨¡å‹)\n",
    "        print(\"\\nğŸ“Š è¨“ç·´ SVM (RBF kernel)...\")\n",
    "        self._train_svm()\n",
    "        \n",
    "        # 2. Random Forest (å°æ¯”)\n",
    "        print(\"\\nğŸ“Š è¨“ç·´ Random Forest...\")\n",
    "        self._train_random_forest()\n",
    "        \n",
    "        # 3. æ¯”è¼ƒçµæœ\n",
    "        print(\"\\nğŸ“Š æ¨¡å‹æ¯”è¼ƒ...\")\n",
    "        self._compare_models()\n",
    "        \n",
    "        # 4. ç‰¹å¾µé‡è¦æ€§åˆ†æ\n",
    "        print(\"\\nğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ...\")\n",
    "        self._analyze_feature_importance()\n",
    "        \n",
    "        # 5. éŒ¯èª¤æ¡ˆä¾‹åˆ†æ\n",
    "        print(\"\\nğŸ“Š éŒ¯èª¤æ¡ˆä¾‹åˆ†æ...\")\n",
    "        self._analyze_errors()\n",
    "        \n",
    "        # 6. å„²å­˜æ¨¡å‹\n",
    "        print(\"\\nğŸ’¾ å„²å­˜æ¨¡å‹...\")\n",
    "        self._save_models()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… Step 5 å®Œæˆï¼\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def _train_svm(self):\n",
    "        \"\"\"è¨“ç·´ SVM æ¨¡å‹\"\"\"\n",
    "        # ä½¿ç”¨ä¸åŒçš„ C å’Œ gamma åƒæ•¸\n",
    "        param_grid = [\n",
    "            {'C': 1, 'gamma': 'scale'},\n",
    "            {'C': 10, 'gamma': 'scale'},\n",
    "            {'C': 100, 'gamma': 'scale'},\n",
    "            {'C': 1, 'gamma': 'auto'},\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        \n",
    "        for params in param_grid:\n",
    "            svm = SVC(kernel='rbf', **params, random_state=42)\n",
    "            \n",
    "            # 5-fold äº¤å‰é©—è­‰\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(svm, self.X_scaled, self.y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            print(f\"   C={params['C']}, gamma={params['gamma']}: {mean_score:.3f} (+/- {scores.std():.3f})\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"\\n   âœ… æœ€ä½³åƒæ•¸: {best_params}\")\n",
    "        print(f\"   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: {best_score:.3f}\")\n",
    "        \n",
    "        # ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹\n",
    "        self.svm_model = SVC(kernel='rbf', **best_params, random_state=42)\n",
    "        self.svm_model.fit(self.X_scaled, self.y)\n",
    "        \n",
    "        # äº¤å‰é©—è­‰é æ¸¬ï¼ˆç”¨æ–¼æ··æ·†çŸ©é™£ï¼‰\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        y_pred_cv = cross_val_predict(self.svm_model, self.X_scaled, self.y, cv=cv)\n",
    "        \n",
    "        self.results['svm'] = {\n",
    "            'model': self.svm_model,\n",
    "            'cv_score': best_score,\n",
    "            'y_pred_cv': y_pred_cv,\n",
    "            'params': best_params\n",
    "        }\n",
    "    \n",
    "    def _train_random_forest(self):\n",
    "        \"\"\"è¨“ç·´ Random Forest æ¨¡å‹\"\"\"\n",
    "        # å˜—è©¦ä¸åŒçš„åƒæ•¸\n",
    "        param_grid = [\n",
    "            {'n_estimators': 100, 'max_depth': None},\n",
    "            {'n_estimators': 200, 'max_depth': 10},\n",
    "            {'n_estimators': 100, 'max_depth': 5},\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        \n",
    "        for params in param_grid:\n",
    "            rf = RandomForestClassifier(**params, random_state=42)\n",
    "            \n",
    "            # 5-fold äº¤å‰é©—è­‰\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(rf, self.X_scaled, self.y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            print(f\"   n_est={params['n_estimators']}, depth={params['max_depth']}: {mean_score:.3f} (+/- {scores.std():.3f})\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"\\n   âœ… æœ€ä½³åƒæ•¸: {best_params}\")\n",
    "        print(f\"   âœ… äº¤å‰é©—è­‰æº–ç¢ºç‡: {best_score:.3f}\")\n",
    "        \n",
    "        # ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹\n",
    "        self.rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "        self.rf_model.fit(self.X_scaled, self.y)\n",
    "        \n",
    "        # äº¤å‰é©—è­‰é æ¸¬\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        y_pred_cv = cross_val_predict(self.rf_model, self.X_scaled, self.y, cv=cv)\n",
    "        \n",
    "        self.results['rf'] = {\n",
    "            'model': self.rf_model,\n",
    "            'cv_score': best_score,\n",
    "            'y_pred_cv': y_pred_cv,\n",
    "            'params': best_params\n",
    "        }\n",
    "    \n",
    "    def _compare_models(self):\n",
    "        \"\"\"æ¯”è¼ƒæ¨¡å‹è¡¨ç¾\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name, result in self.results.items():\n",
    "            model_display = \"SVM (RBF)\" if model_name == 'svm' else \"Random Forest\"\n",
    "            print(f\"\\nğŸ¤– {model_display}\")\n",
    "            print(f\"   äº¤å‰é©—è­‰æº–ç¢ºç‡: {result['cv_score']:.3f}\")\n",
    "            \n",
    "            # åˆ†é¡å ±å‘Š\n",
    "            y_pred = result['y_pred_cv']\n",
    "            print(\"\\n   åˆ†é¡å ±å‘Š:\")\n",
    "            report = classification_report(self.y, y_pred, \n",
    "                                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                                          digits=3)\n",
    "            print(\"   \" + report.replace(\"\\n\", \"\\n   \"))\n",
    "            \n",
    "            # æ··æ·†çŸ©é™£\n",
    "            cm = confusion_matrix(self.y, y_pred)\n",
    "            print(f\"\\n   æ··æ·†çŸ©é™£:\")\n",
    "            print(f\"   {cm}\")\n",
    "        \n",
    "        # æ¨è–¦æ¨¡å‹\n",
    "        best_model = max(self.results.items(), key=lambda x: x[1]['cv_score'])\n",
    "        print(f\"\\nâœ… æ¨è–¦æ¨¡å‹: {'SVM (RBF)' if best_model[0] == 'svm' else 'Random Forest'}\")\n",
    "        print(f\"   æº–ç¢ºç‡: {best_model[1]['cv_score']:.3f}\")\n",
    "    \n",
    "    def _analyze_feature_importance(self):\n",
    "        \"\"\"åˆ†æç‰¹å¾µé‡è¦æ€§ï¼ˆRandom Forestï¼‰\"\"\"\n",
    "        if 'rf' not in self.results:\n",
    "            return\n",
    "        \n",
    "        rf_model = self.results['rf']['model']\n",
    "        feature_names = self.X.columns if hasattr(self.X, 'columns') else [f'F{i}' for i in range(self.X.shape[1])]\n",
    "        \n",
    "        # å–å¾—ç‰¹å¾µé‡è¦æ€§\n",
    "        importances = rf_model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        print(\"\\nğŸ” Top 10 é‡è¦ç‰¹å¾µ (Random Forest):\")\n",
    "        for i, idx in enumerate(indices[:10], 1):\n",
    "            print(f\"   {i:2d}. {feature_names[idx]:30s}: {importances[idx]:.4f}\")\n",
    "        \n",
    "        # å„²å­˜å®Œæ•´ç‰¹å¾µé‡è¦æ€§\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'feature_importance.csv')\n",
    "        importance_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n   ğŸ’¾ å®Œæ•´ç‰¹å¾µé‡è¦æ€§: {csv_path}\")\n",
    "    \n",
    "    def _analyze_errors(self):\n",
    "        \"\"\"åˆ†æéŒ¯èª¤æ¡ˆä¾‹\"\"\"\n",
    "        print(\"\\nâŒ éŒ¯èª¤æ¡ˆä¾‹åˆ†æ (SVM):\")\n",
    "        \n",
    "        if 'svm' not in self.results:\n",
    "            return\n",
    "        \n",
    "        y_pred = self.results['svm']['y_pred_cv']\n",
    "        errors = self.y != y_pred\n",
    "        \n",
    "        print(f\"   éŒ¯èª¤æ¨£æœ¬æ•¸: {errors.sum()} / {len(self.y)} ({errors.sum()/len(self.y)*100:.1f}%)\")\n",
    "        \n",
    "        # åˆ†æå“ªäº›é¡åˆ¥å®¹æ˜“æ··æ·†\n",
    "        error_pairs = []\n",
    "        for true_label, pred_label in zip(self.y[errors], y_pred[errors]):\n",
    "            error_pairs.append((int(true_label), int(pred_label)))\n",
    "        \n",
    "        from collections import Counter\n",
    "        error_counts = Counter(error_pairs)\n",
    "        \n",
    "        print(\"\\n   æœ€å¸¸è¦‹çš„æ··æ·† (çœŸå¯¦ â†’ é æ¸¬):\")\n",
    "        for (true_l, pred_l), count in error_counts.most_common(5):\n",
    "            print(f\"      Stage {true_l} â†’ Stage {pred_l}: {count} æ¬¡\")\n",
    "        \n",
    "        # å„²å­˜éŒ¯èª¤æ¡ˆä¾‹\n",
    "        error_df = self.metadata[errors].copy()\n",
    "        error_df['true_label'] = self.y[errors]\n",
    "        error_df['predicted_label'] = y_pred[errors]\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'error_cases.csv')\n",
    "        error_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n   ğŸ’¾ éŒ¯èª¤æ¡ˆä¾‹è©³æƒ…: {csv_path}\")\n",
    "    \n",
    "    def _save_models(self):\n",
    "        \"\"\"å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹\"\"\"\n",
    "        # å„²å­˜ SVM\n",
    "        if 'svm' in self.results:\n",
    "            svm_path = os.path.join(self.output_dir, 'svm_model.pkl')\n",
    "            with open(svm_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.results['svm']['model'],\n",
    "                    'scaler': self.scaler,\n",
    "                    'params': self.results['svm']['params'],\n",
    "                    'cv_score': self.results['svm']['cv_score']\n",
    "                }, f)\n",
    "            print(f\"   âœ… SVM: {svm_path}\")\n",
    "        \n",
    "        # å„²å­˜ Random Forest\n",
    "        if 'rf' in self.results:\n",
    "            rf_path = os.path.join(self.output_dir, 'rf_model.pkl')\n",
    "            with open(rf_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.results['rf']['model'],\n",
    "                    'scaler': self.scaler,\n",
    "                    'params': self.results['rf']['params'],\n",
    "                    'cv_score': self.results['rf']['cv_score']\n",
    "                }, f)\n",
    "            print(f\"   âœ… Random Forest: {rf_path}\")\n",
    "        \n",
    "        print(\"\\nâœ… æ¨¡å‹å·²å„²å­˜ï¼Œå¯ä»¥é€²å…¥ Step 6ï¼ˆéƒ¨ç½²ï¼‰\")\n",
    "\n",
    "# è¼‰å…¥ç‰¹å¾µ\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features']\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "trainer = ModelTrainer(X, y, metadata)\n",
    "trainer.train_and_evaluate_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307dd5-29aa-493e-ba29-cf9b5a3fffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da50e85-8ce9-450e-967e-666ac6817137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥ä¸‹ç‚ºå˜—è©¦èª¿æ•´æ”¹é€²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f17f23-31a3-4399-b853-f2b383a8b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ğŸ“Š å¯¦é©— 1ï¼šé¡åˆ¥æ¬Šé‡èª¿æ•´\n",
    "#æ¸¬è©¦ 3 ç¨®ç­–ç•¥ï¼š\n",
    "\n",
    "#Noneï¼ˆåŸºæº–ï¼‰\n",
    "\n",
    "#Balancedï¼ˆè‡ªå‹•å¹³è¡¡ï¼‰\n",
    "\n",
    "#Customï¼ˆå¼·åŒ– Stage 2ï¼Œçµ¦ 1.5 å€æ¬Šé‡ï¼‰\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 2ï¼šä¸åŒçª—å£å¤§å°\n",
    "#æ¸¬è©¦ 4 ç¨®çª—å£ï¼š\n",
    "\n",
    "#60 ç§’ï¼šæ›´ç´°ç·»ï¼Œæ¨£æœ¬æ›´å¤š\n",
    "\n",
    "#120 ç§’ï¼šç•¶å‰åŸºæº–\n",
    "\n",
    "#180 ç§’ï¼šæ›´ç©©å®š\n",
    "\n",
    "#240 ç§’ï¼šæœ€ç©©å®šä½†æ¨£æœ¬è¼ƒå°‘\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 3ï¼šç‰¹å¾µé¸æ“‡\n",
    "#æ¸¬è©¦ä½¿ç”¨ 10, 20, 30, 40, 53 å€‹ç‰¹å¾µçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82898acd-dbd9-430e-b371-5cc0b85f744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# å®šç¾©è·¯å¾‘\n",
    "path = './data/processed/'\n",
    "\n",
    "# å–å¾—è©²ç›®éŒ„ä¸‹æ‰€æœ‰æª”æ¡ˆèˆ‡è³‡æ–™å¤¾åç¨±\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# æ‰“å°å‡ºä¾†\n",
    "for file_name in file_list:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68280ca-1511-4d9c-9aa6-68b3d80ddb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 14a: è¼‰å…¥å¿…è¦è®Šæ•¸ =====\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"ğŸ“‚ è¼‰å…¥å¿…è¦è³‡æ–™...\")\n",
    "\n",
    "# 1. è¼‰å…¥ maturity_levelsï¼ˆ4ç­‰ç´šï¼‰\n",
    "with open('data/processed/maturity_levels_4class.pkl', 'rb') as f:\n",
    "    maturity_levels = pickle.load(f)\n",
    "print(f\"âœ… maturity_levels: {len(maturity_levels)} é¡†é³³æ¢¨\")\n",
    "\n",
    "# 2. æª¢æŸ¥ arduino_features æ˜¯å¦å­˜åœ¨\n",
    "if 'arduino_features' not in globals():\n",
    "    print(\"\\nâš ï¸  arduino_features ä¸å­˜åœ¨\")\n",
    "    print(\"   è«‹é‡æ–°åŸ·è¡Œ Cell 2 (å¿«é€Ÿè¼‰å…¥å™¨)\")\n",
    "    print(\"   æˆ–åŸ·è¡Œä»¥ä¸‹ç¨‹å¼ç¢¼é‡æ–°è¼‰å…¥...\\n\")\n",
    "    \n",
    "    # å¿«é€Ÿé‡æ–°è¼‰å…¥ï¼ˆå¦‚æœä½ æœ‰åŸå§‹ CSVï¼‰\n",
    "    # é€™è£¡éœ€è¦é‡æ–°åŸ·è¡Œ Cell 2 çš„ç¨‹å¼ç¢¼\n",
    "else:\n",
    "    print(f\"âœ… arduino_features: {len(arduino_features)} é¡†é³³æ¢¨\")\n",
    "\n",
    "print(\"\\nâœ… è®Šæ•¸è¼‰å…¥å®Œæˆï¼å¯ä»¥åŸ·è¡Œ Cell 14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e904cb3-b689-46d2-b949-1836dd59aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ğŸ“Š å¯¦é©— 1ï¼šé¡åˆ¥æ¬Šé‡èª¿æ•´\n",
    "#æ¸¬è©¦ 3 ç¨®ç­–ç•¥ï¼š\n",
    "\n",
    "#Noneï¼ˆåŸºæº–ï¼‰\n",
    "\n",
    "#Balancedï¼ˆè‡ªå‹•å¹³è¡¡ï¼‰\n",
    "\n",
    "#Customï¼ˆå¼·åŒ– Stage 2ï¼Œçµ¦ 1.5 å€æ¬Šé‡ï¼‰\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 2ï¼šä¸åŒçª—å£å¤§å°\n",
    "#æ¸¬è©¦ 4 ç¨®çª—å£ï¼š\n",
    "\n",
    "#60 ç§’ï¼šæ›´ç´°ç·»ï¼Œæ¨£æœ¬æ›´å¤š\n",
    "\n",
    "#120 ç§’ï¼šç•¶å‰åŸºæº–\n",
    "\n",
    "#180 ç§’ï¼šæ›´ç©©å®š\n",
    "\n",
    "#240 ç§’ï¼šæœ€ç©©å®šä½†æ¨£æœ¬è¼ƒå°‘\n",
    "\n",
    "#ğŸ“Š å¯¦é©— 3ï¼šç‰¹å¾µé¸æ“‡\n",
    "#æ¸¬è©¦ä½¿ç”¨ 10, 20, 30, 40, 53 å€‹ç‰¹å¾µçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff02202-370f-4c80-96b0-51c7d2122748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æº–å‚™åŸ·è¡Œæ”¹é€²å¯¦é©—...\n",
      "   ç¢ºä¿å·²åŸ·è¡Œï¼š\n",
      "   - arduino_features å·²è¼‰å…¥\n",
      "   - maturity_levels å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´š\n",
      "   - FeatureEngineering å·²å®šç¾©ï¼ˆCell 11ï¼‰\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ æ¨¡å‹æ”¹é€²å¯¦é©—ï¼ˆåŸºæ–¼çœŸå¯¦æ¨™ç±¤ï¼‰\n",
      "======================================================================\n",
      "åŸºæº–æ¨¡å‹: Random Forest, æº–ç¢ºç‡ = 0.955\n",
      "\n",
      "ğŸ“Š å¯¦é©— 1: é¡åˆ¥æ¬Šé‡èª¿æ•´\n",
      "----------------------------------------------------------------------\n",
      "   æª¢æ¸¬åˆ°çš„é¡åˆ¥: [0 1 2 3]\n",
      "ğŸ“ˆ None (åŸºæº–)                     : æº–ç¢ºç‡=0.955 (+0.000)\n",
      "   âœ… Stage 1 Recall: 0.901 (åŸºæº–=0.901)\n",
      "ğŸ“ˆ Balanced                      : æº–ç¢ºç‡=0.964 (+0.009)\n",
      "   âœ… Stage 1 Recall: 0.926 (åŸºæº–=0.901)\n",
      "ğŸ“ˆ Custom (å¼·åŒ–Stage1)             : æº–ç¢ºç‡=0.964 (+0.009)\n",
      "   âœ… Stage 1 Recall: 0.926 (åŸºæº–=0.901)\n",
      "ğŸ“ˆ Custom (å¼·åŒ–Stage2)             : æº–ç¢ºç‡=0.955 (+0.000)\n",
      "   âœ… Stage 1 Recall: 0.901 (åŸºæº–=0.901)\n",
      "\n",
      "ğŸ“Š å¯¦é©— 2: ä¸åŒçª—å£å¤§å°\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 60 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 44 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (448, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 448\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(185), np.int64(1): np.int64(163), np.int64(2): np.int64(71), np.int64(3): np.int64(29)}\n",
      "============================================================\n",
      "ğŸ“ˆ çª—å£ 60s: æº–ç¢ºç‡=0.984 (+0.029), æ¨£æœ¬æ•¸=448\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 120 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 120 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 29 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 22 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 37 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 22 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 22 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 30 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (222, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 222\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(93), np.int64(1): np.int64(81), np.int64(2): np.int64(34), np.int64(3): np.int64(14)}\n",
      "============================================================\n",
      "ğŸ“ˆ çª—å£ 120s: æº–ç¢ºç‡=0.955 (+0.000), æ¨£æœ¬æ•¸=222\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 180 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 180 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 19 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 20 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 20 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 25 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 14 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 20 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (148, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 148\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(61), np.int64(1): np.int64(56), np.int64(2): np.int64(22), np.int64(3): np.int64(9)}\n",
      "============================================================\n",
      "ğŸ“‰ çª—å£ 180s: æº–ç¢ºç‡=0.905 (-0.050), æ¨£æœ¬æ•¸=148\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 240 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 240 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 14 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 11 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 18 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 11 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 11 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 15 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (110, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 110\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(50), np.int64(1): np.int64(36), np.int64(2): np.int64(18), np.int64(3): np.int64(6)}\n",
      "============================================================\n",
      "ğŸ“‰ çª—å£ 240s: æº–ç¢ºç‡=0.873 (-0.082), æ¨£æœ¬æ•¸=110\n",
      "\n",
      "ğŸ“Š å¯¦é©— 3: ç‰¹å¾µé¸æ“‡\n",
      "----------------------------------------------------------------------\n",
      "ğŸ“‰ ä½¿ç”¨ 10 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.937 (-0.018)\n",
      "\n",
      "   Top 20 ç‰¹å¾µ:\n",
      "       1. MQ2_delta_mean\n",
      "       2. MQ2_delta_max_abs\n",
      "       3. MQ3_mean\n",
      "       4. MQ3_min\n",
      "       5. MQ3_max\n",
      "       6. MQ3_auc\n",
      "       7. MQ3_delta_mean\n",
      "       8. MQ3_delta_max_abs\n",
      "       9. MQ9_mean\n",
      "      10. MQ9_min\n",
      "ğŸ“ˆ ä½¿ç”¨ 20 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.964 (+0.009)\n",
      "ğŸ“ˆ ä½¿ç”¨ 30 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.969 (+0.014)\n",
      "ğŸ“ˆ ä½¿ç”¨ 40 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.964 (+0.009)\n",
      "ğŸ“ˆ ä½¿ç”¨ 53 å€‹ç‰¹å¾µ: æº–ç¢ºç‡=0.955 (+0.000)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ å¯¦é©—ç¸½çµå ±å‘Š\n",
      "======================================================================\n",
      "\n",
      "ğŸ† æœ€ä½³æ–¹æ¡ˆ: window_60s\n",
      "   æº–ç¢ºç‡: 0.984\n",
      "   æ”¹é€²å¹…åº¦: +0.029\n",
      "\n",
      "âœ… æ‰¾åˆ°æ”¹é€²æ–¹æ¡ˆï¼æ¯”åŸºæº–æå‡ 3.08%\n",
      "   å¾ 0.955 â†’ 0.984\n",
      "\n",
      "ğŸ’¾ å®Œæ•´å ±å‘Š: models/improvements/improvement_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 24375 (\\N{CJK UNIFIED IDEOGRAPH-5F37}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 36914 (\\N{CJK UNIFIED IDEOGRAPH-9032}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 23526 (\\N{CJK UNIFIED IDEOGRAPH-5BE6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 23565 (\\N{CJK UNIFIED IDEOGRAPH-5C0D}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 65288 (\\N{FULLWIDTH LEFT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 27161 (\\N{CJK UNIFIED IDEOGRAPH-6A19}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 31844 (\\N{CJK UNIFIED IDEOGRAPH-7C64}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:288: UserWarning: Glyph 65289 (\\N{FULLWIDTH RIGHT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 24375 (\\N{CJK UNIFIED IDEOGRAPH-5F37}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 36914 (\\N{CJK UNIFIED IDEOGRAPH-9032}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 23526 (\\N{CJK UNIFIED IDEOGRAPH-5BE6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 23565 (\\N{CJK UNIFIED IDEOGRAPH-5C0D}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 65288 (\\N{FULLWIDTH LEFT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 27161 (\\N{CJK UNIFIED IDEOGRAPH-6A19}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 31844 (\\N{CJK UNIFIED IDEOGRAPH-7C64}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 65289 (\\N{FULLWIDTH RIGHT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_2018/1607245292.py:291: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š å°æ¯”åœ–: models/improvements/improvement_comparison.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24375 (\\N{CJK UNIFIED IDEOGRAPH-5F37}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 36914 (\\N{CJK UNIFIED IDEOGRAPH-9032}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 23526 (\\N{CJK UNIFIED IDEOGRAPH-5BE6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 23565 (\\N{CJK UNIFIED IDEOGRAPH-5C0D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 65288 (\\N{FULLWIDTH LEFT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 27161 (\\N{CJK UNIFIED IDEOGRAPH-6A19}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 31844 (\\N{CJK UNIFIED IDEOGRAPH-7C64}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 65289 (\\N{FULLWIDTH RIGHT PARENTHESIS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 13: æ¨¡å‹æ”¹é€²å¯¦é©—ï¼ˆæ›´æ–°ç‰ˆï¼‰=====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "class ModelImprovement:\n",
    "    \"\"\"ç³»çµ±åŒ–æ¸¬è©¦å¤šç¨®æ”¹é€²æ–¹æ¡ˆ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = {}\n",
    "        self.baseline_score = 0.955  # âœ… æ›´æ–°ç‚ºçœŸå¯¦åŸºæº–\n",
    "        self.output_dir = 'models/improvements'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        \"\"\"åŸ·è¡Œæ‰€æœ‰æ”¹é€²å¯¦é©—\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸ”¬ æ¨¡å‹æ”¹é€²å¯¦é©—ï¼ˆåŸºæ–¼çœŸå¯¦æ¨™ç±¤ï¼‰\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"åŸºæº–æ¨¡å‹: Random Forest, æº–ç¢ºç‡ = {self.baseline_score:.3f}\\n\")\n",
    "        \n",
    "        # å¯¦é©— 1: é¡åˆ¥æ¬Šé‡\n",
    "        print(\"ğŸ“Š å¯¦é©— 1: é¡åˆ¥æ¬Šé‡èª¿æ•´\")\n",
    "        print(\"-\"*70)\n",
    "        self._experiment_class_weights()\n",
    "        \n",
    "        # å¯¦é©— 2: ä¸åŒçª—å£å¤§å°\n",
    "        print(\"\\nğŸ“Š å¯¦é©— 2: ä¸åŒçª—å£å¤§å°\")\n",
    "        print(\"-\"*70)\n",
    "        self._experiment_window_sizes()\n",
    "        \n",
    "        # å¯¦é©— 3: ç‰¹å¾µé¸æ“‡\n",
    "        print(\"\\nğŸ“Š å¯¦é©— 3: ç‰¹å¾µé¸æ“‡\")\n",
    "        print(\"-\"*70)\n",
    "        self._experiment_feature_selection()\n",
    "        \n",
    "        # ç¸½çµå ±å‘Š\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“‹ å¯¦é©—ç¸½çµå ±å‘Š\")\n",
    "        print(\"=\"*70)\n",
    "        self._generate_summary_report()\n",
    "        \n",
    "        return self.experiments\n",
    "    \n",
    "    def _experiment_class_weights(self):\n",
    "        \"\"\"å¯¦é©— 1: æ¸¬è©¦é¡åˆ¥æ¬Šé‡\"\"\"\n",
    "        # è¼‰å…¥åŸºæº–ç‰¹å¾µ\n",
    "        with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X, y = data['features'], data['labels']\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # è‡ªå‹•æª¢æ¸¬é¡åˆ¥æ•¸\n",
    "        unique_classes = np.unique(y)\n",
    "        print(f\"   æª¢æ¸¬åˆ°çš„é¡åˆ¥: {unique_classes}\")\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒæ¬Šé‡ç­–ç•¥\n",
    "        weight_strategies = {\n",
    "            'None (åŸºæº–)': None,\n",
    "            'Balanced': 'balanced',\n",
    "            'Custom (å¼·åŒ–Stage1)': {c: (1.3 if c == 1 else 1.0) for c in unique_classes},  # Stage 1 recall è¼ƒä½\n",
    "            'Custom (å¼·åŒ–Stage2)': {c: (1.2 if c == 2 else 1.0) for c in unique_classes}\n",
    "        }\n",
    "        \n",
    "        for name, weights in weight_strategies.items():\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                class_weight=weights,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            # è©³ç´°åˆ†é¡å ±å‘Š\n",
    "            y_pred = cross_val_predict(rf, X_scaled, y, cv=cv)\n",
    "            report = classification_report(y, y_pred, \n",
    "                                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                                          output_dict=True)\n",
    "            \n",
    "            # ç‰¹åˆ¥é—œæ³¨ Stage 1 (recall æœ€ä½)\n",
    "            stage1_recall = report['Stage 1']['recall']\n",
    "            \n",
    "            self.experiments[f'class_weight_{name}'] = {\n",
    "                'accuracy': mean_score,\n",
    "                'stage1_recall': stage1_recall,\n",
    "                'improvement': mean_score - self.baseline_score\n",
    "            }\n",
    "            \n",
    "            improvement_icon = \"ğŸ“ˆ\" if mean_score > self.baseline_score else \"ğŸ“‰\" if mean_score < self.baseline_score else \"â¡ï¸\"\n",
    "            stage1_icon = \"âœ…\" if stage1_recall > 0.901 else \"âš ï¸\"\n",
    "            \n",
    "            print(f\"{improvement_icon} {name:30s}: æº–ç¢ºç‡={mean_score:.3f} ({mean_score-self.baseline_score:+.3f})\")\n",
    "            print(f\"   {stage1_icon} Stage 1 Recall: {stage1_recall:.3f} (åŸºæº–=0.901)\")\n",
    "    \n",
    "    def _experiment_window_sizes(self):\n",
    "        \"\"\"å¯¦é©— 2: æ¸¬è©¦ä¸åŒçª—å£å¤§å°\"\"\"\n",
    "        # æª¢æŸ¥ FeatureEngineering æ˜¯å¦å¯ç”¨\n",
    "        try:\n",
    "            FeatureEngineering\n",
    "        except NameError:\n",
    "            print(\"âš ï¸  FeatureEngineering æœªå®šç¾©ï¼Œè·³éæ­¤å¯¦é©—\")\n",
    "            print(\"   æç¤ºï¼šéœ€è¦é‡æ–°åŸ·è¡Œ Cell 11 æ‰èƒ½æ¸¬è©¦ä¸åŒçª—å£å¤§å°\")\n",
    "            return\n",
    "        \n",
    "        window_sizes = [60, 120, 180, 240]\n",
    "        \n",
    "        for ws in window_sizes:\n",
    "            print(f\"\\nğŸ” æ¸¬è©¦çª—å£å¤§å°: {ws} ç§’\")\n",
    "            \n",
    "            try:\n",
    "                # é‡æ–°æå–ç‰¹å¾µ\n",
    "                feature_engineer = FeatureEngineering(\n",
    "                    arduino_features, \n",
    "                    maturity_levels,\n",
    "                    window_size=ws\n",
    "                )\n",
    "                \n",
    "                X, y, metadata = feature_engineer.extract_all_features()\n",
    "                \n",
    "                # æ¨™æº–åŒ–\n",
    "                scaler = StandardScaler()\n",
    "                X_scaled = scaler.fit_transform(X)\n",
    "                \n",
    "                # è¨“ç·´ Random Forest\n",
    "                rf = RandomForestClassifier(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=10,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "                mean_score = scores.mean()\n",
    "                \n",
    "                # è©³ç´°åˆ†æ\n",
    "                y_pred = cross_val_predict(rf, X_scaled, y, cv=cv)\n",
    "                report = classification_report(y, y_pred, \n",
    "                                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                                              output_dict=True)\n",
    "                \n",
    "                self.experiments[f'window_{ws}s'] = {\n",
    "                    'accuracy': mean_score,\n",
    "                    'n_samples': len(y),\n",
    "                    'stage1_recall': report['Stage 1']['recall'],\n",
    "                    'improvement': mean_score - self.baseline_score\n",
    "                }\n",
    "                \n",
    "                improvement_icon = \"ğŸ“ˆ\" if mean_score > self.baseline_score else \"ğŸ“‰\" if mean_score < self.baseline_score else \"â¡ï¸\"\n",
    "                \n",
    "                print(f\"{improvement_icon} çª—å£ {ws}s: æº–ç¢ºç‡={mean_score:.3f} ({mean_score-self.baseline_score:+.3f}), æ¨£æœ¬æ•¸={len(y)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ çª—å£ {ws}s å¤±æ•—: {e}\")\n",
    "                self.experiments[f'window_{ws}s'] = {'accuracy': 0, 'error': str(e)}\n",
    "    \n",
    "    def _experiment_feature_selection(self):\n",
    "        \"\"\"å¯¦é©— 3: æ¸¬è©¦ç‰¹å¾µé¸æ“‡\"\"\"\n",
    "        # è¼‰å…¥åŸºæº–ç‰¹å¾µ\n",
    "        with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X, y = data['features'], data['labels']\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒæ•¸é‡çš„ç‰¹å¾µ\n",
    "        n_features_list = [10, 20, 30, 40, 53]\n",
    "        \n",
    "        for n_features in n_features_list:\n",
    "            if n_features >= X.shape[1]:\n",
    "                # ä½¿ç”¨å…¨éƒ¨ç‰¹å¾µ\n",
    "                X_selected = X_scaled\n",
    "            else:\n",
    "                # ç‰¹å¾µé¸æ“‡\n",
    "                selector = SelectKBest(f_classif, k=n_features)\n",
    "                X_selected = selector.fit_transform(X_scaled, y)\n",
    "                \n",
    "                # é¡¯ç¤ºé¸ä¸­çš„ç‰¹å¾µ\n",
    "                if n_features == 20:\n",
    "                    selected_features = selector.get_support(indices=True)\n",
    "                    feature_names = data['features'].columns\n",
    "                    print(f\"\\n   Top {n_features} ç‰¹å¾µ:\")\n",
    "                    for i, idx in enumerate(selected_features[:10], 1):\n",
    "                        print(f\"      {i:2d}. {feature_names[idx]}\")\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(rf, X_selected, y, cv=cv, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            self.experiments[f'features_{n_features}'] = {\n",
    "                'accuracy': mean_score,\n",
    "                'n_features': n_features,\n",
    "                'improvement': mean_score - self.baseline_score\n",
    "            }\n",
    "            \n",
    "            improvement_icon = \"ğŸ“ˆ\" if mean_score > self.baseline_score else \"ğŸ“‰\" if mean_score < self.baseline_score else \"â¡ï¸\"\n",
    "            \n",
    "            print(f\"{improvement_icon} ä½¿ç”¨ {n_features:2d} å€‹ç‰¹å¾µ: æº–ç¢ºç‡={mean_score:.3f} ({mean_score-self.baseline_score:+.3f})\")\n",
    "    \n",
    "    def _generate_summary_report(self):\n",
    "        \"\"\"ç”Ÿæˆç¸½çµå ±å‘Š\"\"\"\n",
    "        # æ‰¾å‡ºæœ€ä½³æ–¹æ¡ˆ\n",
    "        valid_experiments = {k: v for k, v in self.experiments.items() \n",
    "                            if 'accuracy' in v and v['accuracy'] > 0}\n",
    "        \n",
    "        if not valid_experiments:\n",
    "            print(\"âš ï¸  æ²’æœ‰æœ‰æ•ˆçš„å¯¦é©—çµæœ\")\n",
    "            return\n",
    "        \n",
    "        best_exp = max(valid_experiments.items(), key=lambda x: x[1]['accuracy'])\n",
    "        best_name, best_result = best_exp\n",
    "        \n",
    "        print(f\"\\nğŸ† æœ€ä½³æ–¹æ¡ˆ: {best_name}\")\n",
    "        print(f\"   æº–ç¢ºç‡: {best_result['accuracy']:.3f}\")\n",
    "        print(f\"   æ”¹é€²å¹…åº¦: {best_result['improvement']:+.3f}\")\n",
    "        \n",
    "        if best_result['accuracy'] > self.baseline_score:\n",
    "            improvement_pct = (best_result['improvement'] / self.baseline_score) * 100\n",
    "            print(f\"\\nâœ… æ‰¾åˆ°æ”¹é€²æ–¹æ¡ˆï¼æ¯”åŸºæº–æå‡ {improvement_pct:.2f}%\")\n",
    "            print(f\"   å¾ {self.baseline_score:.3f} â†’ {best_result['accuracy']:.3f}\")\n",
    "        elif best_result['accuracy'] == self.baseline_score:\n",
    "            print(f\"\\nâ¡ï¸  æ”¹é€²æ–¹æ¡ˆèˆ‡åŸºæº–æŒå¹³ï¼Œæ¨¡å‹å·²é”æœ€å„ª\")\n",
    "        else:\n",
    "            print(f\"\\nâ¡ï¸  åŸºæº–æ¨¡å‹ (120sçª—å£ + 53ç‰¹å¾µ) å·²ç¶“æ˜¯æœ€å„ªé…ç½®\")\n",
    "        \n",
    "        # å„²å­˜å ±å‘Š\n",
    "        report_df = pd.DataFrame([\n",
    "            {\n",
    "                'experiment': name,\n",
    "                'accuracy': result.get('accuracy', 0),\n",
    "                'improvement': result.get('improvement', 0),\n",
    "                **{k: v for k, v in result.items() if k not in ['accuracy', 'improvement']}\n",
    "            }\n",
    "            for name, result in self.experiments.items()\n",
    "        ]).sort_values('accuracy', ascending=False)\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'improvement_report.csv')\n",
    "        report_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ’¾ å®Œæ•´å ±å‘Š: {csv_path}\")\n",
    "        \n",
    "        # è¦–è¦ºåŒ–å°æ¯”\n",
    "        self._plot_comparison(report_df)\n",
    "    \n",
    "    def _plot_comparison(self, report_df):\n",
    "        \"\"\"ç¹ªè£½å°æ¯”åœ–\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            experiments = report_df['experiment'].tolist()\n",
    "            accuracies = report_df['accuracy'].tolist()\n",
    "            \n",
    "            colors = ['green' if acc > self.baseline_score else 'red' if acc < self.baseline_score else 'gray' \n",
    "                     for acc in accuracies]\n",
    "            \n",
    "            ax.barh(experiments, accuracies, color=colors, alpha=0.6)\n",
    "            ax.axvline(self.baseline_score, color='blue', linestyle='--', linewidth=2,\n",
    "                      label=f'åŸºæº– ({self.baseline_score:.3f})')\n",
    "            \n",
    "            ax.set_xlabel('æº–ç¢ºç‡', fontsize=12)\n",
    "            ax.set_title('æ¨¡å‹æ”¹é€²å¯¦é©—å°æ¯”ï¼ˆçœŸå¯¦æ¨™ç±¤ï¼‰', fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = os.path.join(self.output_dir, 'improvement_comparison.png')\n",
    "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"ğŸ“Š å°æ¯”åœ–: {plot_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ç„¡æ³•ç¹ªè£½åœ–è¡¨: {e}\")\n",
    "\n",
    "# åŸ·è¡Œæ”¹é€²å¯¦é©—\n",
    "print(\"âœ… æº–å‚™åŸ·è¡Œæ”¹é€²å¯¦é©—...\")\n",
    "print(\"   ç¢ºä¿å·²åŸ·è¡Œï¼š\")\n",
    "print(\"   - arduino_features å·²è¼‰å…¥\")\n",
    "print(\"   - maturity_levels å·²é‡æ–°æ˜ å°„ç‚º 4 ç­‰ç´š\")\n",
    "print(\"   - FeatureEngineering å·²å®šç¾©ï¼ˆCell 11ï¼‰\\n\")\n",
    "\n",
    "improver = ModelImprovement()\n",
    "results = improver.run_all_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "516887c3-586d-4d40-a617-c5de710620f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹åš´æ ¼é©—è­‰...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ æ¸¬è©¦æ›´å°çª—å£ï¼ˆ30s, 45sï¼‰\n",
      "======================================================================\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 30 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 30 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 119 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 90 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 150 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 90 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 89 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 120 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (898, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 898\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(370), np.int64(1): np.int64(325), np.int64(2): np.int64(144), np.int64(3): np.int64(59)}\n",
      "============================================================\n",
      "âœ… çª—å£ 30s (LOSO): æº–ç¢ºç‡=0.622, æ¨£æœ¬æ•¸=898\n",
      "\n",
      "ğŸ” æ¸¬è©¦çª—å£å¤§å°: 45 ç§’\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 45 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 79 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 80 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 80 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 100 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 80 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (598, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 598\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(246), np.int64(1): np.int64(218), np.int64(2): np.int64(95), np.int64(3): np.int64(39)}\n",
      "============================================================\n",
      "âœ… çª—å£ 45s (LOSO): æº–ç¢ºç‡=0.617, æ¨£æœ¬æ•¸=598\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ LOSO åš´æ ¼é©—è­‰ï¼š60ç§’çª—å£\n",
      "======================================================================\n",
      "èªªæ˜: Leave-One-Subject-Out (LOSO)\n",
      "      æ¯æ¬¡ç•™ä¸€å€‹é³³æ¢¨åšæ¸¬è©¦ï¼Œç¢ºä¿æ¨¡å‹èƒ½æ³›åŒ–åˆ°æ–°é³³æ¢¨\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 44 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (448, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 448\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(185), np.int64(1): np.int64(163), np.int64(2): np.int64(71), np.int64(3): np.int64(29)}\n",
      "============================================================\n",
      "   ğŸ Pineapple 04: æ¸¬è©¦æ¨£æœ¬= 59, æº–ç¢ºç‡=0.627\n",
      "   ğŸ Pineapple 02: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.750\n",
      "   ğŸ Pineapple 03: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.500\n",
      "   ğŸ Pineapple 06: æ¸¬è©¦æ¨£æœ¬= 45, æº–ç¢ºç‡=0.667\n",
      "   ğŸ Pineapple 01: æ¸¬è©¦æ¨£æœ¬= 75, æº–ç¢ºç‡=0.600\n",
      "   ğŸ Pineapple 08: æ¸¬è©¦æ¨£æœ¬= 45, æº–ç¢ºç‡=0.667\n",
      "   ğŸ Pineapple 07: æ¸¬è©¦æ¨£æœ¬= 44, æº–ç¢ºç‡=0.477\n",
      "   ğŸ Pineapple 05: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.500\n",
      "\n",
      "   è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "                 precision    recall  f1-score   support\n",
      "   \n",
      "        Stage 0      0.656     0.741     0.695       185\n",
      "        Stage 1      0.586     0.564     0.575       163\n",
      "        Stage 2      0.667     0.141     0.233        71\n",
      "        Stage 3      0.433     1.000     0.604        29\n",
      "   \n",
      "       accuracy                          0.598       448\n",
      "      macro avg      0.585     0.611     0.527       448\n",
      "   weighted avg      0.618     0.598     0.572       448\n",
      "   \n",
      "\n",
      "   æ··æ·†çŸ©é™£:\n",
      "   [[137  40   0   8]\n",
      "    [ 43  92   5  23]\n",
      "    [ 29  25  10   7]\n",
      "    [  0   0   0  29]]\n",
      "\n",
      "======================================================================\n",
      "âœ… LOSO æœ€çµ‚çµæœ\n",
      "======================================================================\n",
      "   çª—å£å¤§å°: 60 ç§’\n",
      "   ç¸½æ¨£æœ¬æ•¸: 448\n",
      "   ç¸½é³³æ¢¨æ•¸: 8\n",
      "   LOSO æº–ç¢ºç‡: 0.598\n",
      "\n",
      "   âœ… æ­¤æº–ç¢ºç‡ä»£è¡¨ã€Œæ¨¡å‹èƒ½æ³›åŒ–åˆ°å¾æœªè¦‹éçš„é³³æ¢¨ã€çš„èƒ½åŠ›\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•\n",
      "======================================================================\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆå›ºå®šæ™‚é–“çª—ï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 44 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 53 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (448, 53)\n",
      "   æ¨™ç±¤æ•¸é‡: 448\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(185), np.int64(1): np.int64(163), np.int64(2): np.int64(71), np.int64(3): np.int64(29)}\n",
      "============================================================\n",
      "\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "\n",
      "æ–¹æ³•                             æº–ç¢ºç‡        èªªæ˜\n",
      "----------------------------------------------------------------------\n",
      "Stratified 5-Fold (ç›®å‰)         0.984      æ¨™æº–é©—è­‰\n",
      "LOSO (æœ€åš´æ ¼)                     0.598      æ³›åŒ–åˆ°æ–°é³³æ¢¨\n",
      "\n",
      "å·®ç•°: 0.386 (LOSOè¼ƒåš´æ ¼)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ åš´æ ¼é©—è­‰æœ€çµ‚å ±å‘Š\n",
      "======================================================================\n",
      "\n",
      "ğŸ† æœ€ä½³é…ç½®ï¼ˆLOSO é©—è­‰ï¼‰\n",
      "   window_30s\n",
      "   æº–ç¢ºç‡: 0.622\n",
      "   æ¨£æœ¬æ•¸: 898\n",
      "\n",
      "ğŸ’¾ è©³ç´°çµæœ: models/strict_validation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== åš´æ ¼é©—è­‰ï¼šLOSO + æ›´å°çª—å£æ¸¬è©¦ =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "class StrictValidation:\n",
    "    \"\"\"æ›´åš´æ ¼çš„æ¨¡å‹é©—è­‰\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def test_smaller_windows(self):\n",
    "        \"\"\"æ¸¬è©¦æ›´å°çš„çª—å£\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸ”¬ æ¸¬è©¦æ›´å°çª—å£ï¼ˆ30s, 45sï¼‰\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        window_sizes = [30, 45]\n",
    "        \n",
    "        for ws in window_sizes:\n",
    "            print(f\"\\nğŸ” æ¸¬è©¦çª—å£å¤§å°: {ws} ç§’\")\n",
    "            \n",
    "            try:\n",
    "                # é‡æ–°æå–ç‰¹å¾µ\n",
    "                feature_engineer = FeatureEngineering(\n",
    "                    arduino_features, \n",
    "                    maturity_levels,\n",
    "                    window_size=ws\n",
    "                )\n",
    "                \n",
    "                X, y, metadata = feature_engineer.extract_all_features()\n",
    "                \n",
    "                # æ¨™æº–åŒ–\n",
    "                scaler = StandardScaler()\n",
    "                X_scaled = scaler.fit_transform(X)\n",
    "                \n",
    "                # LOSO é©—è­‰\n",
    "                loso_acc = self._loso_validation(X_scaled, y, metadata, ws)\n",
    "                \n",
    "                self.results[f'window_{ws}s'] = {\n",
    "                    'accuracy': loso_acc,\n",
    "                    'n_samples': len(y),\n",
    "                    'validation': 'LOSO'\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ… çª—å£ {ws}s (LOSO): æº–ç¢ºç‡={loso_acc:.3f}, æ¨£æœ¬æ•¸={len(y)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ çª—å£ {ws}s å¤±æ•—: {e}\")\n",
    "    \n",
    "    def validate_best_model(self, window_size=60):\n",
    "        \"\"\"ç”¨ LOSO åš´æ ¼é©—è­‰æœ€ä½³æ¨¡å‹ï¼ˆ60ç§’çª—å£ï¼‰\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸ”¬ LOSO åš´æ ¼é©—è­‰ï¼š{window_size}ç§’çª—å£\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"èªªæ˜: Leave-One-Subject-Out (LOSO)\")\n",
    "        print(\"      æ¯æ¬¡ç•™ä¸€å€‹é³³æ¢¨åšæ¸¬è©¦ï¼Œç¢ºä¿æ¨¡å‹èƒ½æ³›åŒ–åˆ°æ–°é³³æ¢¨\\n\")\n",
    "        \n",
    "        # é‡æ–°æå–ç‰¹å¾µ\n",
    "        feature_engineer = FeatureEngineering(\n",
    "            arduino_features, \n",
    "            maturity_levels,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        \n",
    "        X, y, metadata = feature_engineer.extract_all_features()\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # LOSO é©—è­‰\n",
    "        loso_acc = self._loso_validation(X_scaled, y, metadata, window_size, verbose=True)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"âœ… LOSO æœ€çµ‚çµæœ\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"   çª—å£å¤§å°: {window_size} ç§’\")\n",
    "        print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "        print(f\"   ç¸½é³³æ¢¨æ•¸: {len(np.unique(metadata['pineapple_id']))}\")\n",
    "        print(f\"   LOSO æº–ç¢ºç‡: {loso_acc:.3f}\")\n",
    "        print(f\"\\n   âœ… æ­¤æº–ç¢ºç‡ä»£è¡¨ã€Œæ¨¡å‹èƒ½æ³›åŒ–åˆ°å¾æœªè¦‹éçš„é³³æ¢¨ã€çš„èƒ½åŠ›\")\n",
    "        \n",
    "        self.results[f'best_model_window_{window_size}s_LOSO'] = {\n",
    "            'accuracy': loso_acc,\n",
    "            'validation': 'LOSO',\n",
    "            'n_samples': len(y)\n",
    "        }\n",
    "        \n",
    "        return loso_acc\n",
    "    \n",
    "    def _loso_validation(self, X, y, metadata, window_size, verbose=False):\n",
    "        \"\"\"åŸ·è¡Œ Leave-One-Subject-Out äº¤å‰é©—è­‰\"\"\"\n",
    "        pineapple_ids = metadata['pineapple_id'].unique()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        for test_pid in pineapple_ids:\n",
    "            # è¨“ç·´é›†ï¼šæ’é™¤ç•¶å‰é³³æ¢¨\n",
    "            train_mask = metadata['pineapple_id'] != test_pid\n",
    "            test_mask = metadata['pineapple_id'] == test_pid\n",
    "            \n",
    "            X_train, y_train = X[train_mask], y[train_mask]\n",
    "            X_test, y_test = X[test_mask], y[test_mask]\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            # é æ¸¬\n",
    "            y_pred = rf.predict(X_test)\n",
    "            \n",
    "            # è¨ˆç®—è©²é³³æ¢¨çš„æº–ç¢ºç‡\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   ğŸ Pineapple {test_pid}: \"\n",
    "                      f\"æ¸¬è©¦æ¨£æœ¬={len(y_test):3d}, \"\n",
    "                      f\"æº–ç¢ºç‡={acc:.3f}\")\n",
    "        \n",
    "        # ç¸½é«”æº–ç¢ºç‡\n",
    "        overall_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n   è©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "            report = classification_report(\n",
    "                all_true, all_preds,\n",
    "                target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                digits=3\n",
    "            )\n",
    "            print(\"   \" + report.replace(\"\\n\", \"\\n   \"))\n",
    "            \n",
    "            print(f\"\\n   æ··æ·†çŸ©é™£:\")\n",
    "            cm = confusion_matrix(all_true, all_preds)\n",
    "            print(\"   \" + str(cm).replace(\"\\n\", \"\\n   \"))\n",
    "        \n",
    "        return overall_acc\n",
    "    \n",
    "    def compare_validation_methods(self, window_size=60):\n",
    "        \"\"\"æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•çš„çµæœ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # è¼‰å…¥ 60ç§’çª—å£çš„ç‰¹å¾µ\n",
    "        feature_engineer = FeatureEngineering(\n",
    "            arduino_features, \n",
    "            maturity_levels,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        X, y, metadata = feature_engineer.extract_all_features()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # 1. Stratified 5-Foldï¼ˆç›®å‰ç”¨çš„ï¼‰\n",
    "        from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        kfold_scores = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "        kfold_acc = kfold_scores.mean()\n",
    "        \n",
    "        # 2. LOSOï¼ˆæœ€åš´æ ¼ï¼‰\n",
    "        loso_acc = self._loso_validation(X_scaled, y, metadata, window_size)\n",
    "        \n",
    "        print(f\"\\nçª—å£å¤§å°: {window_size} ç§’\\n\")\n",
    "        print(f\"{'æ–¹æ³•':<30s} {'æº–ç¢ºç‡':<10s} {'èªªæ˜'}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Stratified 5-Fold (ç›®å‰)':<30s} {kfold_acc:.3f}      æ¨™æº–é©—è­‰\")\n",
    "        print(f\"{'LOSO (æœ€åš´æ ¼)':<30s} {loso_acc:.3f}      æ³›åŒ–åˆ°æ–°é³³æ¢¨\")\n",
    "        print(f\"\\nå·®ç•°: {abs(kfold_acc - loso_acc):.3f} \"\n",
    "              f\"({'LOSOè¼ƒåš´æ ¼' if loso_acc < kfold_acc else 'çµæœä¸€è‡´'})\")\n",
    "        \n",
    "        return {\n",
    "            'kfold': kfold_acc,\n",
    "            'loso': loso_acc\n",
    "        }\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"ç”Ÿæˆæœ€çµ‚å ±å‘Š\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“‹ åš´æ ¼é©—è­‰æœ€çµ‚å ±å‘Š\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"âš ï¸  æ²’æœ‰é©—è­‰çµæœ\")\n",
    "            return\n",
    "        \n",
    "        # æ‰¾å‡ºæœ€ä½³é…ç½®\n",
    "        best = max(self.results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        best_name, best_result = best\n",
    "        \n",
    "        print(f\"\\nğŸ† æœ€ä½³é…ç½®ï¼ˆLOSO é©—è­‰ï¼‰\")\n",
    "        print(f\"   {best_name}\")\n",
    "        print(f\"   æº–ç¢ºç‡: {best_result['accuracy']:.3f}\")\n",
    "        print(f\"   æ¨£æœ¬æ•¸: {best_result['n_samples']}\")\n",
    "        \n",
    "        # å„²å­˜çµæœ\n",
    "        results_df = pd.DataFrame([\n",
    "            {'config': name, **result}\n",
    "            for name, result in self.results.items()\n",
    "        ]).sort_values('accuracy', ascending=False)\n",
    "        \n",
    "        results_df.to_csv('models/strict_validation_results.csv', \n",
    "                         index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ’¾ è©³ç´°çµæœ: models/strict_validation_results.csv\")\n",
    "\n",
    "# ===== åŸ·è¡Œåš´æ ¼é©—è­‰ =====\n",
    "print(\"ğŸš€ é–‹å§‹åš´æ ¼é©—è­‰...\\n\")\n",
    "\n",
    "validator = StrictValidation()\n",
    "\n",
    "# 1. æ¸¬è©¦æ›´å°çª—å£ï¼ˆ30s, 45sï¼‰\n",
    "validator.test_smaller_windows()\n",
    "\n",
    "# 2. ç”¨ LOSO é©—è­‰æœ€ä½³æ¨¡å‹ï¼ˆ60sï¼‰\n",
    "validator.validate_best_model(window_size=60)\n",
    "\n",
    "# 3. æ¯”è¼ƒä¸åŒé©—è­‰æ–¹æ³•\n",
    "validator.compare_validation_methods(window_size=60)\n",
    "\n",
    "# 4. ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "validator.generate_final_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13f45272-3afa-4fe6-9529-7e1d9573707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åŸ·è¡Œæ–¹æ¡ˆ Aï¼šæ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆæ¶ˆé™¤å€‹é«”å·®ç•°ç‰ˆï¼‰\n",
      "============================================================\n",
      "çª—å£å¤§å°: 60 ç§’\n",
      "æ”¹é€²ç­–ç•¥:\n",
      "  âœ“ é³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\n",
      "  âœ“ ç›¸å°è®ŠåŒ–ç‰¹å¾µ\n",
      "  âœ“ æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 04...\n",
      "   âœ“ æå– 59 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:3023: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:3024: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 03...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 06...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 01...\n",
      "   âœ“ æå– 75 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 08...\n",
      "   âœ“ æå– 45 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 07...\n",
      "   âœ“ æå– 44 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "ğŸ è™•ç† Pineapple 05...\n",
      "   âœ“ æå– 60 å€‹çª—å£ï¼Œæ¯å€‹ 66 ç¶­ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "âœ… ç‰¹å¾µæå–å®Œæˆï¼\n",
      "   ç‰¹å¾µçŸ©é™£: (448, 66)\n",
      "   æ¨™ç±¤æ•¸é‡: 448\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(185), np.int64(1): np.int64(163), np.int64(2): np.int64(71), np.int64(3): np.int64(29)}\n",
      "============================================================\n",
      "\n",
      "ğŸ’¾ å„²å­˜æ”¹é€²ç‰ˆç‰¹å¾µ...\n",
      "   âœ… CSV: data/processed_normalized/feature_matrix_normalized.csv\n",
      "   âœ… Labels: data/processed_normalized/labels_normalized.npy\n",
      "   âœ… Metadata: data/processed_normalized/metadata_normalized.csv\n",
      "   âœ… Pickle: data/processed_normalized/feature_data_normalized.pkl\n",
      "\n",
      "âœ… æ”¹é€²ç‰ˆç‰¹å¾µå„²å­˜å®Œæˆï¼\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ”¹é€²ç‰ˆç‰¹å¾µæ‘˜è¦:\n",
      "   ç¸½æ¨£æœ¬æ•¸: 448\n",
      "   ç‰¹å¾µç¶­åº¦: 66 (åŸæœ¬ 53 â†’ ç¾åœ¨ 66)\n",
      "   æ–°å¢ç‰¹å¾µ: äºŒéšå·®åˆ†ã€ç›¸å°è®ŠåŒ–ã€æ„Ÿæ¸¬å™¨ç›¸é—œæ€§\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ A: æ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹ =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ImprovedFeatureEngineering:\n",
    "    \"\"\"\n",
    "    æ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹\n",
    "    \n",
    "    æ”¹é€²ç­–ç•¥:\n",
    "    1. é³³æ¢¨å…§éƒ¨æ­£è¦åŒ–ï¼ˆZ-score per pineappleï¼‰\n",
    "    2. ç›¸å°è®ŠåŒ–ç‰¹å¾µï¼ˆç›¸å°æ–¼åˆå§‹ç‹€æ…‹ï¼‰\n",
    "    3. æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µ\n",
    "    4. æ¸›å°‘çµ•å°æ•¸å€¼ä¾è³´\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, arduino_features, maturity_labels, window_size=60):\n",
    "        self.arduino_features = arduino_features\n",
    "        self.maturity_labels = maturity_labels\n",
    "        self.window_size = window_size\n",
    "        self.sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "        \n",
    "        self.feature_matrix = None\n",
    "        self.labels = None\n",
    "        self.metadata = None\n",
    "        \n",
    "        self.output_dir = 'data/processed_normalized'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"æ‰¹æ¬¡æå–æ‰€æœ‰ç‰¹å¾µï¼ˆæ”¹é€²ç‰ˆï¼‰\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”§ Step 4: ç‰¹å¾µå·¥ç¨‹ï¼ˆæ¶ˆé™¤å€‹é«”å·®ç•°ç‰ˆï¼‰\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"çª—å£å¤§å°: {self.window_size} ç§’\")\n",
    "        print(\"æ”¹é€²ç­–ç•¥:\")\n",
    "        print(\"  âœ“ é³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\")\n",
    "        print(\"  âœ“ ç›¸å°è®ŠåŒ–ç‰¹å¾µ\")\n",
    "        print(\"  âœ“ æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µ\\n\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        for pid in self.arduino_features.keys():\n",
    "            if pid not in self.maturity_labels:\n",
    "                print(f\"âš ï¸  {pid}: æ‰¾ä¸åˆ°å°æ‡‰çš„æˆç†Ÿåº¦æ¨™ç±¤ï¼Œè·³é\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ğŸ è™•ç† Pineapple {pid}...\")\n",
    "            \n",
    "            # åˆä½µè©²é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\n",
    "            combined_df, combined_labels = self._combine_pineapple_data(pid)\n",
    "            \n",
    "            if combined_df is None:\n",
    "                continue\n",
    "            \n",
    "            # ğŸ”¥ é—œéµæ”¹é€²ï¼šé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\n",
    "            combined_df = self._normalize_per_pineapple(combined_df, pid)\n",
    "            \n",
    "            # æ»‘å‹•çª—å£æå–ç‰¹å¾µ\n",
    "            features, labels, metadata = self._sliding_window_extraction(\n",
    "                combined_df, combined_labels, pid\n",
    "            )\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            print(f\"   âœ“ æå– {len(features)} å€‹çª—å£ï¼Œæ¯å€‹ {features.shape[1]} ç¶­ç‰¹å¾µ\\n\")\n",
    "        \n",
    "        # åˆä½µæ‰€æœ‰é³³æ¢¨çš„ç‰¹å¾µ\n",
    "        self.feature_matrix = pd.DataFrame(\n",
    "            np.vstack(all_features),\n",
    "            columns=self._get_feature_names()\n",
    "        )\n",
    "        self.labels = np.hstack(all_labels)\n",
    "        self.metadata = pd.DataFrame(all_metadata)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"âœ… ç‰¹å¾µæå–å®Œæˆï¼\")\n",
    "        print(f\"   ç‰¹å¾µçŸ©é™£: {self.feature_matrix.shape}\")\n",
    "        print(f\"   æ¨™ç±¤æ•¸é‡: {len(self.labels)}\")\n",
    "        print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.feature_matrix, self.labels, self.metadata\n",
    "    \n",
    "    def _normalize_per_pineapple(self, df, pid):\n",
    "        \"\"\"\n",
    "        ğŸ”¥ é—œéµæ”¹é€²ï¼šé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–\n",
    "        \n",
    "        å°æ¯é¡†é³³æ¢¨çš„æ„Ÿæ¸¬å™¨æ•¸æ“šåš Z-score æ¨™æº–åŒ–ï¼Œ\n",
    "        æ¶ˆé™¤å€‹é«”åŸºç·šå·®ç•°\n",
    "        \"\"\"\n",
    "        df_normalized = df.copy()\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "            if col_rs_r0 in df.columns:\n",
    "                values = df[col_rs_r0].values\n",
    "                \n",
    "                # Z-score æ¨™æº–åŒ–ï¼ˆè©²é³³æ¢¨å…§éƒ¨ï¼‰\n",
    "                mean = np.mean(values)\n",
    "                std = np.std(values)\n",
    "                \n",
    "                if std > 0:\n",
    "                    df_normalized[col_rs_r0] = (values - mean) / std\n",
    "                else:\n",
    "                    df_normalized[col_rs_r0] = 0\n",
    "        \n",
    "        return df_normalized\n",
    "    \n",
    "    def _combine_pineapple_data(self, pid):\n",
    "        \"\"\"åˆä½µå–®é¡†é³³æ¢¨çš„æ‰€æœ‰æ—¥æœŸæ•¸æ“š\"\"\"\n",
    "        df_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        date_dict = self.arduino_features[pid]\n",
    "        labels = self.maturity_labels[pid]\n",
    "        \n",
    "        offset = 0\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date].copy()\n",
    "            n_samples = len(df)\n",
    "            \n",
    "            window_labels = labels[offset:offset+n_samples]\n",
    "            \n",
    "            df_list.append(df)\n",
    "            label_list.append(window_labels)\n",
    "            \n",
    "            offset += n_samples\n",
    "        \n",
    "        if not df_list:\n",
    "            return None, None\n",
    "        \n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        combined_labels = np.hstack(label_list)\n",
    "        \n",
    "        return combined_df, combined_labels\n",
    "    \n",
    "    def _sliding_window_extraction(self, df, labels, pid):\n",
    "        \"\"\"æ»‘å‹•çª—å£æå–ç‰¹å¾µ\"\"\"\n",
    "        features = []\n",
    "        window_labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        n_samples = len(df)\n",
    "        step_size = self.window_size\n",
    "        \n",
    "        for start_idx in range(0, n_samples - self.window_size + 1, step_size):\n",
    "            end_idx = start_idx + self.window_size\n",
    "            \n",
    "            window_df = df.iloc[start_idx:end_idx]\n",
    "            window_label_array = labels[start_idx:end_idx]\n",
    "            \n",
    "            # è©²çª—å£çš„ä¸»è¦æ¨™ç±¤\n",
    "            unique, counts = np.unique(window_label_array, return_counts=True)\n",
    "            if len(unique) == 0:\n",
    "                continue\n",
    "            majority_label = unique[np.argmax(counts)]\n",
    "            \n",
    "            # ğŸ”¥ æå–æ”¹é€²ç‰ˆç‰¹å¾µ\n",
    "            feature_vector = self._extract_improved_features(window_df, start_idx, end_idx)\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            window_labels.append(majority_label)\n",
    "            metadata.append({\n",
    "                'pineapple_id': pid,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'majority_label': int(majority_label),\n",
    "                'label_purity': max(counts) / len(window_label_array)\n",
    "            })\n",
    "        \n",
    "        return np.array(features), np.array(window_labels), metadata\n",
    "    \n",
    "    def _extract_improved_features(self, window_df, start_idx, end_idx):\n",
    "        \"\"\"\n",
    "        ğŸ”¥ æ”¹é€²ç‰ˆç‰¹å¾µæå–\n",
    "        \n",
    "        é‡é»ï¼š\n",
    "        1. å·²ç¶“åšéé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–ï¼ˆZ-scoreï¼‰ï¼Œæ‰€ä»¥å‡å€¼/æœ€å€¼æ›´æœ‰æ„ç¾©\n",
    "        2. å¢åŠ æ™‚é–“åºåˆ—å·®åˆ†ç‰¹å¾µï¼ˆä¸€éšã€äºŒéšï¼‰\n",
    "        3. å¢åŠ ç›¸å°è®ŠåŒ–ç‰¹å¾µ\n",
    "        4. æ¸›å°‘è·¨æ„Ÿæ¸¬å™¨æ¯”ä¾‹ï¼ˆå› ç‚ºå·²æ­£è¦åŒ–ï¼Œæ¯”ä¾‹æ„ç¾©é™ä½ï¼‰\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "            col_delta = f'{sensor}_delta_Rs_R0'\n",
    "            \n",
    "            if col_rs_r0 in window_df.columns:\n",
    "                data = window_df[col_rs_r0].values\n",
    "                \n",
    "                # === åŸºæœ¬çµ±è¨ˆç‰¹å¾µï¼ˆæ­£è¦åŒ–å¾Œï¼‰===\n",
    "                features.append(np.mean(data))           # å¹³å‡å€¼\n",
    "                features.append(np.std(data))            # æ¨™æº–å·®\n",
    "                features.append(np.min(data))            # æœ€å°å€¼\n",
    "                features.append(np.max(data))            # æœ€å¤§å€¼\n",
    "                features.append(np.max(data) - np.min(data))  # åæ‡‰å¹…åº¦\n",
    "                \n",
    "                # === æ™‚é–“åºåˆ—ç‰¹å¾µ ===\n",
    "                # æ–œç‡ï¼ˆä¸€éšè®ŠåŒ–è¶¨å‹¢ï¼‰\n",
    "                if len(data) > 1:\n",
    "                    x = np.arange(len(data))\n",
    "                    slope, _, _, _, _ = linregress(x, data)\n",
    "                    features.append(slope)\n",
    "                else:\n",
    "                    features.append(0)\n",
    "                \n",
    "                # äºŒéšå·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼Œè®ŠåŒ–çš„è®ŠåŒ–ï¼‰\n",
    "                if len(data) > 2:\n",
    "                    first_diff = np.diff(data)\n",
    "                    second_diff = np.diff(first_diff)\n",
    "                    features.append(np.mean(second_diff))    # å¹³å‡åŠ é€Ÿåº¦\n",
    "                    features.append(np.std(second_diff))     # åŠ é€Ÿåº¦è®Šç•°\n",
    "                else:\n",
    "                    features.extend([0, 0])\n",
    "                \n",
    "                # AUCï¼ˆç´¯ç©åæ‡‰ï¼‰\n",
    "                try:\n",
    "                    auc = np.trapezoid(data, dx=1)\n",
    "                except AttributeError:\n",
    "                    if len(data) > 1:\n",
    "                        auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                    else:\n",
    "                        auc = data[0] if len(data) > 0 else 0\n",
    "                features.append(auc)\n",
    "                \n",
    "                # === ç›¸å°è®ŠåŒ–ç‰¹å¾µï¼ˆç›¸å°æ–¼çª—å£åˆå§‹å€¼ï¼‰===\n",
    "                if len(data) > 0 and abs(data[0]) > 1e-6:\n",
    "                    relative_change = (data[-1] - data[0]) / (abs(data[0]) + 1e-6)\n",
    "                    features.append(relative_change)\n",
    "                else:\n",
    "                    features.append(0)\n",
    "            else:\n",
    "                features.extend([0] * 10)  # 10 å€‹ç‰¹å¾µ\n",
    "            \n",
    "            # === Delta ç‰¹å¾µï¼ˆç¬æ™‚è®ŠåŒ–ï¼‰===\n",
    "            if col_delta in window_df.columns:\n",
    "                data = window_df[col_delta].values\n",
    "                \n",
    "                features.append(np.mean(data))\n",
    "                features.append(np.std(data))\n",
    "                features.append(np.max(np.abs(data)))\n",
    "            else:\n",
    "                features.extend([0] * 3)\n",
    "        \n",
    "        # === è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µï¼ˆæ¸›å°‘ï¼Œå› ç‚ºæ­£è¦åŒ–å¾Œæ¯”ä¾‹æ„ç¾©é™ä½ï¼‰===\n",
    "        # åªä¿ç•™é—œéµçš„å”æ–¹å·®ç‰¹å¾µ\n",
    "        mq3_mean = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0\n",
    "        mq135_mean = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0\n",
    "        \n",
    "        # MQ3 vs MQ135 çš„å”åŒè®ŠåŒ–ï¼ˆç›¸é—œæ€§ï¼‰\n",
    "        if 'MQ3_Rs_R0' in window_df.columns and 'MQ135_Rs_R0' in window_df.columns:\n",
    "            if len(window_df) > 1:\n",
    "                corr = np.corrcoef(window_df['MQ3_Rs_R0'], window_df['MQ135_Rs_R0'])[0, 1]\n",
    "                if np.isnan(corr):\n",
    "                    corr = 0\n",
    "            else:\n",
    "                corr = 0\n",
    "        else:\n",
    "            corr = 0\n",
    "        features.append(corr)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _get_feature_names(self):\n",
    "        \"\"\"ç”Ÿæˆç‰¹å¾µåç¨±\"\"\"\n",
    "        names = []\n",
    "        \n",
    "        for sensor in self.sensor_cols:\n",
    "            # åŸºæœ¬ç‰¹å¾µï¼ˆ10å€‹ï¼‰\n",
    "            names.extend([\n",
    "                f'{sensor}_mean_norm',           # æ­£è¦åŒ–å¾Œçš„å‡å€¼\n",
    "                f'{sensor}_std_norm',\n",
    "                f'{sensor}_min_norm',\n",
    "                f'{sensor}_max_norm',\n",
    "                f'{sensor}_range_norm',\n",
    "                f'{sensor}_slope',\n",
    "                f'{sensor}_accel_mean',          # äºŒéšå·®åˆ†\n",
    "                f'{sensor}_accel_std',\n",
    "                f'{sensor}_auc_norm',\n",
    "                f'{sensor}_relative_change'      # ç›¸å°è®ŠåŒ–\n",
    "            ])\n",
    "            \n",
    "            # Delta ç‰¹å¾µï¼ˆ3å€‹ï¼‰\n",
    "            names.extend([\n",
    "                f'{sensor}_delta_mean',\n",
    "                f'{sensor}_delta_std',\n",
    "                f'{sensor}_delta_max_abs'\n",
    "            ])\n",
    "        \n",
    "        # è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µï¼ˆ1å€‹ï¼‰\n",
    "        names.append('MQ3_MQ135_correlation')\n",
    "        \n",
    "        return names\n",
    "    \n",
    "    def save_features(self):\n",
    "        \"\"\"å„²å­˜ç‰¹å¾µèˆ‡æ¨™ç±¤\"\"\"\n",
    "        print(\"\\nğŸ’¾ å„²å­˜æ”¹é€²ç‰ˆç‰¹å¾µ...\")\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, 'feature_matrix_normalized.csv')\n",
    "        self.feature_matrix.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… CSV: {csv_path}\")\n",
    "        \n",
    "        label_path = os.path.join(self.output_dir, 'labels_normalized.npy')\n",
    "        np.save(label_path, self.labels)\n",
    "        print(f\"   âœ… Labels: {label_path}\")\n",
    "        \n",
    "        meta_path = os.path.join(self.output_dir, 'metadata_normalized.csv')\n",
    "        self.metadata.to_csv(meta_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… Metadata: {meta_path}\")\n",
    "        \n",
    "        pkl_path = os.path.join(self.output_dir, 'feature_data_normalized.pkl')\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'features': self.feature_matrix,\n",
    "                'labels': self.labels,\n",
    "                'metadata': self.metadata,\n",
    "                'feature_names': self.feature_matrix.columns.tolist(),\n",
    "                'window_size': self.window_size\n",
    "            }, f)\n",
    "        print(f\"   âœ… Pickle: {pkl_path}\")\n",
    "        \n",
    "        print(\"\\nâœ… æ”¹é€²ç‰ˆç‰¹å¾µå„²å­˜å®Œæˆï¼\")\n",
    "\n",
    "# ===== åŸ·è¡Œæ”¹é€²ç‰ˆç‰¹å¾µå·¥ç¨‹ =====\n",
    "print(\"ğŸš€ åŸ·è¡Œæ–¹æ¡ˆ Aï¼šæ¶ˆé™¤å€‹é«”å·®ç•°çš„ç‰¹å¾µå·¥ç¨‹\\n\")\n",
    "\n",
    "improved_engineer = ImprovedFeatureEngineering(\n",
    "    arduino_features, \n",
    "    maturity_levels,\n",
    "    window_size=60  # å…ˆç”¨ 60 ç§’æ¸¬è©¦\n",
    ")\n",
    "\n",
    "X_improved, y_improved, metadata_improved = improved_engineer.extract_all_features()\n",
    "improved_engineer.save_features()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ”¹é€²ç‰ˆç‰¹å¾µæ‘˜è¦:\")\n",
    "print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(y_improved)}\")\n",
    "print(f\"   ç‰¹å¾µç¶­åº¦: {X_improved.shape[1]} (åŸæœ¬ 53 â†’ ç¾åœ¨ 66)\")\n",
    "print(f\"   æ–°å¢ç‰¹å¾µ: äºŒéšå·®åˆ†ã€ç›¸å°è®ŠåŒ–ã€æ„Ÿæ¸¬å™¨ç›¸é—œæ€§\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dbaef3c-551d-4894-93f1-f0b7571e05e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ LOSO é©—è­‰æ”¹é€²ç‰ˆç‰¹å¾µ...\n",
      "============================================================\n",
      "\n",
      "é€å€‹é³³æ¢¨æ¸¬è©¦ï¼ˆLOSOï¼‰:\n",
      "   ğŸ Pineapple 04: æ¸¬è©¦æ¨£æœ¬= 59, æº–ç¢ºç‡=0.305\n",
      "   ğŸ Pineapple 02: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.783\n",
      "   ğŸ Pineapple 03: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.383\n",
      "   ğŸ Pineapple 06: æ¸¬è©¦æ¨£æœ¬= 45, æº–ç¢ºç‡=0.178\n",
      "   ğŸ Pineapple 01: æ¸¬è©¦æ¨£æœ¬= 75, æº–ç¢ºç‡=0.773\n",
      "   ğŸ Pineapple 08: æ¸¬è©¦æ¨£æœ¬= 45, æº–ç¢ºç‡=0.889\n",
      "   ğŸ Pineapple 07: æ¸¬è©¦æ¨£æœ¬= 44, æº–ç¢ºç‡=0.886\n",
      "   ğŸ Pineapple 05: æ¸¬è©¦æ¨£æœ¬= 60, æº–ç¢ºç‡=0.233\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ”¹é€²å‰ vs æ”¹é€²å¾Œå°æ¯”\n",
      "============================================================\n",
      "æ”¹é€²å‰ (åŸå§‹ç‰¹å¾µ):    LOSO = 0.598 (59.8%)\n",
      "æ”¹é€²å¾Œ (æ­£è¦åŒ–ç‰¹å¾µ):  LOSO = 0.551 (55.1%)\n",
      "æ”¹é€²å¹…åº¦:             -4.7%\n",
      "============================================================\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.554     0.719     0.626       185\n",
      "     Stage 1      0.538     0.436     0.481       163\n",
      "     Stage 2      0.298     0.197     0.237        71\n",
      "     Stage 3      1.000     1.000     1.000        29\n",
      "\n",
      "    accuracy                          0.551       448\n",
      "   macro avg      0.597     0.588     0.586       448\n",
      "weighted avg      0.536     0.551     0.536       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== ç«‹å³ç”¨ LOSO æ¸¬è©¦æ”¹é€²æ•ˆæœ =====\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\nğŸ”¬ LOSO é©—è­‰æ”¹é€²ç‰ˆç‰¹å¾µ...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ”¹é€²ç‰ˆç‰¹å¾µ\n",
    "X = X_improved.values if hasattr(X_improved, 'values') else X_improved\n",
    "y = y_improved\n",
    "\n",
    "# æ¨™æº–åŒ–ï¼ˆå…¨å±€ï¼Œå› ç‚ºå·²ç¶“åšéé³³æ¢¨å…§éƒ¨æ­£è¦åŒ–ï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# LOSO é©—è­‰\n",
    "pineapple_ids = metadata_improved['pineapple_id'].unique()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "print(\"\\né€å€‹é³³æ¢¨æ¸¬è©¦ï¼ˆLOSOï¼‰:\")\n",
    "for test_pid in pineapple_ids:\n",
    "    train_mask = metadata_improved['pineapple_id'] != test_pid\n",
    "    test_mask = metadata_improved['pineapple_id'] == test_pid\n",
    "    \n",
    "    X_train, y_train = X_scaled[train_mask], y[train_mask]\n",
    "    X_test, y_test = X_scaled[test_mask], y[test_mask]\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred = rf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test)\n",
    "    \n",
    "    print(f\"   ğŸ Pineapple {test_pid}: æ¸¬è©¦æ¨£æœ¬={len(y_test):3d}, æº–ç¢ºç‡={acc:.3f}\")\n",
    "\n",
    "# ç¸½é«”çµæœ\n",
    "loso_acc_improved = accuracy_score(all_true, all_preds)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š æ”¹é€²å‰ vs æ”¹é€²å¾Œå°æ¯”\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"æ”¹é€²å‰ (åŸå§‹ç‰¹å¾µ):    LOSO = 0.598 (59.8%)\")\n",
    "print(f\"æ”¹é€²å¾Œ (æ­£è¦åŒ–ç‰¹å¾µ):  LOSO = {loso_acc_improved:.3f} ({loso_acc_improved*100:.1f}%)\")\n",
    "print(f\"æ”¹é€²å¹…åº¦:             {(loso_acc_improved - 0.598)*100:+.1f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# è©³ç´°å ±å‘Š\n",
    "print(\"\\nè©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "report = classification_report(\n",
    "    all_true, all_preds,\n",
    "    target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "    digits=3\n",
    ")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eed5ce4e-353e-491c-a6dc-5c961c01458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹åŸ·è¡Œæ–¹æ¡ˆ C\n",
      "\n",
      "\n",
      "ã€æ–¹æ³• 1ã€‘å–®å€‹æœ€ç›¸ä¼¼é³³æ¢¨æ¨¡å‹\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "ğŸ”¬ æ–¹æ¡ˆ C: Domain Adaptation (å¤šä»»å‹™å­¸ç¿’)\n",
      "============================================================\n",
      "ç­–ç•¥: ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹\n",
      "\n",
      "è¨“ç·´èˆ‡æ¸¬è©¦:\n",
      "   ğŸ æ¸¬è©¦ 04: æ¨£æœ¬= 29, æœ€ç›¸ä¼¼=03, è·é›¢=1.91, æº–ç¢ºç‡=0.552\n",
      "   ğŸ æ¸¬è©¦ 02: æ¨£æœ¬= 30, æœ€ç›¸ä¼¼=01, è·é›¢=1.00, æº–ç¢ºç‡=0.733\n",
      "   ğŸ æ¸¬è©¦ 03: æ¨£æœ¬= 30, æœ€ç›¸ä¼¼=04, è·é›¢=1.91, æº–ç¢ºç‡=0.500\n",
      "   ğŸ æ¸¬è©¦ 06: æ¨£æœ¬= 22, æœ€ç›¸ä¼¼=05, è·é›¢=1.34, æº–ç¢ºç‡=0.682\n",
      "   ğŸ æ¸¬è©¦ 01: æ¨£æœ¬= 37, æœ€ç›¸ä¼¼=02, è·é›¢=1.00, æº–ç¢ºç‡=0.676\n",
      "   ğŸ æ¸¬è©¦ 08: æ¨£æœ¬= 22, æœ€ç›¸ä¼¼=07, è·é›¢=2.82, æº–ç¢ºç‡=0.682\n",
      "   ğŸ æ¸¬è©¦ 07: æ¨£æœ¬= 22, æœ€ç›¸ä¼¼=08, è·é›¢=2.82, æº–ç¢ºç‡=0.636\n",
      "   ğŸ æ¸¬è©¦ 05: æ¨£æœ¬= 30, æœ€ç›¸ä¼¼=06, è·é›¢=1.34, æº–ç¢ºç‡=0.533\n",
      "\n",
      "============================================================\n",
      "âœ… Domain Adaptation çµæœ\n",
      "============================================================\n",
      "LOSO æº–ç¢ºç‡: 0.622 (62.2%)\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.593     0.785     0.676        93\n",
      "     Stage 1      0.590     0.568     0.579        81\n",
      "     Stage 2      0.714     0.147     0.244        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.622       222\n",
      "   macro avg      0.724     0.625     0.625       222\n",
      "weighted avg      0.636     0.622     0.595       222\n",
      "\n",
      "\n",
      "\n",
      "ã€æ–¹æ³• 2ã€‘Top-3 ç›¸ä¼¼é³³æ¢¨é›†æˆ\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "ğŸ”¬ æ–¹æ¡ˆ C åŠ å¼·ç‰ˆ: é›†æˆ Top-3 ç›¸ä¼¼é³³æ¢¨\n",
      "============================================================\n",
      "è¨“ç·´èˆ‡æ¸¬è©¦:\n",
      "   ğŸ æ¸¬è©¦ 04: æ¨£æœ¬= 29, Top-3=[03(1.9), 02(7.5), 01(7.6)], æº–ç¢ºç‡=0.276\n",
      "   ğŸ æ¸¬è©¦ 02: æ¨£æœ¬= 30, Top-3=[01(1.0), 04(7.5), 03(7.9)], æº–ç¢ºç‡=0.600\n",
      "   ğŸ æ¸¬è©¦ 03: æ¨£æœ¬= 30, Top-3=[04(1.9), 02(7.9), 01(7.9)], æº–ç¢ºç‡=0.367\n",
      "   ğŸ æ¸¬è©¦ 06: æ¨£æœ¬= 22, Top-3=[05(1.3), 04(8.2), 07(8.5)], æº–ç¢ºç‡=0.682\n",
      "   ğŸ æ¸¬è©¦ 01: æ¨£æœ¬= 37, Top-3=[02(1.0), 04(7.6), 03(7.9)], æº–ç¢ºç‡=0.405\n",
      "   ğŸ æ¸¬è©¦ 08: æ¨£æœ¬= 22, Top-3=[07(2.8), 03(9.4), 06(9.7)], æº–ç¢ºç‡=0.136\n",
      "   ğŸ æ¸¬è©¦ 07: æ¨£æœ¬= 22, Top-3=[08(2.8), 03(8.1), 06(8.5)], æº–ç¢ºç‡=0.318\n",
      "   ğŸ æ¸¬è©¦ 05: æ¨£æœ¬= 30, Top-3=[06(1.3), 04(8.4), 07(8.6)], æº–ç¢ºç‡=0.300\n",
      "\n",
      "============================================================\n",
      "âœ… é›†æˆ Domain Adaptation çµæœ\n",
      "============================================================\n",
      "LOSO æº–ç¢ºç‡: 0.387 (38.7%)\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.415     0.548     0.472        93\n",
      "     Stage 1      0.354     0.432     0.389        81\n",
      "     Stage 2      0.000     0.000     0.000        34\n",
      "     Stage 3      0.000     0.000     0.000        14\n",
      "\n",
      "    accuracy                          0.387       222\n",
      "   macro avg      0.192     0.245     0.215       222\n",
      "weighted avg      0.303     0.387     0.340       222\n",
      "\n",
      "\n",
      "\n",
      "ã€æ–¹æ³• 3ã€‘Top-5 ç›¸ä¼¼é³³æ¢¨é›†æˆ\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "ğŸ”¬ æ–¹æ¡ˆ C åŠ å¼·ç‰ˆ: é›†æˆ Top-5 ç›¸ä¼¼é³³æ¢¨\n",
      "============================================================\n",
      "è¨“ç·´èˆ‡æ¸¬è©¦:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ æ¸¬è©¦ 04: æ¨£æœ¬= 29, Top-5=[03(1.9), 02(7.5), 01(7.6), 06(8.2), 05(8.4)], æº–ç¢ºç‡=0.276\n",
      "   ğŸ æ¸¬è©¦ 02: æ¨£æœ¬= 30, Top-5=[01(1.0), 04(7.5), 03(7.9), 06(8.6), 05(8.9)], æº–ç¢ºç‡=0.600\n",
      "   ğŸ æ¸¬è©¦ 03: æ¨£æœ¬= 30, Top-5=[04(1.9), 02(7.9), 01(7.9), 07(8.1), 06(8.5)], æº–ç¢ºç‡=0.500\n",
      "   ğŸ æ¸¬è©¦ 06: æ¨£æœ¬= 22, Top-5=[05(1.3), 04(8.2), 07(8.5), 03(8.5), 02(8.6)], æº–ç¢ºç‡=0.455\n",
      "   ğŸ æ¸¬è©¦ 01: æ¨£æœ¬= 37, Top-5=[02(1.0), 04(7.6), 03(7.9), 06(8.7), 05(9.0)], æº–ç¢ºç‡=0.405\n",
      "   ğŸ æ¸¬è©¦ 08: æ¨£æœ¬= 22, Top-5=[07(2.8), 03(9.4), 06(9.7), 05(10.0), 04(10.5)], æº–ç¢ºç‡=0.136\n",
      "   ğŸ æ¸¬è©¦ 07: æ¨£æœ¬= 22, Top-5=[08(2.8), 03(8.1), 06(8.5), 05(8.6), 04(9.0)], æº–ç¢ºç‡=0.318\n",
      "   ğŸ æ¸¬è©¦ 05: æ¨£æœ¬= 30, Top-5=[06(1.3), 04(8.4), 07(8.6), 03(8.7), 02(8.9)], æº–ç¢ºç‡=0.400\n",
      "\n",
      "============================================================\n",
      "âœ… é›†æˆ Domain Adaptation çµæœ\n",
      "============================================================\n",
      "LOSO æº–ç¢ºç‡: 0.396 (39.6%)\n",
      "\n",
      "è©³ç´°åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.417     0.624     0.500        93\n",
      "     Stage 1      0.361     0.370     0.366        81\n",
      "     Stage 2      0.000     0.000     0.000        34\n",
      "     Stage 3      0.000     0.000     0.000        14\n",
      "\n",
      "    accuracy                          0.396       222\n",
      "   macro avg      0.195     0.249     0.216       222\n",
      "weighted avg      0.307     0.396     0.343       222\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆç¸½çµå°æ¯”\n",
      "============================================================\n",
      "åŸå§‹æ–¹æ³•ï¼ˆå…¨å±€æ¨¡å‹ï¼‰:        LOSO = 0.598 (59.8%)\n",
      "æ–¹æ¡ˆ Aï¼ˆæ­£è¦åŒ–ç‰¹å¾µï¼‰:        LOSO = 0.551 (55.1%)\n",
      "æ–¹æ¡ˆ C-1ï¼ˆå–®å€‹ç›¸ä¼¼ï¼‰:        LOSO = 0.622 (62.2%)\n",
      "æ–¹æ¡ˆ C-2ï¼ˆTop-3 é›†æˆï¼‰:      LOSO = 0.387 (38.7%)\n",
      "æ–¹æ¡ˆ C-3ï¼ˆTop-5 é›†æˆï¼‰:      LOSO = 0.396 (39.6%)\n",
      "============================================================\n",
      "\n",
      "âœ… æ‰¾åˆ°æ”¹é€²ï¼æå‡ +2.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ C: Domain Adaptationï¼ˆå¤šä»»å‹™å­¸ç¿’ç‰ˆï¼‰=====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pickle\n",
    "\n",
    "class DomainAdaptationModel:\n",
    "    \"\"\"\n",
    "    Domain Adaptation æ¨¡å‹\n",
    "    \n",
    "    ç­–ç•¥ï¼š\n",
    "    1. ç‚ºæ¯é¡†é³³æ¢¨è¨“ç·´ä¸€å€‹å°ˆå±¬æ¨¡å‹\n",
    "    2. æ¸¬è©¦æ™‚ï¼Œæ‰¾å‡ºæœ€ç›¸ä¼¼çš„è¨“ç·´é³³æ¢¨\n",
    "    3. ç”¨è©²é³³æ¢¨çš„æ¨¡å‹ä¾†é æ¸¬\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pineapple_models = {}      # æ¯é¡†é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n",
    "        self.pineapple_centroids = {}   # æ¯é¡†é³³æ¢¨çš„ç‰¹å¾µä¸­å¿ƒé»\n",
    "        self.global_scaler = StandardScaler()\n",
    "    \n",
    "    def train_loso(self, X, y, metadata):\n",
    "        \"\"\"\n",
    "        LOSO è¨“ç·´ï¼šæ¯æ¬¡ç•™ä¸€é¡†é³³æ¢¨æ¸¬è©¦\n",
    "        åŒæ™‚ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ğŸ”¬ æ–¹æ¡ˆ C: Domain Adaptation (å¤šä»»å‹™å­¸ç¿’)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"ç­–ç•¥: ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹\\n\")\n",
    "        \n",
    "        # å…¨å±€æ¨™æº–åŒ–\n",
    "        X_scaled = self.global_scaler.fit_transform(X)\n",
    "        \n",
    "        pineapple_ids = metadata['pineapple_id'].unique()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        print(\"è¨“ç·´èˆ‡æ¸¬è©¦:\")\n",
    "        for test_pid in pineapple_ids:\n",
    "            train_mask = metadata['pineapple_id'] != test_pid\n",
    "            test_mask = metadata['pineapple_id'] == test_pid\n",
    "            \n",
    "            X_train, y_train = X_scaled[train_mask], y[train_mask]\n",
    "            X_test, y_test = X_scaled[test_mask], y[test_mask]\n",
    "            \n",
    "            train_pids = metadata.loc[train_mask, 'pineapple_id'].unique()\n",
    "            \n",
    "            # === ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹å°ˆå±¬æ¨¡å‹ ===\n",
    "            pineapple_specific_models = {}\n",
    "            pineapple_specific_centroids = {}\n",
    "            \n",
    "            for train_pid in train_pids:\n",
    "                pid_mask = metadata['pineapple_id'] == train_pid\n",
    "                X_pid = X_scaled[pid_mask]\n",
    "                y_pid = y[pid_mask]\n",
    "                \n",
    "                # è¨“ç·´è©²é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n",
    "                rf_pid = RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf_pid.fit(X_pid, y_pid)\n",
    "                \n",
    "                pineapple_specific_models[train_pid] = rf_pid\n",
    "                pineapple_specific_centroids[train_pid] = np.mean(X_pid, axis=0)\n",
    "            \n",
    "            # === æ¸¬è©¦ï¼šæ‰¾æœ€ç›¸ä¼¼çš„é³³æ¢¨æ¨¡å‹ ===\n",
    "            test_centroid = np.mean(X_test, axis=0)\n",
    "            \n",
    "            # è¨ˆç®—èˆ‡æ¯é¡†è¨“ç·´é³³æ¢¨çš„è·é›¢\n",
    "            distances = {\n",
    "                pid: euclidean(test_centroid, centroid)\n",
    "                for pid, centroid in pineapple_specific_centroids.items()\n",
    "            }\n",
    "            \n",
    "            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„é³³æ¢¨\n",
    "            most_similar_pid = min(distances, key=distances.get)\n",
    "            \n",
    "            # ç”¨æœ€ç›¸ä¼¼é³³æ¢¨çš„æ¨¡å‹ä¾†é æ¸¬\n",
    "            best_model = pineapple_specific_models[most_similar_pid]\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "            \n",
    "            print(f\"   ğŸ æ¸¬è©¦ {test_pid}: \"\n",
    "                  f\"æ¨£æœ¬={len(y_test):3d}, \"\n",
    "                  f\"æœ€ç›¸ä¼¼={most_similar_pid}, \"\n",
    "                  f\"è·é›¢={distances[most_similar_pid]:.2f}, \"\n",
    "                  f\"æº–ç¢ºç‡={acc:.3f}\")\n",
    "        \n",
    "        overall_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ… Domain Adaptation çµæœ\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"LOSO æº–ç¢ºç‡: {overall_acc:.3f} ({overall_acc*100:.1f}%)\")\n",
    "        \n",
    "        # è©³ç´°å ±å‘Š\n",
    "        print(f\"\\nè©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "        report = classification_report(\n",
    "            all_true, all_preds,\n",
    "            target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "            digits=3\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        return overall_acc, all_preds, all_true\n",
    "\n",
    "\n",
    "class EnsembleAdaptationModel:\n",
    "    \"\"\"\n",
    "    æ–¹æ¡ˆ C åŠ å¼·ç‰ˆï¼šé›†æˆå¤šå€‹ç›¸ä¼¼é³³æ¢¨çš„æ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_similar=3):\n",
    "        self.n_similar = n_similar  # ä½¿ç”¨æœ€ç›¸ä¼¼çš„ N é¡†é³³æ¢¨\n",
    "        self.global_scaler = StandardScaler()\n",
    "    \n",
    "    def train_loso(self, X, y, metadata):\n",
    "        \"\"\"\n",
    "        LOSO è¨“ç·´ï¼šé›†æˆå¤šå€‹ç›¸ä¼¼é³³æ¢¨çš„æ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ğŸ”¬ æ–¹æ¡ˆ C åŠ å¼·ç‰ˆ: é›†æˆ Top-{self.n_similar} ç›¸ä¼¼é³³æ¢¨\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        X_scaled = self.global_scaler.fit_transform(X)\n",
    "        \n",
    "        pineapple_ids = metadata['pineapple_id'].unique()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        \n",
    "        print(\"è¨“ç·´èˆ‡æ¸¬è©¦:\")\n",
    "        for test_pid in pineapple_ids:\n",
    "            train_mask = metadata['pineapple_id'] != test_pid\n",
    "            test_mask = metadata['pineapple_id'] == test_pid\n",
    "            \n",
    "            X_train, y_train = X_scaled[train_mask], y[train_mask]\n",
    "            X_test, y_test = X_scaled[test_mask], y[test_mask]\n",
    "            \n",
    "            train_pids = metadata.loc[train_mask, 'pineapple_id'].unique()\n",
    "            \n",
    "            # ç‚ºæ¯é¡†è¨“ç·´é³³æ¢¨å»ºç«‹æ¨¡å‹å’Œä¸­å¿ƒé»\n",
    "            pineapple_models = {}\n",
    "            pineapple_centroids = {}\n",
    "            \n",
    "            for train_pid in train_pids:\n",
    "                pid_mask = metadata['pineapple_id'] == train_pid\n",
    "                X_pid = X_scaled[pid_mask]\n",
    "                y_pid = y[pid_mask]\n",
    "                \n",
    "                rf_pid = RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                rf_pid.fit(X_pid, y_pid)\n",
    "                \n",
    "                pineapple_models[train_pid] = rf_pid\n",
    "                pineapple_centroids[train_pid] = np.mean(X_pid, axis=0)\n",
    "            \n",
    "            # æ¸¬è©¦ï¼šæ‰¾ Top-N ç›¸ä¼¼çš„é³³æ¢¨\n",
    "            test_centroid = np.mean(X_test, axis=0)\n",
    "            \n",
    "            distances = {\n",
    "                pid: euclidean(test_centroid, centroid)\n",
    "                for pid, centroid in pineapple_centroids.items()\n",
    "            }\n",
    "            \n",
    "            # æ’åºæ‰¾å‡ºæœ€ç›¸ä¼¼çš„ N é¡†\n",
    "            sorted_pids = sorted(distances, key=distances.get)\n",
    "            top_n_pids = sorted_pids[:min(self.n_similar, len(sorted_pids))]\n",
    "            \n",
    "            # é›†æˆé æ¸¬ï¼ˆæŠ•ç¥¨ï¼‰\n",
    "            ensemble_preds = []\n",
    "            for pid in top_n_pids:\n",
    "                model = pineapple_models[pid]\n",
    "                pred = model.predict(X_test)\n",
    "                ensemble_preds.append(pred)\n",
    "            \n",
    "            # å¤šæ•¸æŠ•ç¥¨\n",
    "            ensemble_preds = np.array(ensemble_preds)\n",
    "            y_pred = []\n",
    "            for i in range(len(X_test)):\n",
    "                votes = ensemble_preds[:, i]\n",
    "                # æ‰¾å‡ºçœ¾æ•¸\n",
    "                unique, counts = np.unique(votes, return_counts=True)\n",
    "                y_pred.append(unique[np.argmax(counts)])\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_preds.extend(y_pred)\n",
    "            all_true.extend(y_test)\n",
    "            \n",
    "            top_n_str = ', '.join([f\"{pid}({distances[pid]:.1f})\" for pid in top_n_pids])\n",
    "            print(f\"   ğŸ æ¸¬è©¦ {test_pid}: \"\n",
    "                  f\"æ¨£æœ¬={len(y_test):3d}, \"\n",
    "                  f\"Top-{self.n_similar}=[{top_n_str}], \"\n",
    "                  f\"æº–ç¢ºç‡={acc:.3f}\")\n",
    "        \n",
    "        overall_acc = accuracy_score(all_true, all_preds)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ… é›†æˆ Domain Adaptation çµæœ\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"LOSO æº–ç¢ºç‡: {overall_acc:.3f} ({overall_acc*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nè©³ç´°åˆ†é¡å ±å‘Š:\")\n",
    "        report = classification_report(\n",
    "            all_true, all_preds,\n",
    "            target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "            digits=3\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        return overall_acc, all_preds, all_true\n",
    "\n",
    "\n",
    "# ===== åŸ·è¡Œæ–¹æ¡ˆ C =====\n",
    "\n",
    "# è¼‰å…¥åŸå§‹ç‰¹å¾µï¼ˆä¸ç”¨æ­£è¦åŒ–ç‰ˆæœ¬ï¼‰\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹åŸ·è¡Œæ–¹æ¡ˆ C\\n\")\n",
    "\n",
    "# æ–¹æ³• 1: å–®å€‹æœ€ç›¸ä¼¼é³³æ¢¨\n",
    "print(\"\\nã€æ–¹æ³• 1ã€‘å–®å€‹æœ€ç›¸ä¼¼é³³æ¢¨æ¨¡å‹\")\n",
    "print(\"-\"*60)\n",
    "model1 = DomainAdaptationModel()\n",
    "acc1, preds1, true1 = model1.train_loso(X, y, metadata)\n",
    "\n",
    "# æ–¹æ³• 2: Top-3 é›†æˆ\n",
    "print(\"\\n\\nã€æ–¹æ³• 2ã€‘Top-3 ç›¸ä¼¼é³³æ¢¨é›†æˆ\")\n",
    "print(\"-\"*60)\n",
    "model2 = EnsembleAdaptationModel(n_similar=3)\n",
    "acc2, preds2, true2 = model2.train_loso(X, y, metadata)\n",
    "\n",
    "# æ–¹æ³• 3: Top-5 é›†æˆ\n",
    "print(\"\\n\\nã€æ–¹æ³• 3ã€‘Top-5 ç›¸ä¼¼é³³æ¢¨é›†æˆ\")\n",
    "print(\"-\"*60)\n",
    "model3 = EnsembleAdaptationModel(n_similar=5)\n",
    "acc3, preds3, true3 = model3.train_loso(X, y, metadata)\n",
    "\n",
    "# æœ€çµ‚å°æ¯”\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆç¸½çµå°æ¯”\")\n",
    "print(\"=\"*60)\n",
    "print(f\"åŸå§‹æ–¹æ³•ï¼ˆå…¨å±€æ¨¡å‹ï¼‰:        LOSO = 0.598 (59.8%)\")\n",
    "print(f\"æ–¹æ¡ˆ Aï¼ˆæ­£è¦åŒ–ç‰¹å¾µï¼‰:        LOSO = 0.551 (55.1%)\")\n",
    "print(f\"æ–¹æ¡ˆ C-1ï¼ˆå–®å€‹ç›¸ä¼¼ï¼‰:        LOSO = {acc1:.3f} ({acc1*100:.1f}%)\")\n",
    "print(f\"æ–¹æ¡ˆ C-2ï¼ˆTop-3 é›†æˆï¼‰:      LOSO = {acc2:.3f} ({acc2*100:.1f}%)\")\n",
    "print(f\"æ–¹æ¡ˆ C-3ï¼ˆTop-5 é›†æˆï¼‰:      LOSO = {acc3:.3f} ({acc3*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_acc = max(acc1, acc2, acc3)\n",
    "if best_acc > 0.598:\n",
    "    improvement = (best_acc - 0.598) * 100\n",
    "    print(f\"\\nâœ… æ‰¾åˆ°æ”¹é€²ï¼æå‡ {improvement:+.1f}%\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  æ–¹æ¡ˆ C æ•ˆæœæœ‰é™ï¼Œå»ºè­°æ”¶é›†æ›´å¤šé³³æ¢¨æ•¸æ“š\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06d15bc8-9053-488a-9ece-6b6ec5851d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: models/final_domain_adaptation_model.pkl\n",
      "   LOSO æº–ç¢ºç‡: 62.2%\n",
      "   åŒ…å« 8 é¡†é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "# ===== ä¿å­˜æ–¹æ¡ˆ C-1 çš„æœ€ä½³æ¨¡å‹ =====\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ç”¨å…¨éƒ¨æ•¸æ“šè¨“ç·´æœ€çµ‚æ¨¡å‹ï¼ˆç‚ºæ¯é¡†é³³æ¢¨è¨“ç·´å°ˆå±¬æ¨¡å‹ï¼‰\n",
    "final_model = DomainAdaptationModel()\n",
    "\n",
    "# è¨“ç·´æ‰€æœ‰é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pineapple_ids = metadata['pineapple_id'].unique()\n",
    "\n",
    "# ç‚ºæ¯é¡†é³³æ¢¨è¨“ç·´æ¨¡å‹\n",
    "for pid in pineapple_ids:\n",
    "    pid_mask = metadata['pineapple_id'] == pid\n",
    "    X_pid = X_scaled[pid_mask]\n",
    "    y_pid = y[pid_mask]\n",
    "    \n",
    "    rf_pid = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_pid.fit(X_pid, y_pid)\n",
    "    \n",
    "    final_model.pineapple_models[pid] = rf_pid\n",
    "    final_model.pineapple_centroids[pid] = np.mean(X_pid, axis=0)\n",
    "\n",
    "final_model.global_scaler = scaler\n",
    "\n",
    "# ä¿å­˜\n",
    "with open('models/final_domain_adaptation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(\"âœ… æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: models/final_domain_adaptation_model.pkl\")\n",
    "print(f\"   LOSO æº–ç¢ºç‡: 62.2%\")\n",
    "print(f\"   åŒ…å« {len(pineapple_ids)} é¡†é³³æ¢¨çš„å°ˆå±¬æ¨¡å‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e577684d-4b46-4f9d-830c-0cb0b3cd1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ J: ç‰¹å¾µé¸æ“‡\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸: 53\n",
      "ğŸ“Š æ¨£æœ¬æ•¸: 222\n",
      "ğŸ“Š é³³æ¢¨æ•¸: 8\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š åˆ†æç‰¹å¾µé‡è¦æ€§...\n",
      "============================================================\n",
      "\n",
      "ğŸ” Top 15 æœ€é‡è¦ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio            0.0503\n",
      "   2. MQ135_auc                      0.0481\n",
      "   3. MQ135_mean                     0.0423\n",
      "   4. MQ9_auc                        0.0412\n",
      "   5. TGS2602_mean                   0.0393\n",
      "   6. MQ9_mean                       0.0387\n",
      "   7. MQ3_min                        0.0359\n",
      "   8. MQ3_MQ2_ratio                  0.0355\n",
      "   9. MQ9_max                        0.0349\n",
      "  10. MQ3_delta_mean                 0.0324\n",
      "  11. MQ135_min                      0.0311\n",
      "  12. TGS2602_auc                    0.0304\n",
      "  13. TGS2602_max                    0.0297\n",
      "  14. MQ3_auc                        0.0294\n",
      "  15. MQ2_auc                        0.0293\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-5 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-5 æº–ç¢ºç‡: 0.752 (75.2%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-10 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-10 æº–ç¢ºç‡: 0.631 (63.1%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-15 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-15 æº–ç¢ºç‡: 0.676 (67.6%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-20 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-20 æº–ç¢ºç‡: 0.649 (64.9%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-25 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-25 æº–ç¢ºç‡: 0.658 (65.8%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ¸¬è©¦ Top-30 ç‰¹å¾µ\n",
      "============================================================\n",
      "âœ… Top-30 æº–ç¢ºç‡: 0.649 (64.9%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ J ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "ç‰¹å¾µæ•¸        æº–ç¢ºç‡        ç›¸æ¯”Baseline     \n",
      "----------------------------------------\n",
      "âœ… 5        75.2%      +13.0%\n",
      "âœ… 10       63.1%      +0.9%\n",
      "âœ… 15       67.6%      +5.4%\n",
      "âœ… 20       64.9%      +2.7%\n",
      "âœ… 25       65.8%      +3.6%\n",
      "âœ… 30       64.9%      +2.7%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³é…ç½®: Top-5 ç‰¹å¾µ\n",
      "   æº–ç¢ºç‡: 75.2%\n",
      "   âœ… æå‡: +13.0 å€‹ç™¾åˆ†é»\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æœ€ä½³æ¨¡å‹è©³ç´°å ±å‘Š (Top-5)\n",
      "============================================================\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.761     0.892     0.822        93\n",
      "     Stage 1      0.730     0.802     0.765        81\n",
      "     Stage 2      0.500     0.147     0.227        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.752       222\n",
      "   macro avg      0.748     0.711     0.703       222\n",
      "weighted avg      0.725     0.752     0.721       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[83  8  2  0]\n",
      " [13 65  3  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "ğŸ” ä½¿ç”¨çš„ Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio            (é‡è¦æ€§: 0.0503)\n",
      "   2. MQ135_auc                      (é‡è¦æ€§: 0.0481)\n",
      "   3. MQ135_mean                     (é‡è¦æ€§: 0.0423)\n",
      "   4. MQ9_auc                        (é‡è¦æ€§: 0.0412)\n",
      "   5. TGS2602_mean                   (é‡è¦æ€§: 0.0393)\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_j_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ J: ç‰¹å¾µé¸æ“‡ï¼ˆç°¡åŒ–é«˜æ•ˆç‰ˆï¼‰=====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ J: ç‰¹å¾µé¸æ“‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "print(f\"\\nğŸ“Š åŸå§‹ç‰¹å¾µæ•¸: {X.shape[1]}\")\n",
    "print(f\"ğŸ“Š æ¨£æœ¬æ•¸: {len(y)}\")\n",
    "print(f\"ğŸ“Š é³³æ¢¨æ•¸: {len(np.unique(pineapple_ids))}\")\n",
    "\n",
    "# ========== å…ˆæ‰¾å‡ºå…¨å±€æœ€é‡è¦çš„ç‰¹å¾µ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š åˆ†æç‰¹å¾µé‡è¦æ€§...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_full = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "X_scaled_full = StandardScaler().fit_transform(X)\n",
    "rf_full.fit(X_scaled_full, y)\n",
    "\n",
    "# å–å¾—ç‰¹å¾µé‡è¦æ€§\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data['features'].columns,\n",
    "    'importance': rf_full.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ” Top 15 æœ€é‡è¦ç‰¹å¾µ:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:30s} {row['importance']:.4f}\")\n",
    "\n",
    "# ========== æ¸¬è©¦ä¸åŒæ•¸é‡çš„ç‰¹å¾µ ==========\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "feature_counts = [5, 10, 15, 20, 25, 30]\n",
    "results = {}\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“Œ æ¸¬è©¦ Top-{n_features} ç‰¹å¾µ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # é¸æ“‡ top N ç‰¹å¾µ\n",
    "    top_features = feature_importance.head(n_features)['feature'].tolist()\n",
    "    top_indices = [data['features'].columns.tolist().index(f) for f in top_features]\n",
    "    X_subset = X[:, top_indices]\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_subset, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_subset[train_idx], X_subset[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # è¨“ç·´\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # é æ¸¬\n",
    "        y_pred.extend(rf.predict(X_test_scaled))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results[n_features] = {\n",
    "        'accuracy': acc,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'features': top_features\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Top-{n_features} æº–ç¢ºç‡: {acc:.3f} ({acc:.1%})\")\n",
    "\n",
    "# ========== è©³ç´°å ±å‘Šæœ€ä½³çµæœ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ J ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'ç‰¹å¾µæ•¸':<10} {'æº–ç¢ºç‡':<10} {'ç›¸æ¯”Baseline':<15}\")\n",
    "print(\"-\" * 40)\n",
    "baseline = 0.622\n",
    "\n",
    "for n_feat in feature_counts:\n",
    "    acc = results[n_feat]['accuracy']\n",
    "    diff = (acc - baseline) * 100\n",
    "    symbol = \"âœ…\" if acc > baseline else \"âš ï¸\"\n",
    "    print(f\"{symbol} {n_feat:<8} {acc:.1%}      {diff:+.1f}%\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ–¹æ³•\n",
    "best_n = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "best_acc = results[best_n]['accuracy']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³é…ç½®: Top-{best_n} ç‰¹å¾µ\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_acc:.1%}\")\n",
    "\n",
    "improvement = (best_acc - baseline) * 100\n",
    "if best_acc > baseline:\n",
    "    print(f\"   âœ… æå‡: +{improvement:.1f} å€‹ç™¾åˆ†é»\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  ä¸‹é™: {improvement:.1f} å€‹ç™¾åˆ†é»\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“Š æœ€ä½³æ¨¡å‹è©³ç´°å ±å‘Š (Top-{best_n})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(\n",
    "    results[best_n]['y_true'], \n",
    "    results[best_n]['y_pred'],\n",
    "    target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "print(\"\\næ··æ·†çŸ©é™£:\")\n",
    "cm = confusion_matrix(results[best_n]['y_true'], results[best_n]['y_pred'])\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nğŸ” ä½¿ç”¨çš„ Top-{best_n} ç‰¹å¾µ:\")\n",
    "for i, feat in enumerate(results[best_n]['features'], 1):\n",
    "    imp = feature_importance[feature_importance['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:30s} (é‡è¦æ€§: {imp:.4f})\")\n",
    "\n",
    "# ========== å„²å­˜çµæœ ==========\n",
    "results_j = {\n",
    "    'all_results': results,\n",
    "    'best_n_features': best_n,\n",
    "    'best_accuracy': best_acc,\n",
    "    'feature_importance': feature_importance,\n",
    "    'baseline': baseline,\n",
    "    'improvement': improvement\n",
    "}\n",
    "\n",
    "with open('models/solution_j_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_j, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_j_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a03466e-c45e-4962-9444-c89415a31df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.4.2)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.8.0)\n",
      "Collecting sklearn-compat<0.2,>=0.1.5 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
      "Downloading imbalanced_learn-0.14.1-py3-none-any.whl (235 kB)\n",
      "Downloading sklearn_compat-0.1.5-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.14.1 sklearn-compat-0.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e89cec86-255a-437a-a0e8-8d405f42108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”§ æ–¹æ¡ˆ J-Plus: æ”¹å–„ Stage 2 è­˜åˆ¥\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
      "ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): 93, np.int64(1): 81, np.int64(2): 34, np.int64(3): 14}\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: é¡åˆ¥æ¬Šé‡å¹³è¡¡ (Class Weight)\n",
      "============================================================\n",
      "\n",
      "âœ… é¡åˆ¥å¹³è¡¡ æº–ç¢ºç‡: 0.676 (67.6%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.717     0.710     0.714        93\n",
      "     Stage 1      0.670     0.802     0.730        81\n",
      "     Stage 2      0.333     0.147     0.204        34\n",
      "     Stage 3      0.778     1.000     0.875        14\n",
      "\n",
      "    accuracy                          0.676       222\n",
      "   macro avg      0.625     0.665     0.631       222\n",
      "weighted avg      0.645     0.676     0.652       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[66 16  7  4]\n",
      " [13 65  3  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: SMOTE éæ¡æ¨£\n",
      "============================================================\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 78, np.int64(1): 66, np.int64(2): 27, np.int64(3): 14} â†’ SMOTEå¾Œ: {np.int64(0): 78, np.int64(1): 78, np.int64(2): 78, np.int64(3): 78}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 78, np.int64(1): 66, np.int64(2): 34, np.int64(3): 14} â†’ SMOTEå¾Œ: {np.int64(0): 78, np.int64(1): 78, np.int64(2): 78, np.int64(3): 78}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 78, np.int64(1): 70, np.int64(2): 30, np.int64(3): 14} â†’ SMOTEå¾Œ: {np.int64(0): 78, np.int64(1): 78, np.int64(2): 78, np.int64(3): 78}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 78, np.int64(1): 73, np.int64(2): 28, np.int64(3): 14} â†’ SMOTEå¾Œ: {np.int64(0): 78, np.int64(1): 78, np.int64(2): 78, np.int64(3): 78}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 85, np.int64(1): 66, np.int64(2): 27, np.int64(3): 14} â†’ SMOTEå¾Œ: {np.int64(0): 85, np.int64(1): 85, np.int64(2): 85, np.int64(3): 85}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 78, np.int64(1): 74, np.int64(2): 34, np.int64(3): 14} â†’ SMOTEå¾Œ: {np.int64(0): 78, np.int64(1): 78, np.int64(2): 78, np.int64(3): 78}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 88, np.int64(1): 76, np.int64(2): 29, np.int64(3): 7} â†’ SMOTEå¾Œ: {np.int64(0): 88, np.int64(1): 88, np.int64(2): 88, np.int64(3): 88}\n",
      "  è¨“ç·´é›†åˆ†å¸ƒ: {np.int64(0): 88, np.int64(1): 76, np.int64(2): 29, np.int64(3): 7} â†’ SMOTEå¾Œ: {np.int64(0): 88, np.int64(1): 88, np.int64(2): 88, np.int64(3): 88}\n",
      "\n",
      "âœ… SMOTE æº–ç¢ºç‡: 0.685 (68.5%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.720     0.720     0.720        93\n",
      "     Stage 1      0.742     0.815     0.776        81\n",
      "     Stage 2      0.238     0.147     0.182        34\n",
      "     Stage 3      0.737     1.000     0.848        14\n",
      "\n",
      "    accuracy                          0.685       222\n",
      "   macro avg      0.609     0.671     0.632       222\n",
      "weighted avg      0.655     0.685     0.666       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[67  7 14  5]\n",
      " [13 66  2  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: æ‰‹å‹•å¼·åŒ– Stage 2 æ¬Šé‡\n",
      "============================================================\n",
      "\n",
      "âœ… å¼·åŒ–æ¬Šé‡ æº–ç¢ºç‡: 0.734 (73.4%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.750     0.839     0.792        93\n",
      "     Stage 1      0.733     0.815     0.772        81\n",
      "     Stage 2      0.357     0.147     0.208        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.734       222\n",
      "   macro avg      0.710     0.700     0.693       222\n",
      "weighted avg      0.700     0.734     0.708       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[78  8  7  0]\n",
      " [13 66  2  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 4: æ›´è¤‡é›œçš„æ¨¡å‹ (deeper trees)\n",
      "============================================================\n",
      "\n",
      "âœ… è¤‡é›œæ¨¡å‹ æº–ç¢ºç‡: 0.725 (72.5%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.745     0.817     0.779        93\n",
      "     Stage 1      0.750     0.815     0.781        81\n",
      "     Stage 2      0.357     0.147     0.208        34\n",
      "     Stage 3      0.778     1.000     0.875        14\n",
      "\n",
      "    accuracy                          0.725       222\n",
      "   macro avg      0.658     0.695     0.661       222\n",
      "weighted avg      0.690     0.725     0.699       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[76  6  7  4]\n",
      " [13 66  2  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ J-Plus ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                   æº–ç¢ºç‡        Stage2 Recall  \n",
      "--------------------------------------------------\n",
      "Baseline (Top-5)     75.2%      14.7%\n",
      "é¡åˆ¥å¹³è¡¡                 67.6%      14.7%\n",
      "SMOTE                68.5%      14.7%\n",
      "å¼·åŒ–æ¬Šé‡                 73.4%      14.7%\n",
      "è¤‡é›œæ¨¡å‹                 72.5%      14.7%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æ¨è–¦æ–¹æ³•: å¼·åŒ–æ¬Šé‡\n",
      "   æº–ç¢ºç‡: 73.4%\n",
      "   Stage 2 Recall: 14.7% (åŸæœ¬ 14.7%)\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jplus_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ J-Plus: æ”¹å–„ Stage 2 è­˜åˆ¥ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”§ æ–¹æ¡ˆ J-Plus: æ”¹å–„ Stage 2 è­˜åˆ¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\")\n",
    "print(f\"ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {dict(Counter(y))}\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: é¡åˆ¥æ¬Šé‡å¹³è¡¡ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: é¡åˆ¥æ¬Šé‡å¹³è¡¡ (Class Weight)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_balanced = []\n",
    "y_pred_balanced = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è‡ªå‹•å¹³è¡¡é¡åˆ¥æ¬Šé‡\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=10, \n",
    "        class_weight='balanced',  # é—œéµï¼šè‡ªå‹•å¹³è¡¡\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_balanced.extend(y_test)\n",
    "    y_pred_balanced.extend(y_pred)\n",
    "\n",
    "acc_balanced = accuracy_score(y_true_balanced, y_pred_balanced)\n",
    "print(f\"\\nâœ… é¡åˆ¥å¹³è¡¡ æº–ç¢ºç‡: {acc_balanced:.3f} ({acc_balanced:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_balanced, y_pred_balanced,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_balanced, y_pred_balanced))\n",
    "\n",
    "# ========== æ–¹æ³• 2: SMOTE éæ¡æ¨£ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: SMOTE éæ¡æ¨£\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_smote = []\n",
    "y_pred_smote = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # æª¢æŸ¥è¨“ç·´é›†ä¸­æ˜¯å¦æœ‰è¶³å¤ çš„ Stage 2 æ¨£æœ¬\n",
    "    unique_classes = np.unique(y_train)\n",
    "    class_counts = Counter(y_train)\n",
    "    \n",
    "    print(f\"  è¨“ç·´é›†åˆ†å¸ƒ: {dict(class_counts)}\", end=\" \")\n",
    "    \n",
    "    # åªåœ¨æœ‰å¤šå€‹é¡åˆ¥ä¸”æ¯é¡è‡³å°‘ 2 å€‹æ¨£æœ¬æ™‚ä½¿ç”¨ SMOTE\n",
    "    if len(unique_classes) >= 2 and min(class_counts.values()) >= 2:\n",
    "        try:\n",
    "            # SMOTE éæ¡æ¨£\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(3, min(class_counts.values())-1))\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "            print(f\"â†’ SMOTEå¾Œ: {dict(Counter(y_train_resampled))}\")\n",
    "        except Exception as e:\n",
    "            print(f\"â†’ SMOTEå¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æ•¸æ“š\")\n",
    "            X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "    else:\n",
    "        print(f\"â†’ è·³éSMOTEï¼ˆé¡åˆ¥ä¸è¶³ï¼‰\")\n",
    "        X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_smote.extend(y_test)\n",
    "    y_pred_smote.extend(y_pred)\n",
    "\n",
    "acc_smote = accuracy_score(y_true_smote, y_pred_smote)\n",
    "print(f\"\\nâœ… SMOTE æº–ç¢ºç‡: {acc_smote:.3f} ({acc_smote:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_smote, y_pred_smote,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_smote, y_pred_smote))\n",
    "\n",
    "# ========== æ–¹æ³• 3: æ‰‹å‹•è¨­å®šæ›´é«˜çš„ Stage 2 æ¬Šé‡ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: æ‰‹å‹•å¼·åŒ– Stage 2 æ¬Šé‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_weighted = []\n",
    "y_pred_weighted = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # æ‰‹å‹•è¨­å®šæ¬Šé‡ï¼šStage 2 çµ¦ 3 å€æ¬Šé‡\n",
    "    class_weights = {\n",
    "        0: 1.0,  # Stage 0\n",
    "        1: 1.0,  # Stage 1\n",
    "        2: 3.0,  # Stage 2 (3å€æ¬Šé‡)\n",
    "        3: 1.0   # Stage 3\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        class_weight=class_weights,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_weighted.extend(y_test)\n",
    "    y_pred_weighted.extend(y_pred)\n",
    "\n",
    "acc_weighted = accuracy_score(y_true_weighted, y_pred_weighted)\n",
    "print(f\"\\nâœ… å¼·åŒ–æ¬Šé‡ æº–ç¢ºç‡: {acc_weighted:.3f} ({acc_weighted:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_weighted, y_pred_weighted,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_weighted, y_pred_weighted))\n",
    "\n",
    "# ========== æ–¹æ³• 4: æ›´æ·±çš„æ¨¹ + æ›´å¤šæ¨¹ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 4: æ›´è¤‡é›œçš„æ¨¡å‹ (deeper trees)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_complex = []\n",
    "y_pred_complex = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # æ›´å¤šæ¨¹ã€æ›´æ·±ã€å¹³è¡¡æ¬Šé‡\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=500,  # å¢åŠ åˆ° 500\n",
    "        max_depth=20,      # å¢åŠ æ·±åº¦\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_complex.extend(y_test)\n",
    "    y_pred_complex.extend(y_pred)\n",
    "\n",
    "acc_complex = accuracy_score(y_true_complex, y_pred_complex)\n",
    "print(f\"\\nâœ… è¤‡é›œæ¨¡å‹ æº–ç¢ºç‡: {acc_complex:.3f} ({acc_complex:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_complex, y_pred_complex,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_complex, y_pred_complex))\n",
    "\n",
    "# ========== ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ J-Plus ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = [\n",
    "    ('Baseline (Top-5)', 0.752, y_true_balanced, y_pred_balanced),  # é€™è£¡ç”¨ç¬¬ä¸€å€‹çš„ y_true\n",
    "    ('é¡åˆ¥å¹³è¡¡', acc_balanced, y_true_balanced, y_pred_balanced),\n",
    "    ('SMOTE', acc_smote, y_true_smote, y_pred_smote),\n",
    "    ('å¼·åŒ–æ¬Šé‡', acc_weighted, y_true_weighted, y_pred_weighted),\n",
    "    ('è¤‡é›œæ¨¡å‹', acc_complex, y_true_complex, y_pred_complex)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<20} {'æº–ç¢ºç‡':<10} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for method, acc, y_true, y_pred in results_summary:\n",
    "    # è¨ˆç®— Stage 2 çš„ recall\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape[0] > 2 and cm[2, :].sum() > 0:\n",
    "        stage2_recall = cm[2, 2] / cm[2, :].sum()\n",
    "    else:\n",
    "        stage2_recall = 0.0\n",
    "    \n",
    "    print(f\"{method:<20} {acc:.1%}      {stage2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ–¹æ³•ï¼ˆç¶œåˆè€ƒæ…®æº–ç¢ºç‡å’Œ Stage 2 recallï¼‰\n",
    "best_idx = np.argmax([r[1] for r in results_summary[1:]])  # è·³é baseline\n",
    "best_method = results_summary[best_idx + 1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æ¨è–¦æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "\n",
    "# è¨ˆç®— Stage 2 æ”¹å–„\n",
    "cm_best = confusion_matrix(best_method[2], best_method[3])\n",
    "if cm_best.shape[0] > 2:\n",
    "    stage2_recall_best = cm_best[2, 2] / cm_best[2, :].sum()\n",
    "    print(f\"   Stage 2 Recall: {stage2_recall_best:.1%} (åŸæœ¬ 14.7%)\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_jplus = {\n",
    "    'balanced': {'accuracy': acc_balanced, 'y_true': y_true_balanced, 'y_pred': y_pred_balanced},\n",
    "    'smote': {'accuracy': acc_smote, 'y_true': y_true_smote, 'y_pred': y_pred_smote},\n",
    "    'weighted': {'accuracy': acc_weighted, 'y_true': y_true_weighted, 'y_pred': y_pred_weighted},\n",
    "    'complex': {'accuracy': acc_complex, 'y_true': y_true_complex, 'y_pred': y_pred_complex},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_jplus_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_jplus, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jplus_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a2ef4-9747-47bb-8014-22dff7b38718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbfda3cd-c409-44e8-8216-f6d90a6c8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ J+H: Top-5 ç‰¹å¾µ + æ•¸æ“šå¢å¼·\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: å¢å¼· Stage 2 (3å€)\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 27 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 108 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 34 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 136 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 30 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 120 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 28 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 112 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 27 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 108 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 34 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 136 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 29 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 116 å€‹ (x4)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 29 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 116 å€‹ (x4)\n",
      "\n",
      "âœ… å¢å¼· Stage 2 æº–ç¢ºç‡: 0.644 (64.4%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.694     0.634     0.663        93\n",
      "     Stage 1      0.699     0.802     0.747        81\n",
      "     Stage 2      0.167     0.147     0.156        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.644       222\n",
      "   macro avg      0.640     0.646     0.642       222\n",
      "weighted avg      0.634     0.644     0.637       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[59 12 22  0]\n",
      " [13 65  3  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: å¢å¼· Stage 2 (5å€ï¼Œæ›´æ¿€é€²)\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 27 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 162 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 34 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 204 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 30 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 180 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 28 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 168 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 27 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 162 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 34 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 204 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 29 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 174 å€‹ (x6)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 29 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 174 å€‹ (x6)\n",
      "\n",
      "âœ… å¢å¼· Stage 2 (5å€) æº–ç¢ºç‡: 0.635 (63.5%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.690     0.624     0.655        93\n",
      "     Stage 1      0.667     0.790     0.723        81\n",
      "     Stage 2      0.179     0.147     0.161        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.635       222\n",
      "   macro avg      0.634     0.640     0.635       222\n",
      "weighted avg      0.623     0.635     0.626       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: å¢å¼· Stage 2 å’Œ Stage 3\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 27 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 135 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 14 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 42 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 34 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 170 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 14 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 42 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 30 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 150 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 14 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 42 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 28 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 140 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 14 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 42 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 27 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 135 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 14 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 42 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 34 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 170 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 14 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 42 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 29 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 145 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 7 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 21 å€‹ (x3)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 2:\n",
      "   åŸå§‹æ¨£æœ¬: 29 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 145 å€‹ (x5)\n",
      "\n",
      "ğŸ”§ å¢å¼· Stage 3:\n",
      "   åŸå§‹æ¨£æœ¬: 7 å€‹\n",
      "   å¢å¼·å¾Œæ¨£æœ¬: 21 å€‹ (x3)\n",
      "\n",
      "âœ… å¢å¼·å¤šé¡åˆ¥ æº–ç¢ºç‡: 0.667 (66.7%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.711     0.688     0.699        93\n",
      "     Stage 1      0.756     0.802     0.778        81\n",
      "     Stage 2      0.167     0.147     0.156        34\n",
      "     Stage 3      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.667       222\n",
      "   macro avg      0.627     0.659     0.642       222\n",
      "weighted avg      0.654     0.667     0.660       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ J+H ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "Baseline (Top-5)          75.2%        14.7%\n",
      "å¢å¼· Stage 2 (3å€)           64.4%        14.7%\n",
      "å¢å¼· Stage 2 (5å€)           63.5%        14.7%\n",
      "å¢å¼· Stage 2+3              66.7%        14.7%\n",
      "\n",
      "ğŸ† ç¶œåˆæœ€ä½³: å¢å¼· Stage 2+3\n",
      "   æº–ç¢ºç‡: 66.7%\n",
      "   Stage 2 Recall: 14.7%\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +4.5%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jh_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#å¤±æ•—!!!!ä¸è¦åŸ·è¡Œ\n",
    "\n",
    "\n",
    "# ===== æ–¹æ¡ˆ J + H: Top-5 ç‰¹å¾µ + æ•¸æ“šå¢å¼· =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ J+H: Top-5 ç‰¹å¾µ + æ•¸æ“šå¢å¼·\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: {top_5_features}\")\n",
    "\n",
    "# ========== æ•¸æ“šå¢å¼·å‡½æ•¸ ==========\n",
    "def augment_data(X, y, target_class=2, augment_factor=3):\n",
    "    \"\"\"\n",
    "    å°ç‰¹å®šé¡åˆ¥åšæ•¸æ“šå¢å¼·\n",
    "    \n",
    "    æ–¹æ³•ï¼š\n",
    "    1. å™ªè²æ³¨å…¥ (Jittering)\n",
    "    2. ç‰¹å¾µç¸®æ”¾ (Scaling)\n",
    "    3. çµ„åˆæ“¾å‹•\n",
    "    \"\"\"\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    # æ‰¾å‡ºç›®æ¨™é¡åˆ¥çš„æ¨£æœ¬\n",
    "    target_indices = np.where(y == target_class)[0]\n",
    "    \n",
    "    print(f\"\\nğŸ”§ å¢å¼· Stage {target_class}:\")\n",
    "    print(f\"   åŸå§‹æ¨£æœ¬: {len(target_indices)} å€‹\")\n",
    "    \n",
    "    for idx in target_indices:\n",
    "        original_sample = X[idx]\n",
    "        \n",
    "        # 1. åŸå§‹æ¨£æœ¬\n",
    "        X_augmented.append(original_sample)\n",
    "        y_augmented.append(target_class)\n",
    "        \n",
    "        # 2-4. ç”Ÿæˆå¢å¼·æ¨£æœ¬\n",
    "        for i in range(augment_factor):\n",
    "            if i == 0:\n",
    "                # æ–¹æ³• 1: é«˜æ–¯å™ªè²æ³¨å…¥\n",
    "                noise = np.random.normal(0, 0.05, size=original_sample.shape)\n",
    "                augmented = original_sample + noise\n",
    "                \n",
    "            elif i == 1:\n",
    "                # æ–¹æ³• 2: ç‰¹å¾µç¸®æ”¾ (Â±5%)\n",
    "                scale = np.random.uniform(0.95, 1.05, size=original_sample.shape)\n",
    "                augmented = original_sample * scale\n",
    "                \n",
    "            else:\n",
    "                # æ–¹æ³• 3: çµ„åˆæ“¾å‹•\n",
    "                noise = np.random.normal(0, 0.03, size=original_sample.shape)\n",
    "                scale = np.random.uniform(0.97, 1.03, size=original_sample.shape)\n",
    "                augmented = original_sample * scale + noise\n",
    "            \n",
    "            X_augmented.append(augmented)\n",
    "            y_augmented.append(target_class)\n",
    "    \n",
    "    print(f\"   å¢å¼·å¾Œæ¨£æœ¬: {len(y_augmented)} å€‹ (x{augment_factor+1})\")\n",
    "    \n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "# ========== æ–¹æ³• 1: åªå¢å¼· Stage 2 ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: å¢å¼· Stage 2 (3å€)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_true_aug2 = []\n",
    "y_pred_aug2 = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # å¢å¼· Stage 2 è¨“ç·´æ¨£æœ¬\n",
    "    X_aug, y_aug = augment_data(X_train, y_train, target_class=2, augment_factor=3)\n",
    "    \n",
    "    # åˆä½µåŸå§‹ + å¢å¼·æ•¸æ“š\n",
    "    X_train_combined = np.vstack([X_train, X_aug])\n",
    "    y_train_combined = np.hstack([y_train, y_aug])\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train_combined)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_aug2.extend(y_test)\n",
    "    y_pred_aug2.extend(y_pred)\n",
    "\n",
    "acc_aug2 = accuracy_score(y_true_aug2, y_pred_aug2)\n",
    "cm_aug2 = confusion_matrix(y_true_aug2, y_pred_aug2)\n",
    "stage2_recall_aug2 = cm_aug2[2, 2] / cm_aug2[2, :].sum() if cm_aug2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¢å¼· Stage 2 æº–ç¢ºç‡: {acc_aug2:.3f} ({acc_aug2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_aug2:.1%} ({cm_aug2[2, 2]}/{cm_aug2[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_aug2, y_pred_aug2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_aug2)\n",
    "\n",
    "# ========== æ–¹æ³• 2: å¢å¼· Stage 2 (5å€) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: å¢å¼· Stage 2 (5å€ï¼Œæ›´æ¿€é€²)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_aug2_5x = []\n",
    "y_pred_aug2_5x = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # 5å€å¢å¼·\n",
    "    X_aug, y_aug = augment_data(X_train, y_train, target_class=2, augment_factor=5)\n",
    "    \n",
    "    X_train_combined = np.vstack([X_train, X_aug])\n",
    "    y_train_combined = np.hstack([y_train, y_aug])\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train_combined)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_aug2_5x.extend(y_test)\n",
    "    y_pred_aug2_5x.extend(y_pred)\n",
    "\n",
    "acc_aug2_5x = accuracy_score(y_true_aug2_5x, y_pred_aug2_5x)\n",
    "cm_aug2_5x = confusion_matrix(y_true_aug2_5x, y_pred_aug2_5x)\n",
    "stage2_recall_aug2_5x = cm_aug2_5x[2, 2] / cm_aug2_5x[2, :].sum() if cm_aug2_5x[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¢å¼· Stage 2 (5å€) æº–ç¢ºç‡: {acc_aug2_5x:.3f} ({acc_aug2_5x:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_aug2_5x:.1%} ({cm_aug2_5x[2, 2]}/{cm_aug2_5x[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_aug2_5x, y_pred_aug2_5x,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 3: å¢å¼·æ‰€æœ‰å°‘æ•¸é¡åˆ¥ (Stage 2 + 3) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: å¢å¼· Stage 2 å’Œ Stage 3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_aug_all = []\n",
    "y_pred_aug_all = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # å¢å¼· Stage 2\n",
    "    X_aug2, y_aug2 = augment_data(X_train, y_train, target_class=2, augment_factor=4)\n",
    "    \n",
    "    # å¢å¼· Stage 3\n",
    "    X_aug3, y_aug3 = augment_data(X_train, y_train, target_class=3, augment_factor=2)\n",
    "    \n",
    "    # åˆä½µ\n",
    "    X_train_combined = np.vstack([X_train, X_aug2, X_aug3])\n",
    "    y_train_combined = np.hstack([y_train, y_aug2, y_aug3])\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train_combined)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_aug_all.extend(y_test)\n",
    "    y_pred_aug_all.extend(y_pred)\n",
    "\n",
    "acc_aug_all = accuracy_score(y_true_aug_all, y_pred_aug_all)\n",
    "cm_aug_all = confusion_matrix(y_true_aug_all, y_pred_aug_all)\n",
    "stage2_recall_aug_all = cm_aug_all[2, 2] / cm_aug_all[2, :].sum() if cm_aug_all[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¢å¼·å¤šé¡åˆ¥ æº–ç¢ºç‡: {acc_aug_all:.3f} ({acc_aug_all:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_aug_all:.1%} ({cm_aug_all[2, 2]}/{cm_aug_all[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_aug_all, y_pred_aug_all,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ J+H ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('Baseline (Top-5)', 0.752, 0.147),\n",
    "    ('å¢å¼· Stage 2 (3å€)', acc_aug2, stage2_recall_aug2),\n",
    "    ('å¢å¼· Stage 2 (5å€)', acc_aug2_5x, stage2_recall_aug2_5x),\n",
    "    ('å¢å¼· Stage 2+3', acc_aug_all, stage2_recall_aug_all)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    print(f\"{method:<25} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³å¹³è¡¡\n",
    "best_idx = 0\n",
    "best_score = 0\n",
    "\n",
    "for i, (method, acc, s2_recall) in enumerate(results[1:], 1):\n",
    "    # ç¶œåˆåˆ†æ•¸: 70% æº–ç¢ºç‡ + 30% Stage 2 recall\n",
    "    score = 0.7 * acc + 0.3 * s2_recall\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_idx = i\n",
    "\n",
    "best_method = results[best_idx]\n",
    "print(f\"\\nğŸ† ç¶œåˆæœ€ä½³: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement = (best_method[1] - 0.622) * 100\n",
    "print(f\"   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement:.1f}%\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_jh = {\n",
    "    'aug2_3x': {'accuracy': acc_aug2, 'stage2_recall': stage2_recall_aug2,\n",
    "                'y_true': y_true_aug2, 'y_pred': y_pred_aug2},\n",
    "    'aug2_5x': {'accuracy': acc_aug2_5x, 'stage2_recall': stage2_recall_aug2_5x,\n",
    "                'y_true': y_true_aug2_5x, 'y_pred': y_pred_aug2_5x},\n",
    "    'aug_all': {'accuracy': acc_aug_all, 'stage2_recall': stage2_recall_aug_all,\n",
    "                'y_true': y_true_aug_all, 'y_pred': y_pred_aug_all},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1],\n",
    "    'best_stage2_recall': best_method[2]\n",
    "}\n",
    "\n",
    "with open('models/solution_jh_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_jh, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_jh_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae319476-198d-4f75-a355-158dfb00a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ F: åŠç›£ç£å­¸ç¿’ + å½æ¨™ç±¤\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: å–®æ¬¡å½æ¨™ç±¤ï¼ˆé«˜ç½®ä¿¡åº¦ > 0.8ï¼‰\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.757\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 37/37 (100.0%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.757\n",
      "\n",
      "ğŸ“Š Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 1.000\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 22/30 (73.3%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 1.000\n",
      "\n",
      "ğŸ“Š Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.800\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 16/30 (53.3%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.800\n",
      "\n",
      "ğŸ“Š Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.759\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 26/29 (89.7%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.759\n",
      "\n",
      "ğŸ“Š Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.500\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 29/30 (96.7%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.533\n",
      "\n",
      "ğŸ“Š Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.864\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 0/22 (0.0%)\n",
      "   âš ï¸  æ²’æœ‰é«˜ç½®ä¿¡åº¦æ¨£æœ¬ï¼Œä½¿ç”¨åˆå§‹é æ¸¬\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.864\n",
      "\n",
      "ğŸ“Š Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.682\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 3/22 (13.6%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.682\n",
      "\n",
      "ğŸ“Š Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: 0.636\n",
      "   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: 5/22 (22.7%)\n",
      "   é‡è¨“å¾Œæº–ç¢ºç‡: 0.636\n",
      "\n",
      "âœ… å–®æ¬¡å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: 0.757 (75.7%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.769     0.892     0.826        93\n",
      "     Stage 1      0.733     0.815     0.772        81\n",
      "     Stage 2      0.500     0.147     0.227        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.757       222\n",
      "   macro avg      0.750     0.714     0.706       222\n",
      "weighted avg      0.729     0.757     0.725       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[83  8  2  0]\n",
      " [12 66  3  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: è¿­ä»£å¼å½æ¨™ç±¤ï¼ˆ3è¼ªè¿­ä»£ï¼‰\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 37/37 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 37/37 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 37/37 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.757\n",
      "\n",
      "ğŸ“Š Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 22/30 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 22/30 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 30/30 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 1.000\n",
      "\n",
      "ğŸ“Š Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 8/30 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 16/30 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 23/30 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.800\n",
      "\n",
      "ğŸ“Š Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 21/29 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 26/29 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 28/29 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.759\n",
      "\n",
      "ğŸ“Š Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 29/30 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 30/30 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 30/30 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.533\n",
      "\n",
      "ğŸ“Š Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 0/22 (é–¾å€¼=0.9)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.864\n",
      "\n",
      "ğŸ“Š Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 0/22 (é–¾å€¼=0.9)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.682\n",
      "\n",
      "ğŸ“Š Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "   è¿­ä»£ 1: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 3/22 (é–¾å€¼=0.9)\n",
      "   è¿­ä»£ 2: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 7/22 (é–¾å€¼=0.8)\n",
      "   è¿­ä»£ 3: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ 8/22 (é–¾å€¼=0.7000000000000001)\n",
      "   æœ€çµ‚æº–ç¢ºç‡: 0.545\n",
      "\n",
      "âœ… è¿­ä»£å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: 0.748 (74.8%)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.769     0.892     0.826        93\n",
      "     Stage 1      0.744     0.790     0.766        81\n",
      "     Stage 2      0.357     0.147     0.208        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.748       222\n",
      "   macro avg      0.717     0.707     0.700       222\n",
      "weighted avg      0.711     0.748     0.721       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[83  6  4  0]\n",
      " [12 64  5  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: ä¿å®ˆå¼å½æ¨™ç±¤ï¼ˆåªå¢å¼· Stage 2 é æ¸¬ï¼‰\n",
      "============================================================\n",
      "\n",
      "âœ… Stage 2 å°ˆç”¨å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: 0.752 (75.2%)\n",
      "âœ… Stage 2 Recall: 14.7%\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.761     0.892     0.822        93\n",
      "     Stage 1      0.730     0.802     0.765        81\n",
      "     Stage 2      0.500     0.147     0.227        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.752       222\n",
      "   macro avg      0.748     0.711     0.703       222\n",
      "weighted avg      0.725     0.752     0.721       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ F ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "Baseline (Top-5)          75.2%        14.7%\n",
      "å–®æ¬¡å½æ¨™ç±¤ (0.8)               75.7%        14.7%\n",
      "è¿­ä»£å½æ¨™ç±¤ (3è¼ª)                74.8%        14.7%\n",
      "Stage 2 å°ˆç”¨                75.2%        14.7%\n",
      "\n",
      "ğŸ† æœ€ä½³æ–¹æ³•: å–®æ¬¡å½æ¨™ç±¤ (0.8)\n",
      "   æº–ç¢ºç‡: 75.7%\n",
      "   Stage 2 Recall: 14.7%\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +13.5%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_f_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ F: åŠç›£ç£å­¸ç¿’ + å½æ¨™ç±¤ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ F: åŠç›£ç£å­¸ç¿’ + å½æ¨™ç±¤\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: å–®æ¬¡å½æ¨™ç±¤ï¼ˆåŸºç¤ç‰ˆï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: å–®æ¬¡å½æ¨™ç±¤ï¼ˆé«˜ç½®ä¿¡åº¦ > 0.8ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pseudo1 = []\n",
    "y_pred_pseudo1 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    print(f\"\\nğŸ“Š Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 1: ç”¨ 7 é¡†é³³æ¢¨è¨“ç·´åˆå§‹æ¨¡å‹\n",
    "    rf_initial = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_initial.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 2: å°æ¸¬è©¦é³³æ¢¨é æ¸¬ï¼ˆå¸¶æ©Ÿç‡ï¼‰\n",
    "    y_test_proba = rf_initial.predict_proba(X_test_scaled)\n",
    "    y_test_pred_initial = rf_initial.predict(X_test_scaled)\n",
    "    max_proba = y_test_proba.max(axis=1)\n",
    "    \n",
    "    # Step 3: é¸æ“‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬ä½œç‚ºå½æ¨™ç±¤\n",
    "    high_confidence_mask = max_proba > 0.8\n",
    "    n_pseudo = high_confidence_mask.sum()\n",
    "    \n",
    "    print(f\"   åˆå§‹æ¨¡å‹æº–ç¢ºç‡: {accuracy_score(y_test, y_test_pred_initial):.3f}\")\n",
    "    print(f\"   é«˜ç½®ä¿¡åº¦æ¨£æœ¬: {n_pseudo}/{len(y_test)} ({n_pseudo/len(y_test)*100:.1f}%)\")\n",
    "    \n",
    "    if n_pseudo > 0:\n",
    "        # å–å¾—å½æ¨™ç±¤\n",
    "        X_pseudo = X_test_scaled[high_confidence_mask]\n",
    "        y_pseudo = y_test_pred_initial[high_confidence_mask]\n",
    "        \n",
    "        # Step 4: åˆä½µè¨“ç·´é›† + å½æ¨™ç±¤ï¼Œé‡æ–°è¨“ç·´\n",
    "        X_train_augmented = np.vstack([X_train_scaled, X_pseudo])\n",
    "        y_train_augmented = np.hstack([y_train, y_pseudo])\n",
    "        \n",
    "        rf_retrained = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_retrained.fit(X_train_augmented, y_train_augmented)\n",
    "        \n",
    "        # Step 5: æœ€çµ‚é æ¸¬\n",
    "        y_pred_final = rf_retrained.predict(X_test_scaled)\n",
    "    else:\n",
    "        print(\"   âš ï¸  æ²’æœ‰é«˜ç½®ä¿¡åº¦æ¨£æœ¬ï¼Œä½¿ç”¨åˆå§‹é æ¸¬\")\n",
    "        y_pred_final = y_test_pred_initial\n",
    "    \n",
    "    final_acc = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"   é‡è¨“å¾Œæº–ç¢ºç‡: {final_acc:.3f}\")\n",
    "    \n",
    "    y_true_pseudo1.extend(y_test)\n",
    "    y_pred_pseudo1.extend(y_pred_final)\n",
    "\n",
    "acc_pseudo1 = accuracy_score(y_true_pseudo1, y_pred_pseudo1)\n",
    "print(f\"\\nâœ… å–®æ¬¡å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: {acc_pseudo1:.3f} ({acc_pseudo1:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_pseudo1, y_pred_pseudo1,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_pseudo1, y_pred_pseudo1))\n",
    "\n",
    "# ========== æ–¹æ³• 2: è¿­ä»£å¼å½æ¨™ç±¤ï¼ˆé€²éšç‰ˆï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: è¿­ä»£å¼å½æ¨™ç±¤ï¼ˆ3è¼ªè¿­ä»£ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pseudo_iter = []\n",
    "y_pred_pseudo_iter = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    print(f\"\\nğŸ“Š Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # è¿­ä»£å¼è‡ªè¨“ç·´\n",
    "    X_labeled = X_train_scaled.copy()\n",
    "    y_labeled = y_train.copy()\n",
    "    X_unlabeled = X_test_scaled.copy()\n",
    "    \n",
    "    confidence_threshold = 0.9\n",
    "    max_iterations = 3\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # è¨“ç·´æ¨¡å‹\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_labeled, y_labeled)\n",
    "        \n",
    "        # é æ¸¬æœªæ¨™è¨»æ•¸æ“š\n",
    "        y_unlabeled_proba = rf.predict_proba(X_unlabeled)\n",
    "        y_unlabeled_pred = rf.predict(X_unlabeled)\n",
    "        max_proba = y_unlabeled_proba.max(axis=1)\n",
    "        \n",
    "        # é¸æ“‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬\n",
    "        high_conf_mask = max_proba > confidence_threshold\n",
    "        n_high_conf = high_conf_mask.sum()\n",
    "        \n",
    "        print(f\"   è¿­ä»£ {iteration+1}: é«˜ç½®ä¿¡åº¦æ¨£æœ¬ {n_high_conf}/{len(X_unlabeled)} (é–¾å€¼={confidence_threshold})\")\n",
    "        \n",
    "        if n_high_conf == 0:\n",
    "            break\n",
    "        \n",
    "        # å°‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬åŠ å…¥è¨“ç·´é›†\n",
    "        X_new_labeled = X_unlabeled[high_conf_mask]\n",
    "        y_new_labeled = y_unlabeled_pred[high_conf_mask]\n",
    "        \n",
    "        X_labeled = np.vstack([X_labeled, X_new_labeled])\n",
    "        y_labeled = np.hstack([y_labeled, y_new_labeled])\n",
    "        \n",
    "        # ç§»é™¤å·²æ¨™è¨»çš„æ¨£æœ¬ï¼ˆå¦‚æœéœ€è¦ç¹¼çºŒè¿­ä»£ï¼‰\n",
    "        # X_unlabeled = X_unlabeled[~high_conf_mask]\n",
    "        \n",
    "        # é™ä½é–¾å€¼ä»¥ç²å–æ›´å¤šæ¨£æœ¬\n",
    "        confidence_threshold = max(0.7, confidence_threshold - 0.1)\n",
    "    \n",
    "    # æœ€çµ‚æ¨¡å‹\n",
    "    rf_final = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_final.fit(X_labeled, y_labeled)\n",
    "    y_pred_final = rf_final.predict(X_test_scaled)\n",
    "    \n",
    "    final_acc = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"   æœ€çµ‚æº–ç¢ºç‡: {final_acc:.3f}\")\n",
    "    \n",
    "    y_true_pseudo_iter.extend(y_test)\n",
    "    y_pred_pseudo_iter.extend(y_pred_final)\n",
    "\n",
    "acc_pseudo_iter = accuracy_score(y_true_pseudo_iter, y_pred_pseudo_iter)\n",
    "print(f\"\\nâœ… è¿­ä»£å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: {acc_pseudo_iter:.3f} ({acc_pseudo_iter:.1%})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_pseudo_iter, y_pred_pseudo_iter,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(confusion_matrix(y_true_pseudo_iter, y_pred_pseudo_iter))\n",
    "\n",
    "# ========== æ–¹æ³• 3: ä¿å®ˆå¼å½æ¨™ç±¤ï¼ˆåªé‡å° Stage 2ï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: ä¿å®ˆå¼å½æ¨™ç±¤ï¼ˆåªå¢å¼· Stage 2 é æ¸¬ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pseudo_stage2 = []\n",
    "y_pred_pseudo_stage2 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # åˆå§‹æ¨¡å‹\n",
    "    rf_initial = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_initial.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # é æ¸¬æ¸¬è©¦é›†\n",
    "    y_test_proba = rf_initial.predict_proba(X_test_scaled)\n",
    "    y_test_pred = rf_initial.predict(X_test_scaled)\n",
    "    \n",
    "    # åªé¸æ“‡é«˜ç½®ä¿¡åº¦çš„ Stage 2 é æ¸¬ä½œç‚ºå½æ¨™ç±¤\n",
    "    stage2_confidence = y_test_proba[:, 2]  # Stage 2 çš„æ©Ÿç‡\n",
    "    stage2_mask = (y_test_pred == 2) & (stage2_confidence > 0.85)\n",
    "    \n",
    "    n_stage2_pseudo = stage2_mask.sum()\n",
    "    \n",
    "    if n_stage2_pseudo > 0:\n",
    "        # åŠ å…¥ Stage 2 å½æ¨™ç±¤\n",
    "        X_pseudo_stage2 = X_test_scaled[stage2_mask]\n",
    "        y_pseudo_stage2 = np.full(n_stage2_pseudo, 2)\n",
    "        \n",
    "        X_train_augmented = np.vstack([X_train_scaled, X_pseudo_stage2])\n",
    "        y_train_augmented = np.hstack([y_train, y_pseudo_stage2])\n",
    "        \n",
    "        # é‡è¨“ï¼Œçµ¦ Stage 2 æ›´é«˜æ¬Šé‡\n",
    "        rf_retrained = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            class_weight={0: 1, 1: 1, 2: 2, 3: 1},  # Stage 2 åŠ æ¬Š\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_retrained.fit(X_train_augmented, y_train_augmented)\n",
    "        \n",
    "        y_pred_final = rf_retrained.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred_final = y_test_pred\n",
    "    \n",
    "    y_true_pseudo_stage2.extend(y_test)\n",
    "    y_pred_pseudo_stage2.extend(y_pred_final)\n",
    "\n",
    "acc_pseudo_stage2 = accuracy_score(y_true_pseudo_stage2, y_pred_pseudo_stage2)\n",
    "cm_stage2 = confusion_matrix(y_true_pseudo_stage2, y_pred_pseudo_stage2)\n",
    "stage2_recall = cm_stage2[2, 2] / cm_stage2[2, :].sum() if cm_stage2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… Stage 2 å°ˆç”¨å½æ¨™ç±¤ LOSOæº–ç¢ºç‡: {acc_pseudo_stage2:.3f} ({acc_pseudo_stage2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall:.1%}\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_pseudo_stage2, y_pred_pseudo_stage2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ F ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('Baseline (Top-5)', 0.752, 0.147),\n",
    "    ('å–®æ¬¡å½æ¨™ç±¤ (0.8)', acc_pseudo1, \n",
    "     confusion_matrix(y_true_pseudo1, y_pred_pseudo1)[2,2]/confusion_matrix(y_true_pseudo1, y_pred_pseudo1)[2,:].sum()),\n",
    "    ('è¿­ä»£å½æ¨™ç±¤ (3è¼ª)', acc_pseudo_iter,\n",
    "     confusion_matrix(y_true_pseudo_iter, y_pred_pseudo_iter)[2,2]/confusion_matrix(y_true_pseudo_iter, y_pred_pseudo_iter)[2,:].sum()),\n",
    "    ('Stage 2 å°ˆç”¨', acc_pseudo_stage2, stage2_recall)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    print(f\"{method:<25} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results])\n",
    "best_method = results[best_idx]\n",
    "\n",
    "print(f\"\\nğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement = (best_method[1] - 0.622) * 100\n",
    "print(f\"   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement:.1f}%\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_f = {\n",
    "    'pseudo1': {'accuracy': acc_pseudo1, 'y_true': y_true_pseudo1, 'y_pred': y_pred_pseudo1},\n",
    "    'pseudo_iter': {'accuracy': acc_pseudo_iter, 'y_true': y_true_pseudo_iter, 'y_pred': y_pred_pseudo_iter},\n",
    "    'pseudo_stage2': {'accuracy': acc_pseudo_stage2, 'y_true': y_true_pseudo_stage2, 'y_pred': y_pred_pseudo_stage2},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_f_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_f, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_f_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19592e61-1bea-4050-9e04-906d26f2851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 1: XGBoost + LightGBM\n",
      "============================================================\n",
      "âš ï¸  XGBoost æœªå®‰è£ï¼Œå˜—è©¦å®‰è£...\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.4.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.17.0)\n",
      "Downloading xgboost-3.1.3-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.3\n",
      "âš ï¸  LightGBM æœªå®‰è£ï¼Œå˜—è©¦å®‰è£...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.4.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.17.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Baseline: Random Forest (åƒè€ƒ)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.752 (75.2%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: XGBoost\n",
      "============================================================\n",
      "\n",
      "âœ… XGBoost æº–ç¢ºç‡: 0.721 (72.1%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.779     0.871     0.822        93\n",
      "     Stage 1      0.678     0.753     0.713        81\n",
      "     Stage 2      0.357     0.147     0.208        34\n",
      "     Stage 3      0.929     0.929     0.929        14\n",
      "\n",
      "    accuracy                          0.721       222\n",
      "   macro avg      0.686     0.675     0.668       222\n",
      "weighted avg      0.687     0.721     0.695       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[81  5  6  1]\n",
      " [17 61  3  0]\n",
      " [ 6 23  5  0]\n",
      " [ 0  1  0 13]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: XGBoost (èª¿åƒç‰ˆ)\n",
      "============================================================\n",
      "\n",
      "âœ… XGBoost (èª¿åƒ) æº–ç¢ºç‡: 0.712 (71.2%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.741     0.860     0.796        93\n",
      "     Stage 1      0.747     0.728     0.738        81\n",
      "     Stage 2      0.250     0.147     0.185        34\n",
      "     Stage 3      0.933     1.000     0.966        14\n",
      "\n",
      "    accuracy                          0.712       222\n",
      "   macro avg      0.668     0.684     0.671       222\n",
      "weighted avg      0.680     0.712     0.692       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: LightGBM\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LightGBM æº–ç¢ºç‡: 0.626 (62.6%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.701     0.656     0.678        93\n",
      "     Stage 1      0.615     0.728     0.667        81\n",
      "     Stage 2      0.217     0.147     0.175        34\n",
      "     Stage 3      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.626       222\n",
      "   macro avg      0.602     0.633     0.613       222\n",
      "weighted avg      0.606     0.626     0.613       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[61 21  9  2]\n",
      " [13 59  9  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 4: LightGBM (é¡åˆ¥å¹³è¡¡)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LightGBM (å¹³è¡¡) æº–ç¢ºç‡: 0.635 (63.5%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.685     0.677     0.681        93\n",
      "     Stage 1      0.596     0.765     0.670        81\n",
      "     Stage 2      0.333     0.147     0.204        34\n",
      "     Stage 3      1.000     0.786     0.880        14\n",
      "\n",
      "    accuracy                          0.635       222\n",
      "   macro avg      0.654     0.594     0.609       222\n",
      "weighted avg      0.618     0.635     0.617       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 1 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "âœ… Random Forest (Baseline) 75.2%        14.7%\n",
      " XGBoost                 72.1%        14.7%\n",
      " XGBoost (èª¿åƒ)            71.2%        14.7%\n",
      " LightGBM                62.6%        14.7%\n",
      " LightGBM (å¹³è¡¡)           63.5%        14.7%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³æ–¹æ³•: Random Forest (Baseline)\n",
      "   æº–ç¢ºç‡: 75.2%\n",
      "   Stage 2 Recall: 14.7%\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +13.0%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆJ (75.2%): +0.0%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_boosting_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 1: XGBoost + LightGBM =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 1: XGBoost + LightGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å…ˆæª¢æŸ¥å¥—ä»¶\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"âœ… XGBoost å¯ç”¨\")\n",
    "    has_xgb = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  XGBoost æœªå®‰è£ï¼Œå˜—è©¦å®‰è£...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
    "    from xgboost import XGBClassifier\n",
    "    has_xgb = True\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    print(\"âœ… LightGBM å¯ç”¨\")\n",
    "    has_lgb = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  LightGBM æœªå®‰è£ï¼Œå˜—è©¦å®‰è£...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'lightgbm'])\n",
    "    from lightgbm import LGBMClassifier\n",
    "    has_lgb = True\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ: {top_5_features}\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== Baseline: Random Forest (åƒè€ƒ) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š Baseline: Random Forest (åƒè€ƒ)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_rf = []\n",
    "y_pred_rf = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_true_rf.extend(y_test)\n",
    "    y_pred_rf.extend(y_pred)\n",
    "\n",
    "acc_rf = accuracy_score(y_true_rf, y_pred_rf)\n",
    "print(f\"Random Forest: {acc_rf:.3f} ({acc_rf:.1%})\")\n",
    "\n",
    "# ========== æ–¹æ³• 1: XGBoost ==========\n",
    "if has_xgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 1: XGBoost\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_xgb = []\n",
    "    y_pred_xgb = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        xgb = XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = xgb.predict(X_test_scaled)\n",
    "        y_true_xgb.extend(y_test)\n",
    "        y_pred_xgb.extend(y_pred)\n",
    "    \n",
    "    acc_xgb = accuracy_score(y_true_xgb, y_pred_xgb)\n",
    "    cm_xgb = confusion_matrix(y_true_xgb, y_pred_xgb)\n",
    "    stage2_recall_xgb = cm_xgb[2, 2] / cm_xgb[2, :].sum() if cm_xgb[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… XGBoost æº–ç¢ºç‡: {acc_xgb:.3f} ({acc_xgb:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_xgb:.1%} ({cm_xgb[2, 2]}/{cm_xgb[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_xgb, y_pred_xgb,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "    print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "    print(cm_xgb)\n",
    "\n",
    "# ========== æ–¹æ³• 2: XGBoost èª¿åƒç‰ˆ ==========\n",
    "if has_xgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 2: XGBoost (èª¿åƒç‰ˆ)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_xgb_tuned = []\n",
    "    y_pred_xgb_tuned = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # è¨ˆç®—é¡åˆ¥æ¬Šé‡\n",
    "        from sklearn.utils.class_weight import compute_sample_weight\n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "        \n",
    "        xgb_tuned = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=0.1,\n",
    "            min_child_weight=3,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb_tuned.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        y_pred = xgb_tuned.predict(X_test_scaled)\n",
    "        y_true_xgb_tuned.extend(y_test)\n",
    "        y_pred_xgb_tuned.extend(y_pred)\n",
    "    \n",
    "    acc_xgb_tuned = accuracy_score(y_true_xgb_tuned, y_pred_xgb_tuned)\n",
    "    cm_xgb_tuned = confusion_matrix(y_true_xgb_tuned, y_pred_xgb_tuned)\n",
    "    stage2_recall_xgb_tuned = cm_xgb_tuned[2, 2] / cm_xgb_tuned[2, :].sum() if cm_xgb_tuned[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… XGBoost (èª¿åƒ) æº–ç¢ºç‡: {acc_xgb_tuned:.3f} ({acc_xgb_tuned:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_xgb_tuned:.1%} ({cm_xgb_tuned[2, 2]}/{cm_xgb_tuned[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_xgb_tuned, y_pred_xgb_tuned,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 3: LightGBM ==========\n",
    "if has_lgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 3: LightGBM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_lgb = []\n",
    "    y_pred_lgb = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        lgb = LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lgb.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = lgb.predict(X_test_scaled)\n",
    "        y_true_lgb.extend(y_test)\n",
    "        y_pred_lgb.extend(y_pred)\n",
    "    \n",
    "    acc_lgb = accuracy_score(y_true_lgb, y_pred_lgb)\n",
    "    cm_lgb = confusion_matrix(y_true_lgb, y_pred_lgb)\n",
    "    stage2_recall_lgb = cm_lgb[2, 2] / cm_lgb[2, :].sum() if cm_lgb[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… LightGBM æº–ç¢ºç‡: {acc_lgb:.3f} ({acc_lgb:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_lgb:.1%} ({cm_lgb[2, 2]}/{cm_lgb[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_lgb, y_pred_lgb,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "    print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "    print(cm_lgb)\n",
    "\n",
    "# ========== æ–¹æ³• 4: LightGBM é¡åˆ¥å¹³è¡¡ ==========\n",
    "if has_lgb:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Œ æ–¹æ³• 4: LightGBM (é¡åˆ¥å¹³è¡¡)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    y_true_lgb_balanced = []\n",
    "    y_pred_lgb_balanced = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y, groups=pineapple_ids):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        lgb_balanced = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lgb_balanced.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = lgb_balanced.predict(X_test_scaled)\n",
    "        y_true_lgb_balanced.extend(y_test)\n",
    "        y_pred_lgb_balanced.extend(y_pred)\n",
    "    \n",
    "    acc_lgb_balanced = accuracy_score(y_true_lgb_balanced, y_pred_lgb_balanced)\n",
    "    cm_lgb_balanced = confusion_matrix(y_true_lgb_balanced, y_pred_lgb_balanced)\n",
    "    stage2_recall_lgb_balanced = cm_lgb_balanced[2, 2] / cm_lgb_balanced[2, :].sum() if cm_lgb_balanced[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… LightGBM (å¹³è¡¡) æº–ç¢ºç‡: {acc_lgb_balanced:.3f} ({acc_lgb_balanced:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall_lgb_balanced:.1%} ({cm_lgb_balanced[2, 2]}/{cm_lgb_balanced[2, :].sum()})\")\n",
    "    print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_true_lgb_balanced, y_pred_lgb_balanced,\n",
    "                              target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                              digits=3))\n",
    "\n",
    "# ========== ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 1 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('Random Forest (Baseline)', acc_rf, \n",
    "     confusion_matrix(y_true_rf, y_pred_rf)[2,2]/confusion_matrix(y_true_rf, y_pred_rf)[2,:].sum()),\n",
    "]\n",
    "\n",
    "if has_xgb:\n",
    "    results.append(('XGBoost', acc_xgb, stage2_recall_xgb))\n",
    "    results.append(('XGBoost (èª¿åƒ)', acc_xgb_tuned, stage2_recall_xgb_tuned))\n",
    "\n",
    "if has_lgb:\n",
    "    results.append(('LightGBM', acc_lgb, stage2_recall_lgb))\n",
    "    results.append(('LightGBM (å¹³è¡¡)', acc_lgb_balanced, stage2_recall_lgb_balanced))\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.757 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results])\n",
    "best_method = results[best_idx]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (best_method[1] - 0.622) * 100\n",
    "improvement_vs_rf = (best_method[1] - 0.752) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆJ (75.2%): {improvement_vs_rf:+.1f}%\")\n",
    "\n",
    "if best_method[1] > 0.757:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ F (75.7%)ï¼\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_boosting = {\n",
    "    'rf': {'accuracy': acc_rf, 'y_true': y_true_rf, 'y_pred': y_pred_rf}\n",
    "}\n",
    "\n",
    "if has_xgb:\n",
    "    results_boosting['xgb'] = {'accuracy': acc_xgb, 'y_true': y_true_xgb, 'y_pred': y_pred_xgb}\n",
    "    results_boosting['xgb_tuned'] = {'accuracy': acc_xgb_tuned, 'y_true': y_true_xgb_tuned, 'y_pred': y_pred_xgb_tuned}\n",
    "\n",
    "if has_lgb:\n",
    "    results_boosting['lgb'] = {'accuracy': acc_lgb, 'y_true': y_true_lgb, 'y_pred': y_pred_lgb}\n",
    "    results_boosting['lgb_balanced'] = {'accuracy': acc_lgb_balanced, 'y_true': y_true_lgb_balanced, 'y_pred': y_pred_lgb_balanced}\n",
    "\n",
    "results_boosting['best_method'] = best_method[0]\n",
    "results_boosting['best_accuracy'] = best_method[1]\n",
    "\n",
    "with open('models/solution_boosting_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_boosting, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_boosting_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7f7ad2e-8824-4516-a5b1-37edbff288e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 2: é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š æº–å‚™ 3 ç¨®ç‰¹å¾µé›†:\n",
      "   - Top-5:  5 ç‰¹å¾µ\n",
      "   - Top-10: 10 ç‰¹å¾µ\n",
      "   - Top-15: 15 ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: ç°¡å–®æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰\n",
      "   çµ„åˆ: RF(Top-5) + RF(Top-10) + RF(Top-15)\n",
      "============================================================\n",
      "\n",
      "âœ… ç°¡å–®æŠ•ç¥¨ æº–ç¢ºç‡: 0.680 (68.0%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.720     0.720     0.720        93\n",
      "     Stage 1      0.607     0.802     0.691        81\n",
      "     Stage 2      0.625     0.147     0.238        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.680       222\n",
      "   macro avg      0.738     0.667     0.663       222\n",
      "weighted avg      0.682     0.680     0.654       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: åŠ æ¬ŠæŠ•ç¥¨\n",
      "   æ¬Šé‡æ ¹æ“šå„æ¨¡å‹åœ¨è¨“ç·´é›†çš„è¡¨ç¾\n",
      "============================================================\n",
      "\n",
      "âœ… åŠ æ¬ŠæŠ•ç¥¨ æº–ç¢ºç‡: 0.685 (68.5%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.725     0.710     0.717        93\n",
      "     Stage 1      0.615     0.827     0.705        81\n",
      "     Stage 2      0.714     0.147     0.244        34\n",
      "     Stage 3      0.933     1.000     0.966        14\n",
      "\n",
      "    accuracy                          0.685       222\n",
      "   macro avg      0.747     0.671     0.658       222\n",
      "weighted avg      0.696     0.685     0.656       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: æ··åˆæ¨¡å‹æŠ•ç¥¨\n",
      "   çµ„åˆ: RF(Top-5) + RF(Top-10) + XGBoost(Top-5)\n",
      "============================================================\n",
      "\n",
      "âœ… æ··åˆæŠ•ç¥¨ æº–ç¢ºç‡: 0.761 (76.1%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.773     0.914     0.837        93\n",
      "     Stage 1      0.750     0.815     0.781        81\n",
      "     Stage 2      0.500     0.147     0.227        34\n",
      "     Stage 3      0.929     0.929     0.929        14\n",
      "\n",
      "    accuracy                          0.761       222\n",
      "   macro avg      0.738     0.701     0.694       222\n",
      "weighted avg      0.732     0.761     0.729       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 4: 5æ¨¡å‹å¤§é›†æˆ\n",
      "   çµ„åˆ: RF(Top-5/10/15) + XGBoost(Top-5/10)\n",
      "============================================================\n",
      "\n",
      "âœ… å¤§é›†æˆ æº–ç¢ºç‡: 0.721 (72.1%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.752     0.817     0.784        93\n",
      "     Stage 1      0.667     0.815     0.733        81\n",
      "     Stage 2      0.625     0.147     0.238        34\n",
      "     Stage 3      0.929     0.929     0.929        14\n",
      "\n",
      "    accuracy                          0.721       222\n",
      "   macro avg      0.743     0.677     0.671       222\n",
      "weighted avg      0.713     0.721     0.691       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 2 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "âœ… å–®ä¸€æ¨¡å‹ (RF Top-5)         75.2%        14.7%\n",
      "âœ… æ–¹æ¡ˆF (å½æ¨™ç±¤)               75.7%        14.7%\n",
      " ç°¡å–®æŠ•ç¥¨ (3RF)              68.0%        14.7%\n",
      " åŠ æ¬ŠæŠ•ç¥¨ (3RF)              68.5%        14.7%\n",
      "ğŸ”¥ æ··åˆæŠ•ç¥¨ (2RF+XGB)          76.1%        14.7%\n",
      " å¤§é›†æˆ (3RF+2XGB)          72.1%        14.7%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³é›†æˆæ–¹æ³•: æ··åˆæŠ•ç¥¨ (2RF+XGB)\n",
      "   æº–ç¢ºç‡: 76.1%\n",
      "   Stage 2 Recall: 14.7%\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +13.9%\n",
      "   ç›¸æ¯”å–®ä¸€RF (75.2%): +0.9%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆF (75.7%): +0.4%\n",
      "\n",
      "   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ Fï¼æ–°ç´€éŒ„ï¼\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_ensemble_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 2: é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰=====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 2: é›†æˆå­¸ç¿’ï¼ˆEnsembleï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "# è¼‰å…¥ä¹‹å‰çš„çµæœï¼ˆç”¨æ–¼çµ„åˆï¼‰\n",
    "with open('models/solution_j_results.pkl', 'rb') as f:\n",
    "    j_results = pickle.load(f)\n",
    "    feature_importance = j_results['feature_importance']\n",
    "\n",
    "# æº–å‚™ä¸åŒç‰¹å¾µé›†\n",
    "top_10_features = feature_importance.head(10)['feature'].tolist()\n",
    "top_15_features = feature_importance.head(15)['feature'].tolist()\n",
    "\n",
    "top_10_indices = [data['features'].columns.tolist().index(f) for f in top_10_features]\n",
    "top_15_indices = [data['features'].columns.tolist().index(f) for f in top_15_features]\n",
    "\n",
    "X_top10 = X[:, top_10_indices]\n",
    "X_top15 = X[:, top_15_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š æº–å‚™ 3 ç¨®ç‰¹å¾µé›†:\")\n",
    "print(f\"   - Top-5:  {len(top_5_features)} ç‰¹å¾µ\")\n",
    "print(f\"   - Top-10: {len(top_10_features)} ç‰¹å¾µ\")\n",
    "print(f\"   - Top-15: {len(top_15_features)} ç‰¹å¾µ\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: ç°¡å–®æŠ•ç¥¨ï¼ˆ3å€‹RFä¸åŒç‰¹å¾µé›†ï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: ç°¡å–®æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰\")\n",
    "print(\"   çµ„åˆ: RF(Top-5) + RF(Top-10) + RF(Top-15)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote1 = []\n",
    "y_pred_vote1 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    # æº–å‚™ 3 å€‹ç‰¹å¾µé›†\n",
    "    datasets = [\n",
    "        (X_top5[train_idx], X_top5[test_idx]),\n",
    "        (X_top10[train_idx], X_top10[test_idx]),\n",
    "        (X_top15[train_idx], X_top15[test_idx])\n",
    "    ]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # è¨“ç·´ 3 å€‹æ¨¡å‹\n",
    "    predictions = []\n",
    "    \n",
    "    for X_train, X_test in datasets:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        pred = rf.predict(X_test_scaled)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰\n",
    "    predictions = np.array(predictions).T  # shape: (n_samples, 3)\n",
    "    y_pred_ensemble = []\n",
    "    \n",
    "    for sample_preds in predictions:\n",
    "        # å¤šæ•¸æ±º\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        y_pred_ensemble.append(majority_vote)\n",
    "    \n",
    "    y_true_vote1.extend(y_test)\n",
    "    y_pred_vote1.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote1 = accuracy_score(y_true_vote1, y_pred_vote1)\n",
    "cm_vote1 = confusion_matrix(y_true_vote1, y_pred_vote1)\n",
    "stage2_recall_vote1 = cm_vote1[2, 2] / cm_vote1[2, :].sum() if cm_vote1[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… ç°¡å–®æŠ•ç¥¨ æº–ç¢ºç‡: {acc_vote1:.3f} ({acc_vote1:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote1:.1%} ({cm_vote1[2, 2]}/{cm_vote1[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote1, y_pred_vote1,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 2: åŠ æ¬ŠæŠ•ç¥¨ï¼ˆæ ¹æ“šæº–ç¢ºç‡åŠ æ¬Šï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: åŠ æ¬ŠæŠ•ç¥¨\")\n",
    "print(\"   æ¬Šé‡æ ¹æ“šå„æ¨¡å‹åœ¨è¨“ç·´é›†çš„è¡¨ç¾\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote2 = []\n",
    "y_pred_vote2 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    datasets = [\n",
    "        (X_top5[train_idx], X_top5[test_idx]),\n",
    "        (X_top10[train_idx], X_top10[test_idx]),\n",
    "        (X_top15[train_idx], X_top15[test_idx])\n",
    "    ]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    predictions = []\n",
    "    probas = []\n",
    "    weights = []\n",
    "    \n",
    "    for X_train, X_test in datasets:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # è¨“ç·´é›†æº–ç¢ºç‡ä½œç‚ºæ¬Šé‡\n",
    "        train_pred = rf.predict(X_train_scaled)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        weights.append(train_acc)\n",
    "        \n",
    "        # æ¸¬è©¦é›†é æ¸¬\n",
    "        pred = rf.predict(X_test_scaled)\n",
    "        proba = rf.predict_proba(X_test_scaled)\n",
    "        predictions.append(pred)\n",
    "        probas.append(proba)\n",
    "    \n",
    "    # åŠ æ¬Šå¹³å‡æ©Ÿç‡\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()  # æ­£è¦åŒ–\n",
    "    \n",
    "    weighted_proba = np.zeros_like(probas[0])\n",
    "    for i, proba in enumerate(probas):\n",
    "        weighted_proba += weights[i] * proba\n",
    "    \n",
    "    y_pred_ensemble = np.argmax(weighted_proba, axis=1)\n",
    "    \n",
    "    y_true_vote2.extend(y_test)\n",
    "    y_pred_vote2.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote2 = accuracy_score(y_true_vote2, y_pred_vote2)\n",
    "cm_vote2 = confusion_matrix(y_true_vote2, y_pred_vote2)\n",
    "stage2_recall_vote2 = cm_vote2[2, 2] / cm_vote2[2, :].sum() if cm_vote2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… åŠ æ¬ŠæŠ•ç¥¨ æº–ç¢ºç‡: {acc_vote2:.3f} ({acc_vote2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote2:.1%} ({cm_vote2[2, 2]}/{cm_vote2[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote2, y_pred_vote2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 3: æ··åˆæ¨¡å‹æŠ•ç¥¨ï¼ˆRF + XGBoostï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: æ··åˆæ¨¡å‹æŠ•ç¥¨\")\n",
    "print(\"   çµ„åˆ: RF(Top-5) + RF(Top-10) + XGBoost(Top-5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote3 = []\n",
    "y_pred_vote3 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # æ¨¡å‹ 1: RF with Top-5\n",
    "    X_train_scaled = scaler.fit_transform(X_top5[train_idx])\n",
    "    X_test_scaled = scaler.transform(X_top5[test_idx])\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf1.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf1.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 2: RF with Top-10\n",
    "    X_train_scaled = scaler.fit_transform(X_top10[train_idx])\n",
    "    X_test_scaled = scaler.transform(X_top10[test_idx])\n",
    "    \n",
    "    rf2 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf2.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf2.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 3: XGBoost with Top-5\n",
    "    X_train_scaled = scaler.fit_transform(X_top5[train_idx])\n",
    "    X_test_scaled = scaler.transform(X_top5[test_idx])\n",
    "    \n",
    "    xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, \n",
    "                       random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    predictions.append(xgb.predict(X_test_scaled))\n",
    "    \n",
    "    # æŠ•ç¥¨\n",
    "    predictions = np.array(predictions).T\n",
    "    y_pred_ensemble = []\n",
    "    \n",
    "    for sample_preds in predictions:\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        y_pred_ensemble.append(majority_vote)\n",
    "    \n",
    "    y_true_vote3.extend(y_test)\n",
    "    y_pred_vote3.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote3 = accuracy_score(y_true_vote3, y_pred_vote3)\n",
    "cm_vote3 = confusion_matrix(y_true_vote3, y_pred_vote3)\n",
    "stage2_recall_vote3 = cm_vote3[2, 2] / cm_vote3[2, :].sum() if cm_vote3[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… æ··åˆæŠ•ç¥¨ æº–ç¢ºç‡: {acc_vote3:.3f} ({acc_vote3:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote3:.1%} ({cm_vote3[2, 2]}/{cm_vote3[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote3, y_pred_vote3,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== æ–¹æ³• 4: 5æ¨¡å‹å¤§é›†æˆ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 4: 5æ¨¡å‹å¤§é›†æˆ\")\n",
    "print(\"   çµ„åˆ: RF(Top-5/10/15) + XGBoost(Top-5/10)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_vote4 = []\n",
    "y_pred_vote4 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    probas = []\n",
    "    \n",
    "    # RF Top-5, Top-10, Top-15\n",
    "    for X_subset in [X_top5, X_top10, X_top15]:\n",
    "        X_train_scaled = scaler.fit_transform(X_subset[train_idx])\n",
    "        X_test_scaled = scaler.transform(X_subset[test_idx])\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        probas.append(rf.predict_proba(X_test_scaled))\n",
    "    \n",
    "    # XGBoost Top-5, Top-10\n",
    "    for X_subset in [X_top5, X_top10]:\n",
    "        X_train_scaled = scaler.fit_transform(X_subset[train_idx])\n",
    "        X_test_scaled = scaler.transform(X_subset[test_idx])\n",
    "        \n",
    "        xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "                           random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "        xgb.fit(X_train_scaled, y_train)\n",
    "        probas.append(xgb.predict_proba(X_test_scaled))\n",
    "    \n",
    "    # å¹³å‡æ©Ÿç‡\n",
    "    avg_proba = np.mean(probas, axis=0)\n",
    "    y_pred_ensemble = np.argmax(avg_proba, axis=1)\n",
    "    \n",
    "    y_true_vote4.extend(y_test)\n",
    "    y_pred_vote4.extend(y_pred_ensemble)\n",
    "\n",
    "acc_vote4 = accuracy_score(y_true_vote4, y_pred_vote4)\n",
    "cm_vote4 = confusion_matrix(y_true_vote4, y_pred_vote4)\n",
    "stage2_recall_vote4 = cm_vote4[2, 2] / cm_vote4[2, :].sum() if cm_vote4[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… å¤§é›†æˆ æº–ç¢ºç‡: {acc_vote4:.3f} ({acc_vote4:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_vote4:.1%} ({cm_vote4[2, 2]}/{cm_vote4[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_vote4, y_pred_vote4,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 2 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('å–®ä¸€æ¨¡å‹ (RF Top-5)', 0.752, 0.147),\n",
    "    ('æ–¹æ¡ˆF (å½æ¨™ç±¤)', 0.757, 0.147),\n",
    "    ('ç°¡å–®æŠ•ç¥¨ (3RF)', acc_vote1, stage2_recall_vote1),\n",
    "    ('åŠ æ¬ŠæŠ•ç¥¨ (3RF)', acc_vote2, stage2_recall_vote2),\n",
    "    ('æ··åˆæŠ•ç¥¨ (2RF+XGB)', acc_vote3, stage2_recall_vote3),\n",
    "    ('å¤§é›†æˆ (3RF+2XGB)', acc_vote4, stage2_recall_vote4)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.757 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results[2:]])  # è·³ébaseline\n",
    "best_method = results[best_idx + 2]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (best_method[1] - 0.622) * 100\n",
    "improvement_vs_single = (best_method[1] - 0.752) * 100\n",
    "improvement_vs_f = (best_method[1] - 0.757) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯”å–®ä¸€RF (75.2%): {improvement_vs_single:+.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆF (75.7%): {improvement_vs_f:+.1f}%\")\n",
    "\n",
    "if best_method[1] > 0.757:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ Fï¼æ–°ç´€éŒ„ï¼\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_ensemble = {\n",
    "    'vote1': {'accuracy': acc_vote1, 'y_true': y_true_vote1, 'y_pred': y_pred_vote1},\n",
    "    'vote2': {'accuracy': acc_vote2, 'y_true': y_true_vote2, 'y_pred': y_pred_vote2},\n",
    "    'vote3': {'accuracy': acc_vote3, 'y_true': y_true_vote3, 'y_pred': y_pred_vote3},\n",
    "    'vote4': {'accuracy': acc_vote4, 'y_true': y_true_vote4, 'y_pred': y_pred_vote4},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_ensemble_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_ensemble, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_ensemble_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f33fe6c7-c3e2-46c6-bf29-a42b66a8f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 5: å¤šéšæ®µç´šè¯æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 1: äºŒéšæ®µç´šè¯\n",
      "   ç¬¬ä¸€å±¤: æ—©æœŸ(0-1) vs æ™šæœŸ(2-3)\n",
      "   ç¬¬äºŒå±¤: å„çµ„å…§ç´°åˆ†\n",
      "============================================================\n",
      "\n",
      "âœ… äºŒéšæ®µç´šè¯ æº–ç¢ºç‡: 0.721 (72.1%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.743     0.806     0.773        93\n",
      "     Stage 1      0.673     0.815     0.737        81\n",
      "     Stage 2      0.556     0.147     0.233        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.721       222\n",
      "   macro avg      0.743     0.692     0.686       222\n",
      "weighted avg      0.705     0.721     0.692       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[75 16  2  0]\n",
      " [13 66  2  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 2: ä¸‰éšæ®µç´šè¯\n",
      "   ç¬¬ä¸€å±¤: Stage 0 vs Stage 1-2 vs Stage 3\n",
      "   ç¬¬äºŒå±¤: Stage 1-2 å†ç´°åˆ†\n",
      "============================================================\n",
      "\n",
      "âœ… ä¸‰éšæ®µç´šè¯ æº–ç¢ºç‡: 0.667 (66.7%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.717     0.710     0.714        93\n",
      "     Stage 1      0.643     0.778     0.704        81\n",
      "     Stage 2      0.278     0.147     0.192        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.667       222\n",
      "   macro avg      0.660     0.659     0.652       222\n",
      "weighted avg      0.641     0.667     0.648       222\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[66 19  8  0]\n",
      " [13 63  5  0]\n",
      " [13 16  5  0]\n",
      " [ 0  0  0 14]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Œ æ–¹æ³• 3: æ™ºèƒ½ç´šè¯ï¼ˆçµåˆç½®ä¿¡åº¦ï¼‰\n",
      "   ç¬¬ä¸€å±¤: æ—©æœŸ vs æ™šæœŸ\n",
      "   ç¬¬äºŒå±¤: ä½ç½®ä¿¡åº¦æ¨£æœ¬é‡åˆ†é¡\n",
      "============================================================\n",
      "\n",
      "âœ… æ™ºèƒ½ç´šè¯ æº–ç¢ºç‡: 0.716 (71.6%)\n",
      "âœ… Stage 2 Recall: 14.7% (5/34)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.743     0.806     0.773        93\n",
      "     Stage 1      0.670     0.802     0.730        81\n",
      "     Stage 2      0.500     0.147     0.227        34\n",
      "     Stage 3      1.000     1.000     1.000        14\n",
      "\n",
      "    accuracy                          0.716       222\n",
      "   macro avg      0.728     0.689     0.683       222\n",
      "weighted avg      0.695     0.716     0.688       222\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 5 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      "âœ… å–®ä¸€æ¨¡å‹ (RF Top-5)         75.2%        14.7%\n",
      "âœ… æ–¹æ¡ˆF (å½æ¨™ç±¤)               75.7%        14.7%\n",
      "âœ… æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)              76.1%        14.7%\n",
      " äºŒéšæ®µç´šè¯                   72.1%        14.7%\n",
      " ä¸‰éšæ®µç´šè¯                   66.7%        14.7%\n",
      " æ™ºèƒ½ç´šè¯                    71.6%        14.7%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³æ–¹æ³•: æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)\n",
      "   æº–ç¢ºç‡: 76.1%\n",
      "   Stage 2 Recall: 14.7%\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +13.9%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): +0.0%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_cascade_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 5: å¤šéšæ®µç´šè¯æ¨¡å‹ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 5: å¤šéšæ®µç´šè¯æ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# ä½¿ç”¨ Top-5 ç‰¹å¾µ\n",
    "top_5_features = ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "X_top5 = X[:, top_5_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨ Top-5 ç‰¹å¾µ\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ========== æ–¹æ³• 1: äºŒéšæ®µç´šè¯ (æ—©æœŸ vs æ™šæœŸ) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 1: äºŒéšæ®µç´šè¯\")\n",
    "print(\"   ç¬¬ä¸€å±¤: æ—©æœŸ(0-1) vs æ™šæœŸ(2-3)\")\n",
    "print(\"   ç¬¬äºŒå±¤: å„çµ„å…§ç´°åˆ†\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_cascade1 = []\n",
    "y_pred_cascade1 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ===== ç¬¬ä¸€å±¤ï¼šç²—åˆ†é¡ =====\n",
    "    # å°‡ Stage 0,1 â†’ 0 (æ—©æœŸ), Stage 2,3 â†’ 1 (æ™šæœŸ)\n",
    "    y_train_coarse = (y_train >= 2).astype(int)\n",
    "    \n",
    "    clf_coarse = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_coarse.fit(X_train_scaled, y_train_coarse)\n",
    "    \n",
    "    # é æ¸¬ç¬¬ä¸€å±¤\n",
    "    y_test_coarse_pred = clf_coarse.predict(X_test_scaled)\n",
    "    \n",
    "    # ===== ç¬¬äºŒå±¤ï¼šç´°åˆ†é¡ =====\n",
    "    y_test_pred = np.zeros(len(y_test), dtype=int)\n",
    "    \n",
    "    # å°æ—©æœŸçµ„ (0-1) çš„æ¨£æœ¬ç´°åˆ†\n",
    "    early_mask_train = y_train < 2\n",
    "    early_mask_test = y_test_coarse_pred == 0\n",
    "    \n",
    "    if early_mask_train.sum() > 0 and early_mask_test.sum() > 0:\n",
    "        X_train_early = X_train_scaled[early_mask_train]\n",
    "        y_train_early = y_train[early_mask_train]\n",
    "        \n",
    "        clf_early = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf_early.fit(X_train_early, y_train_early)\n",
    "        \n",
    "        X_test_early = X_test_scaled[early_mask_test]\n",
    "        y_test_pred[early_mask_test] = clf_early.predict(X_test_early)\n",
    "    \n",
    "    # å°æ™šæœŸçµ„ (2-3) çš„æ¨£æœ¬ç´°åˆ†\n",
    "    late_mask_train = y_train >= 2\n",
    "    late_mask_test = y_test_coarse_pred == 1\n",
    "    \n",
    "    if late_mask_train.sum() > 0 and late_mask_test.sum() > 0:\n",
    "        X_train_late = X_train_scaled[late_mask_train]\n",
    "        y_train_late = y_train[late_mask_train]\n",
    "        \n",
    "        clf_late = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf_late.fit(X_train_late, y_train_late)\n",
    "        \n",
    "        X_test_late = X_test_scaled[late_mask_test]\n",
    "        y_test_pred[late_mask_test] = clf_late.predict(X_test_late)\n",
    "    \n",
    "    y_true_cascade1.extend(y_test)\n",
    "    y_pred_cascade1.extend(y_test_pred)\n",
    "\n",
    "acc_cascade1 = accuracy_score(y_true_cascade1, y_pred_cascade1)\n",
    "cm_cascade1 = confusion_matrix(y_true_cascade1, y_pred_cascade1)\n",
    "stage2_recall_cascade1 = cm_cascade1[2, 2] / cm_cascade1[2, :].sum() if cm_cascade1[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… äºŒéšæ®µç´šè¯ æº–ç¢ºç‡: {acc_cascade1:.3f} ({acc_cascade1:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_cascade1:.1%} ({cm_cascade1[2, 2]}/{cm_cascade1[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_cascade1, y_pred_cascade1,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_cascade1)\n",
    "\n",
    "# ========== æ–¹æ³• 2: ä¸‰éšæ®µç´šè¯ (0 vs 1-2 vs 3) ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 2: ä¸‰éšæ®µç´šè¯\")\n",
    "print(\"   ç¬¬ä¸€å±¤: Stage 0 vs Stage 1-2 vs Stage 3\")\n",
    "print(\"   ç¬¬äºŒå±¤: Stage 1-2 å†ç´°åˆ†\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_cascade2 = []\n",
    "y_pred_cascade2 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ===== ç¬¬ä¸€å±¤ï¼šä¸‰åˆ†é¡ =====\n",
    "    # 0 â†’ 0, 1-2 â†’ 1, 3 â†’ 2\n",
    "    y_train_coarse = y_train.copy()\n",
    "    y_train_coarse[y_train == 1] = 1\n",
    "    y_train_coarse[y_train == 2] = 1\n",
    "    y_train_coarse[y_train == 3] = 2\n",
    "    \n",
    "    clf_coarse = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_coarse.fit(X_train_scaled, y_train_coarse)\n",
    "    \n",
    "    y_test_coarse_pred = clf_coarse.predict(X_test_scaled)\n",
    "    \n",
    "    # ===== ç¬¬äºŒå±¤ï¼šä¸­é–“çµ„ (1-2) ç´°åˆ† =====\n",
    "    y_test_pred = y_test_coarse_pred.copy()\n",
    "    \n",
    "    # Stage 0 å’Œ Stage 3 ç›´æ¥æ˜ å°„å›å»\n",
    "    y_test_pred[y_test_coarse_pred == 0] = 0\n",
    "    y_test_pred[y_test_coarse_pred == 2] = 3\n",
    "    \n",
    "    # Stage 1-2 éœ€è¦ç´°åˆ†\n",
    "    middle_mask_train = (y_train == 1) | (y_train == 2)\n",
    "    middle_mask_test = y_test_coarse_pred == 1\n",
    "    \n",
    "    if middle_mask_train.sum() > 0 and middle_mask_test.sum() > 0:\n",
    "        X_train_middle = X_train_scaled[middle_mask_train]\n",
    "        y_train_middle = y_train[middle_mask_train]\n",
    "        \n",
    "        clf_middle = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        clf_middle.fit(X_train_middle, y_train_middle)\n",
    "        \n",
    "        X_test_middle = X_test_scaled[middle_mask_test]\n",
    "        y_test_pred[middle_mask_test] = clf_middle.predict(X_test_middle)\n",
    "    \n",
    "    y_true_cascade2.extend(y_test)\n",
    "    y_pred_cascade2.extend(y_test_pred)\n",
    "\n",
    "acc_cascade2 = accuracy_score(y_true_cascade2, y_pred_cascade2)\n",
    "cm_cascade2 = confusion_matrix(y_true_cascade2, y_pred_cascade2)\n",
    "stage2_recall_cascade2 = cm_cascade2[2, 2] / cm_cascade2[2, :].sum() if cm_cascade2[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… ä¸‰éšæ®µç´šè¯ æº–ç¢ºç‡: {acc_cascade2:.3f} ({acc_cascade2:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_cascade2:.1%} ({cm_cascade2[2, 2]}/{cm_cascade2[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_cascade2, y_pred_cascade2,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_cascade2)\n",
    "\n",
    "# ========== æ–¹æ³• 3: æ™ºèƒ½ç´šè¯ï¼ˆåŠ æ¬Šç¬¬äºŒå±¤ï¼‰==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Œ æ–¹æ³• 3: æ™ºèƒ½ç´šè¯ï¼ˆçµåˆç½®ä¿¡åº¦ï¼‰\")\n",
    "print(\"   ç¬¬ä¸€å±¤: æ—©æœŸ vs æ™šæœŸ\")\n",
    "print(\"   ç¬¬äºŒå±¤: ä½ç½®ä¿¡åº¦æ¨£æœ¬é‡åˆ†é¡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_cascade3 = []\n",
    "y_pred_cascade3 = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_top5, y, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ===== ç¬¬ä¸€å±¤ï¼šç²—åˆ†é¡ï¼ˆå¸¶ç½®ä¿¡åº¦ï¼‰=====\n",
    "    y_train_coarse = (y_train >= 2).astype(int)\n",
    "    \n",
    "    clf_coarse = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_coarse.fit(X_train_scaled, y_train_coarse)\n",
    "    \n",
    "    y_test_coarse_pred = clf_coarse.predict(X_test_scaled)\n",
    "    y_test_coarse_proba = clf_coarse.predict_proba(X_test_scaled)\n",
    "    coarse_confidence = y_test_coarse_proba.max(axis=1)\n",
    "    \n",
    "    # ===== ç¬¬äºŒå±¤ï¼šç´°åˆ†é¡ =====\n",
    "    # è¨“ç·´å°ˆé–€çš„ç´°åˆ†é¡å™¨\n",
    "    clf_fine = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf_fine.fit(X_train_scaled, y_train)\n",
    "    y_test_fine_pred = clf_fine.predict(X_test_scaled)\n",
    "    y_test_fine_proba = clf_fine.predict_proba(X_test_scaled)\n",
    "    fine_confidence = y_test_fine_proba.max(axis=1)\n",
    "    \n",
    "    # ===== æ™ºèƒ½èåˆ =====\n",
    "    y_test_pred = np.zeros(len(y_test), dtype=int)\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if coarse_confidence[i] > 0.8:  # é«˜ç½®ä¿¡åº¦ï¼Œä½¿ç”¨ç´šè¯çµæœ\n",
    "            if y_test_coarse_pred[i] == 0:  # æ—©æœŸ\n",
    "                # å¾æ—©æœŸæ¨£æœ¬è¨“ç·´\n",
    "                early_mask = y_train < 2\n",
    "                if early_mask.sum() > 0:\n",
    "                    X_train_early = X_train_scaled[early_mask]\n",
    "                    y_train_early = y_train[early_mask]\n",
    "                    clf_early = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "                    clf_early.fit(X_train_early, y_train_early)\n",
    "                    y_test_pred[i] = clf_early.predict(X_test_scaled[i:i+1])[0]\n",
    "                else:\n",
    "                    y_test_pred[i] = y_test_fine_pred[i]\n",
    "            else:  # æ™šæœŸ\n",
    "                late_mask = y_train >= 2\n",
    "                if late_mask.sum() > 0:\n",
    "                    X_train_late = X_train_scaled[late_mask]\n",
    "                    y_train_late = y_train[late_mask]\n",
    "                    clf_late = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "                    clf_late.fit(X_train_late, y_train_late)\n",
    "                    y_test_pred[i] = clf_late.predict(X_test_scaled[i:i+1])[0]\n",
    "                else:\n",
    "                    y_test_pred[i] = y_test_fine_pred[i]\n",
    "        else:  # ä½ç½®ä¿¡åº¦ï¼Œä½¿ç”¨å…¨å±€ç´°åˆ†é¡å™¨\n",
    "            y_test_pred[i] = y_test_fine_pred[i]\n",
    "    \n",
    "    y_true_cascade3.extend(y_test)\n",
    "    y_pred_cascade3.extend(y_test_pred)\n",
    "\n",
    "acc_cascade3 = accuracy_score(y_true_cascade3, y_pred_cascade3)\n",
    "cm_cascade3 = confusion_matrix(y_true_cascade3, y_pred_cascade3)\n",
    "stage2_recall_cascade3 = cm_cascade3[2, 2] / cm_cascade3[2, :].sum() if cm_cascade3[2, :].sum() > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… æ™ºèƒ½ç´šè¯ æº–ç¢ºç‡: {acc_cascade3:.3f} ({acc_cascade3:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_cascade3:.1%} ({cm_cascade3[2, 2]}/{cm_cascade3[2, :].sum()})\")\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_cascade3, y_pred_cascade3,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 5 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('å–®ä¸€æ¨¡å‹ (RF Top-5)', 0.752, 0.147),\n",
    "    ('æ–¹æ¡ˆF (å½æ¨™ç±¤)', 0.757, 0.147),\n",
    "    ('æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)', 0.761, 0.147),\n",
    "    ('äºŒéšæ®µç´šè¯', acc_cascade1, stage2_recall_cascade1),\n",
    "    ('ä¸‰éšæ®µç´šè¯', acc_cascade2, stage2_recall_cascade2),\n",
    "    ('æ™ºèƒ½ç´šè¯', acc_cascade3, stage2_recall_cascade3)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc, s2_recall in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.761 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³\n",
    "best_idx = np.argmax([r[1] for r in results])\n",
    "best_method = results[best_idx]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_method[1]:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_method[2]:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (best_method[1] - 0.622) * 100\n",
    "improvement_vs_ensemble = (best_method[1] - 0.761) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): {improvement_vs_ensemble:+.1f}%\")\n",
    "\n",
    "if best_method[1] > 0.761:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ 2ï¼æ–°ç´€éŒ„ï¼\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_cascade = {\n",
    "    'cascade1': {'accuracy': acc_cascade1, 'y_true': y_true_cascade1, 'y_pred': y_pred_cascade1},\n",
    "    'cascade2': {'accuracy': acc_cascade2, 'y_true': y_true_cascade2, 'y_pred': y_pred_cascade2},\n",
    "    'cascade3': {'accuracy': acc_cascade3, 'y_true': y_true_cascade3, 'y_pred': y_pred_cascade3},\n",
    "    'best_method': best_method[0],\n",
    "    'best_accuracy': best_method[1]\n",
    "}\n",
    "\n",
    "with open('models/solution_cascade_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_cascade, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_cascade_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b499bd30-dd70-4a6b-b49f-ffdc2cb30e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 3: çª—å£å¤§å°å„ªåŒ–\n",
      "============================================================\n",
      "â³ é€™éœ€è¦é‡æ–°æå–ç‰¹å¾µï¼Œé è¨ˆ 20-30 åˆ†é˜...\n",
      "\n",
      "âœ… ä½¿ç”¨å·²è¼‰å…¥çš„ arduino_features å’Œ maturity_levels\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 60 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 448 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(185), np.int64(1): np.int64(163), np.int64(2): np.int64(71), np.int64(3): np.int64(29)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ3_MQ2_ratio: 0.0501\n",
      "   2. MQ135_TGS2602_ratio: 0.0493\n",
      "   3. MQ9_auc: 0.0404\n",
      "   4. TGS2602_mean: 0.0397\n",
      "   5. MQ135_mean: 0.0396\n",
      "\n",
      "âœ… çª—å£ 60ç§’ æº–ç¢ºç‡: 0.600 (60.0%)\n",
      "âœ… Stage 2 Recall: 14.1%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 90 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 298 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(124), np.int64(1): np.int64(107), np.int64(2): np.int64(48), np.int64(3): np.int64(19)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0535\n",
      "   2. MQ9_max: 0.0421\n",
      "   3. MQ135_auc: 0.0419\n",
      "   4. TGS2602_mean: 0.0418\n",
      "   5. MQ3_MQ2_ratio: 0.0406\n",
      "\n",
      "âœ… çª—å£ 90ç§’ æº–ç¢ºç‡: 0.604 (60.4%)\n",
      "âœ… Stage 2 Recall: 14.6%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 120 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 222 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(93), np.int64(1): np.int64(81), np.int64(2): np.int64(34), np.int64(3): np.int64(14)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0501\n",
      "   2. MQ135_auc: 0.0484\n",
      "   3. TGS2602_mean: 0.0421\n",
      "   4. MQ3_MQ2_ratio: 0.0397\n",
      "   5. MQ135_mean: 0.0389\n",
      "\n",
      "âœ… çª—å£ 120ç§’ æº–ç¢ºç‡: 0.631 (63.1%)\n",
      "âœ… Stage 2 Recall: 14.7%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 150 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 178 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(74), np.int64(1): np.int64(65), np.int64(2): np.int64(28), np.int64(3): np.int64(11)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. TGS2602_auc: 0.0456\n",
      "   2. MQ3_MQ2_ratio: 0.0445\n",
      "   3. MQ135_TGS2602_ratio: 0.0438\n",
      "   4. TGS2602_mean: 0.0430\n",
      "   5. MQ135_auc: 0.0404\n",
      "\n",
      "âœ… çª—å£ 150ç§’ æº–ç¢ºç‡: 0.646 (64.6%)\n",
      "âœ… Stage 2 Recall: 14.3%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: 180 ç§’\n",
      "============================================================\n",
      "â³ æå–ç‰¹å¾µä¸­...\n",
      "âœ… æå–å®Œæˆ: 148 å€‹æ¨£æœ¬, 53 å€‹ç‰¹å¾µ\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(61), np.int64(1): np.int64(56), np.int64(2): np.int64(22), np.int64(3): np.int64(9)}\n",
      "\n",
      "ğŸ” Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio: 0.0460\n",
      "   2. MQ135_auc: 0.0439\n",
      "   3. TGS2602_mean: 0.0361\n",
      "   4. MQ9_max: 0.0340\n",
      "   5. MQ3_MQ2_ratio: 0.0332\n",
      "\n",
      "âœ… çª—å£ 180ç§’ æº–ç¢ºç‡: 0.662 (66.2%)\n",
      "âœ… Stage 2 Recall: 13.6%\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 3 ç¸½çµ\n",
      "============================================================\n",
      "\n",
      "çª—å£å¤§å°         æ¨£æœ¬æ•¸        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-------------------------------------------------------\n",
      " 60ç§’      448        60.0%        14.1%\n",
      " 90ç§’      298        60.4%        14.6%\n",
      " 120ç§’      222        63.1%        14.7%\n",
      " 150ç§’      178        64.6%        14.3%\n",
      " 180ç§’      148        66.2%        13.6%\n",
      "\n",
      "============================================================\n",
      "ğŸ† æœ€ä½³çª—å£å¤§å°: 180 ç§’\n",
      "   æº–ç¢ºç‡: 66.2%\n",
      "   Stage 2 Recall: 13.6%\n",
      "   æ¨£æœ¬æ•¸: 148\n",
      "\n",
      "   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +4.0%\n",
      "   ç›¸æ¯” 120ç§’çª—å£: +3.2%\n",
      "   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): -9.9%\n",
      "\n",
      "ğŸ” æœ€ä½³çª—å£çš„ Top-5 ç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio\n",
      "   2. MQ135_auc\n",
      "   3. TGS2602_mean\n",
      "   4. MQ9_max\n",
      "   5. MQ3_MQ2_ratio\n",
      "\n",
      "åˆ†é¡å ±å‘Š (180ç§’çª—å£):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.714     0.738     0.726        61\n",
      "     Stage 1      0.616     0.804     0.698        56\n",
      "     Stage 2      0.429     0.136     0.207        22\n",
      "     Stage 3      1.000     0.556     0.714         9\n",
      "\n",
      "    accuracy                          0.662       148\n",
      "   macro avg      0.690     0.558     0.586       148\n",
      "weighted avg      0.652     0.662     0.637       148\n",
      "\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_window_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== æ–¹æ¡ˆ 3: çª—å£å¤§å°å„ªåŒ– =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 3: çª—å£å¤§å°å„ªåŒ–\")\n",
    "print(\"=\"*60)\n",
    "print(\"â³ é€™éœ€è¦é‡æ–°æå–ç‰¹å¾µï¼Œé è¨ˆ 20-30 åˆ†é˜...\")\n",
    "\n",
    "# è¼‰å…¥åŸå§‹æ™‚é–“åºåˆ—æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    original_data = pickle.load(f)\n",
    "\n",
    "# éœ€è¦é‡æ–°è¼‰å…¥ arduino_features å’Œ maturity_levels\n",
    "# æª¢æŸ¥æ˜¯å¦å·²ç¶“è¼‰å…¥\n",
    "try:\n",
    "    arduino_features\n",
    "    maturity_levels\n",
    "    print(\"\\nâœ… ä½¿ç”¨å·²è¼‰å…¥çš„ arduino_features å’Œ maturity_levels\")\n",
    "except NameError:\n",
    "    print(\"\\nâš ï¸  éœ€è¦é‡æ–°è¼‰å…¥åŸå§‹æ•¸æ“š...\")\n",
    "    print(\"è«‹ç¢ºä¿å·²ç¶“åŸ·è¡Œé Cell 13 (è¼‰å…¥ arduino_features)\")\n",
    "    raise\n",
    "\n",
    "# ========== ç‰¹å¾µæå–å‡½æ•¸ï¼ˆä¸åŒçª—å£ï¼‰==========\n",
    "def extract_features_with_window(arduino_features, maturity_levels, window_size=120):\n",
    "    \"\"\"\n",
    "    ç”¨æŒ‡å®šçª—å£å¤§å°æå–ç‰¹å¾µ\n",
    "    \"\"\"\n",
    "    sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_pineapple_ids = []\n",
    "    \n",
    "    for pid in arduino_features.keys():\n",
    "        if pid not in maturity_levels:\n",
    "            continue\n",
    "        \n",
    "        # åˆä½µè©²é³³æ¢¨æ‰€æœ‰æ—¥æœŸçš„æ•¸æ“š\n",
    "        date_dict = arduino_features[pid]\n",
    "        pine_labels = maturity_levels[pid]\n",
    "        \n",
    "        all_data = []\n",
    "        offset = 0\n",
    "        \n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date]\n",
    "            all_data.append(df)\n",
    "            offset += len(df)\n",
    "        \n",
    "        if not all_data:\n",
    "            continue\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # æ»‘å‹•çª—å£æå–\n",
    "        n_samples = len(combined_df)\n",
    "        \n",
    "        for start_idx in range(0, n_samples - window_size + 1, window_size):\n",
    "            end_idx = start_idx + window_size\n",
    "            \n",
    "            window_df = combined_df.iloc[start_idx:end_idx]\n",
    "            label_window = pine_labels[start_idx:end_idx]\n",
    "            \n",
    "            # å–çœ¾æ•¸æ¨™ç±¤\n",
    "            unique, counts = np.unique(label_window, return_counts=True)\n",
    "            majority_label = unique[np.argmax(counts)]\n",
    "            \n",
    "            # æå–ç‰¹å¾µ\n",
    "            feature_vector = []\n",
    "            \n",
    "            for sensor in sensor_cols:\n",
    "                col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "                \n",
    "                if col_rs_r0 in window_df.columns:\n",
    "                    data = window_df[col_rs_r0].values\n",
    "                    \n",
    "                    # çµ±è¨ˆç‰¹å¾µ\n",
    "                    feature_vector.append(np.mean(data))\n",
    "                    feature_vector.append(np.std(data))\n",
    "                    feature_vector.append(np.min(data))\n",
    "                    feature_vector.append(np.max(data))\n",
    "                    feature_vector.append(np.max(data) - np.min(data))\n",
    "                    \n",
    "                    # æ–œç‡\n",
    "                    if len(data) > 1:\n",
    "                        x = np.arange(len(data))\n",
    "                        slope, _, _, _, _ = linregress(x, data)\n",
    "                        feature_vector.append(slope)\n",
    "                    else:\n",
    "                        feature_vector.append(0)\n",
    "                    \n",
    "                    # AUC\n",
    "                    try:\n",
    "                        auc = np.trapezoid(data, dx=1)\n",
    "                    except AttributeError:\n",
    "                        if len(data) > 1:\n",
    "                            auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                        else:\n",
    "                            auc = data[0] if len(data) > 0 else 0\n",
    "                    feature_vector.append(auc)\n",
    "                else:\n",
    "                    feature_vector.extend([0] * 7)\n",
    "                \n",
    "                # Delta ç‰¹å¾µ\n",
    "                col_delta = f'{sensor}_delta_Rs_R0'\n",
    "                if col_delta in window_df.columns:\n",
    "                    data = window_df[col_delta].values\n",
    "                    feature_vector.append(np.mean(data))\n",
    "                    feature_vector.append(np.std(data))\n",
    "                    feature_vector.append(np.max(np.abs(data)))\n",
    "                else:\n",
    "                    feature_vector.extend([0] * 3)\n",
    "            \n",
    "            # è·¨æ„Ÿæ¸¬å™¨ç‰¹å¾µ\n",
    "            mq3 = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0\n",
    "            mq2 = window_df['MQ2_Rs_R0'].mean() if 'MQ2_Rs_R0' in window_df.columns else 0\n",
    "            feature_vector.append(mq3 / (mq2 + 1e-6))\n",
    "            \n",
    "            mq135 = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0\n",
    "            tgs = window_df['TGS2602_Rs_R0'].mean() if 'TGS2602_Rs_R0' in window_df.columns else 0\n",
    "            feature_vector.append(mq135 / (tgs + 1e-6))\n",
    "            \n",
    "            all_means = [window_df[f'{s}_Rs_R0'].mean() \n",
    "                        for s in sensor_cols \n",
    "                        if f'{s}_Rs_R0' in window_df.columns]\n",
    "            feature_vector.append(np.mean(all_means) if all_means else 0)\n",
    "            \n",
    "            all_features.append(feature_vector)\n",
    "            all_labels.append(majority_label)\n",
    "            all_pineapple_ids.append(pid)\n",
    "    \n",
    "    return np.array(all_features), np.array(all_labels), np.array(all_pineapple_ids)\n",
    "\n",
    "# ========== æ¸¬è©¦ä¸åŒçª—å£å¤§å° ==========\n",
    "window_sizes = [60, 90, 120, 150, 180]\n",
    "results_by_window = {}\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š æ¸¬è©¦çª—å£å¤§å°: {window_size} ç§’\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # æå–ç‰¹å¾µ\n",
    "    print(f\"â³ æå–ç‰¹å¾µä¸­...\")\n",
    "    X_window, y_window, pineapple_ids_window = extract_features_with_window(\n",
    "        arduino_features, maturity_levels, window_size=window_size\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æå–å®Œæˆ: {X_window.shape[0]} å€‹æ¨£æœ¬, {X_window.shape[1]} å€‹ç‰¹å¾µ\")\n",
    "    print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(y_window, return_counts=True)))}\")\n",
    "    \n",
    "    # æ‰¾å‡º Top-5 ç‰¹å¾µï¼ˆä½¿ç”¨å…¨æ•¸æ“šï¼‰\n",
    "    feature_names = [\n",
    "        f'{sensor}_{feat}' \n",
    "        for sensor in ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "        for feat in ['mean', 'std', 'min', 'max', 'range', 'slope', 'auc', \n",
    "                     'delta_mean', 'delta_std', 'delta_max_abs']\n",
    "    ] + ['MQ3_MQ2_ratio', 'MQ135_TGS2602_ratio', 'all_sensors_mean']\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_temp.fit(StandardScaler().fit_transform(X_window), y_window)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names[:X_window.shape[1]],\n",
    "        'importance': rf_temp.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    top_5_indices = feature_importance.head(5).index.tolist()\n",
    "    X_top5 = X_window[:, top_5_indices]\n",
    "    \n",
    "    print(f\"\\nğŸ” Top-5 ç‰¹å¾µ:\")\n",
    "    for i, (idx, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "        print(f\"   {i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # LOSO æ¸¬è©¦\n",
    "    logo = LeaveOneGroupOut()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for train_idx, test_idx in logo.split(X_top5, y_window, groups=pineapple_ids_window):\n",
    "        X_train, X_test = X_top5[train_idx], X_top5[test_idx]\n",
    "        y_train, y_test = y_window[train_idx], y_window[test_idx]\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred.extend(rf.predict(X_test_scaled))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    stage2_recall = cm[2, 2] / cm[2, :].sum() if cm.shape[0] > 2 and cm[2, :].sum() > 0 else 0\n",
    "    \n",
    "    print(f\"\\nâœ… çª—å£ {window_size}ç§’ æº–ç¢ºç‡: {acc:.3f} ({acc:.1%})\")\n",
    "    print(f\"âœ… Stage 2 Recall: {stage2_recall:.1%}\")\n",
    "    \n",
    "    results_by_window[window_size] = {\n",
    "        'accuracy': acc,\n",
    "        'stage2_recall': stage2_recall,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'n_samples': len(y_true),\n",
    "        'top5_features': feature_importance.head(5)['feature'].tolist()\n",
    "    }\n",
    "\n",
    "# ========== ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 3 ç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'çª—å£å¤§å°':<12} {'æ¨£æœ¬æ•¸':<10} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "baseline_acc = 0.761  # æ–¹æ¡ˆ2çš„çµæœ\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    result = results_by_window[window_size]\n",
    "    acc = result['accuracy']\n",
    "    s2_recall = result['stage2_recall']\n",
    "    n_samples = result['n_samples']\n",
    "    \n",
    "    symbol = \"ğŸ”¥\" if acc > baseline_acc else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {window_size}ç§’      {n_samples:<10} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³çª—å£\n",
    "best_window = max(results_by_window.keys(), key=lambda k: results_by_window[k]['accuracy'])\n",
    "best_result = results_by_window[best_window]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ† æœ€ä½³çª—å£å¤§å°: {best_window} ç§’\")\n",
    "print(f\"   æº–ç¢ºç‡: {best_result['accuracy']:.1%}\")\n",
    "print(f\"   Stage 2 Recall: {best_result['stage2_recall']:.1%}\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {best_result['n_samples']}\")\n",
    "\n",
    "improvement_vs_baseline = (best_result['accuracy'] - 0.622) * 100\n",
    "improvement_vs_120 = (best_result['accuracy'] - results_by_window[120]['accuracy']) * 100\n",
    "improvement_vs_ensemble = (best_result['accuracy'] - baseline_acc) * 100\n",
    "\n",
    "print(f\"\\n   ç›¸æ¯”åŸå§‹ Baseline (62.2%): +{improvement_vs_baseline:.1f}%\")\n",
    "print(f\"   ç›¸æ¯” 120ç§’çª—å£: {improvement_vs_120:+.1f}%\")\n",
    "print(f\"   ç›¸æ¯”æ–¹æ¡ˆ2 (76.1%): {improvement_vs_ensemble:+.1f}%\")\n",
    "\n",
    "if best_result['accuracy'] > baseline_acc:\n",
    "    print(f\"\\n   ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ 2ï¼æ–°ç´€éŒ„ï¼\")\n",
    "\n",
    "print(f\"\\nğŸ” æœ€ä½³çª—å£çš„ Top-5 ç‰¹å¾µ:\")\n",
    "for i, feat in enumerate(best_result['top5_features'], 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "# è©³ç´°å ±å‘Š\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š ({best_window}ç§’çª—å£):\")\n",
    "print(classification_report(best_result['y_true'], best_result['y_pred'],\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "with open('models/solution_window_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_by_window, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_window_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "038ca6d0-0ae2-4538-ab44-7f508c9a22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ’¾ ä¿å­˜æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨) æœ€çµ‚æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š ç‰¹å¾µé›†:\n",
      "   Top-5:  ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean']\n",
      "   Top-10: ['MQ135_TGS2602_ratio', 'MQ135_auc', 'MQ135_mean', 'MQ9_auc', 'TGS2602_mean', 'MQ9_mean', 'MQ3_min', 'MQ3_MQ2_ratio', 'MQ9_max', 'MQ3_delta_mean']\n",
      "\n",
      "ğŸ”¥ è¨“ç·´æœ€çµ‚æ¨¡å‹...\n",
      "   ä½¿ç”¨å…¨éƒ¨ 222 å€‹æ¨£æœ¬\n",
      "âœ… æ¨¡å‹1 (RF Top-5) è¨“ç·´å®Œæˆ\n",
      "âœ… æ¨¡å‹2 (RF Top-10) è¨“ç·´å®Œæˆ\n",
      "âœ… æ¨¡å‹3 (XGBoost Top-5) è¨“ç·´å®Œæˆ\n",
      "\n",
      "ğŸ” é©—è­‰æœ€çµ‚æ¨¡å‹...\n",
      "âœ… è¨“ç·´é›†æº–ç¢ºç‡: 1.000 (100.0%)\n",
      "   Stage 0: 100.0% (93 å€‹æ¨£æœ¬)\n",
      "   Stage 1: 100.0% (81 å€‹æ¨£æœ¬)\n",
      "   Stage 2: 100.0% (34 å€‹æ¨£æœ¬)\n",
      "   Stage 3: 100.0% (14 å€‹æ¨£æœ¬)\n",
      "\n",
      "ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œé…ç½®...\n",
      "âœ… ä¸»æ¨¡å‹å·²ä¿å­˜: models/final_ensemble_model.pkl\n",
      "âœ… ä½¿ç”¨èªªæ˜å·²ä¿å­˜: models/MODEL_USAGE.md\n",
      "âœ… è¼•é‡ç‰ˆæ¨¡å‹å·²ä¿å­˜: models/ensemble_model_lightweight.pkl\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ æ¨¡å‹ä¿å­˜å®Œæˆï¼\n",
      "============================================================\n",
      "\n",
      "ğŸ“¦ å·²ä¿å­˜æ–‡ä»¶:\n",
      "   1. models/final_ensemble_model.pkl (å®Œæ•´æ¨¡å‹åŒ…)\n",
      "   2. models/ensemble_model_lightweight.pkl (è¼•é‡ç‰ˆ)\n",
      "   3. models/MODEL_USAGE.md (ä½¿ç”¨èªªæ˜)\n",
      "\n",
      "ğŸ“Š æ¨¡å‹æ€§èƒ½:\n",
      "   LOSOæº–ç¢ºç‡: 76.1%\n",
      "   è¨“ç·´é›†æº–ç¢ºç‡: 100.0%\n",
      "   ç›¸æ¯”åŸå§‹Baselineæå‡: 13.9%\n",
      "\n",
      "ğŸ”‘ Top-5 é—œéµç‰¹å¾µ:\n",
      "   1. MQ135_TGS2602_ratio\n",
      "   2. MQ135_auc\n",
      "   3. MQ135_mean\n",
      "   4. MQ9_auc\n",
      "   5. TGS2602_mean\n",
      "\n",
      "âœ… å¯ä»¥ç›´æ¥ç”¨æ–¼æ–°é³³æ¢¨é æ¸¬ï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== ä¿å­˜æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨) æœ€çµ‚æ¨¡å‹ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ’¾ ä¿å­˜æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨) æœ€çµ‚æ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "with open('data/processed/feature_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['features'].values\n",
    "y = data['labels']\n",
    "metadata = data['metadata']\n",
    "pineapple_ids = metadata['pineapple_id'].values\n",
    "\n",
    "# è¼‰å…¥ç‰¹å¾µé‡è¦æ€§\n",
    "with open('models/solution_j_results.pkl', 'rb') as f:\n",
    "    j_results = pickle.load(f)\n",
    "    feature_importance = j_results['feature_importance']\n",
    "\n",
    "# æº–å‚™ä¸åŒç‰¹å¾µé›†\n",
    "top_5_features = feature_importance.head(5)['feature'].tolist()\n",
    "top_10_features = feature_importance.head(10)['feature'].tolist()\n",
    "\n",
    "top_5_indices = [data['features'].columns.tolist().index(f) for f in top_5_features]\n",
    "top_10_indices = [data['features'].columns.tolist().index(f) for f in top_10_features]\n",
    "\n",
    "X_top5 = X[:, top_5_indices]\n",
    "X_top10 = X[:, top_10_indices]\n",
    "\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾µé›†:\")\n",
    "print(f\"   Top-5:  {top_5_features}\")\n",
    "print(f\"   Top-10: {top_10_features}\")\n",
    "\n",
    "# ========== è¨“ç·´æœ€çµ‚æ¨¡å‹ï¼ˆç”¨å…¨éƒ¨æ•¸æ“šï¼‰==========\n",
    "print(f\"\\nğŸ”¥ è¨“ç·´æœ€çµ‚æ¨¡å‹...\")\n",
    "print(f\"   ä½¿ç”¨å…¨éƒ¨ {len(X)} å€‹æ¨£æœ¬\")\n",
    "\n",
    "# æ¨¡å‹ 1: RF with Top-5\n",
    "scaler_top5 = StandardScaler()\n",
    "X_top5_scaled = scaler_top5.fit_transform(X_top5)\n",
    "\n",
    "rf_top5 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_top5.fit(X_top5_scaled, y)\n",
    "print(f\"âœ… æ¨¡å‹1 (RF Top-5) è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# æ¨¡å‹ 2: RF with Top-10\n",
    "scaler_top10 = StandardScaler()\n",
    "X_top10_scaled = scaler_top10.fit_transform(X_top10)\n",
    "\n",
    "rf_top10 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_top10.fit(X_top10_scaled, y)\n",
    "print(f\"âœ… æ¨¡å‹2 (RF Top-10) è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# æ¨¡å‹ 3: XGBoost with Top-5\n",
    "xgb_top5 = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_top5.fit(X_top5_scaled, y)\n",
    "print(f\"âœ… æ¨¡å‹3 (XGBoost Top-5) è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# ========== é©—è­‰æ¨¡å‹ ==========\n",
    "print(f\"\\nğŸ” é©—è­‰æœ€çµ‚æ¨¡å‹...\")\n",
    "\n",
    "# å‰µå»ºé›†æˆé æ¸¬\n",
    "from collections import Counter\n",
    "\n",
    "def ensemble_predict(X_input):\n",
    "    X_top5_input = X_input[:, top_5_indices]\n",
    "    X_top10_input = X_input[:, top_10_indices]\n",
    "    \n",
    "    X_top5_scaled = scaler_top5.transform(X_top5_input)\n",
    "    X_top10_scaled = scaler_top10.transform(X_top10_input)\n",
    "    \n",
    "    pred1 = rf_top5.predict(X_top5_scaled)\n",
    "    pred2 = rf_top10.predict(X_top10_scaled)\n",
    "    pred3 = xgb_top5.predict(X_top5_scaled)\n",
    "    \n",
    "    predictions = np.array([pred1, pred2, pred3]).T\n",
    "    \n",
    "    ensemble_pred = []\n",
    "    for sample_preds in predictions:\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        ensemble_pred.append(majority_vote)\n",
    "    \n",
    "    return np.array(ensemble_pred)\n",
    "\n",
    "y_pred_final = ensemble_predict(X)\n",
    "train_accuracy = (y_pred_final == y).mean()\n",
    "\n",
    "print(f\"âœ… è¨“ç·´é›†æº–ç¢ºç‡: {train_accuracy:.3f} ({train_accuracy:.1%})\")\n",
    "\n",
    "# å„é¡åˆ¥æº–ç¢ºç‡\n",
    "for stage in range(4):\n",
    "    stage_mask = y == stage\n",
    "    stage_acc = (y_pred_final[stage_mask] == y[stage_mask]).mean()\n",
    "    print(f\"   Stage {stage}: {stage_acc:.1%} ({stage_mask.sum()} å€‹æ¨£æœ¬)\")\n",
    "\n",
    "# ========== ä¿å­˜æ¨¡å‹å’Œé…ç½® ==========\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜æ¨¡å‹å’Œé…ç½®...\")\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹åŒ…\n",
    "model_package = {\n",
    "    # æ¨¡å‹\n",
    "    'rf_top5': rf_top5,\n",
    "    'rf_top10': rf_top10,\n",
    "    'xgb_top5': xgb_top5,\n",
    "    \n",
    "    # æ¨™æº–åŒ–å™¨\n",
    "    'scaler_top5': scaler_top5,\n",
    "    'scaler_top10': scaler_top10,\n",
    "    \n",
    "    # ç‰¹å¾µé…ç½®\n",
    "    'top_5_features': top_5_features,\n",
    "    'top_10_features': top_10_features,\n",
    "    'top_5_indices': top_5_indices,\n",
    "    'top_10_indices': top_10_indices,\n",
    "    'all_feature_names': data['features'].columns.tolist(),\n",
    "    \n",
    "    # æ¨¡å‹ä¿¡æ¯\n",
    "    'model_name': 'æ··åˆæŠ•ç¥¨é›†æˆ (RF Top-5 + RF Top-10 + XGBoost Top-5)',\n",
    "    'loso_accuracy': 0.761,\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_training_samples': len(X),\n",
    "    'label_distribution': dict(zip(*np.unique(y, return_counts=True))),\n",
    "    \n",
    "    # æ€§èƒ½å ±å‘Š\n",
    "    'performance': {\n",
    "        'loso_accuracy': 0.761,\n",
    "        'improvement_vs_baseline': 0.139,\n",
    "        'stage_0_recall': 0.914,\n",
    "        'stage_1_recall': 0.815,\n",
    "        'stage_2_recall': 0.147,\n",
    "        'stage_3_recall': 0.929\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜ä¸»æ¨¡å‹\n",
    "with open('models/final_ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(f\"âœ… ä¸»æ¨¡å‹å·²ä¿å­˜: models/final_ensemble_model.pkl\")\n",
    "\n",
    "# ========== å‰µå»ºä½¿ç”¨èªªæ˜ ==========\n",
    "usage_text = \"# æ–¹æ¡ˆ2 æ··åˆæŠ•ç¥¨æ¨¡å‹ä½¿ç”¨èªªæ˜\\n\\n\"\n",
    "usage_text += \"## æ¨¡å‹ä¿¡æ¯\\n\"\n",
    "usage_text += f\"- æ¨¡å‹åç¨±: æ··åˆæŠ•ç¥¨é›†æˆ\\n\"\n",
    "usage_text += f\"- LOSOæº–ç¢ºç‡: 76.1%\\n\"\n",
    "usage_text += f\"- è¨“ç·´æ—¥æœŸ: {model_package['training_date']}\\n\"\n",
    "usage_text += f\"- è¨“ç·´æ¨£æœ¬æ•¸: {model_package['n_training_samples']}\\n\\n\"\n",
    "\n",
    "usage_text += \"## è¼‰å…¥æ¨¡å‹\\n\\n\"\n",
    "usage_text += \"```python\\n\"\n",
    "usage_text += \"import pickle\\n\"\n",
    "usage_text += \"with open('models/final_ensemble_model.pkl', 'rb') as f:\\n\"\n",
    "usage_text += \"    model = pickle.load(f)\\n\"\n",
    "usage_text += \"```\\n\\n\"\n",
    "\n",
    "usage_text += \"## Top-5 é—œéµç‰¹å¾µ\\n\"\n",
    "for i, feat in enumerate(top_5_features, 1):\n",
    "    usage_text += f\"{i}. {feat}\\n\"\n",
    "\n",
    "usage_text += \"\\n## æ€§èƒ½æŒ‡æ¨™\\n\"\n",
    "usage_text += f\"- LOSOæº–ç¢ºç‡: 76.1%\\n\"\n",
    "usage_text += f\"- Stage 0 Recall: 91.4%\\n\"\n",
    "usage_text += f\"- Stage 1 Recall: 81.5%\\n\"\n",
    "usage_text += f\"- Stage 2 Recall: 14.7%\\n\"\n",
    "usage_text += f\"- Stage 3 Recall: 92.9%\\n\"\n",
    "\n",
    "# ä¿å­˜ä½¿ç”¨èªªæ˜\n",
    "with open('models/MODEL_USAGE.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(usage_text)\n",
    "\n",
    "print(f\"âœ… ä½¿ç”¨èªªæ˜å·²ä¿å­˜: models/MODEL_USAGE.md\")\n",
    "\n",
    "# ========== ä¿å­˜è¼•é‡ç‰ˆ ==========\n",
    "lightweight_model = {\n",
    "    'models': {\n",
    "        'rf_top5': rf_top5,\n",
    "        'rf_top10': rf_top10,\n",
    "        'xgb_top5': xgb_top5\n",
    "    },\n",
    "    'scalers': {\n",
    "        'scaler_top5': scaler_top5,\n",
    "        'scaler_top10': scaler_top10\n",
    "    },\n",
    "    'config': {\n",
    "        'top_5_indices': top_5_indices,\n",
    "        'top_10_indices': top_10_indices\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/ensemble_model_lightweight.pkl', 'wb') as f:\n",
    "    pickle.dump(lightweight_model, f)\n",
    "\n",
    "print(f\"âœ… è¼•é‡ç‰ˆæ¨¡å‹å·²ä¿å­˜: models/ensemble_model_lightweight.pkl\")\n",
    "\n",
    "# ========== ç¸½çµ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ æ¨¡å‹ä¿å­˜å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“¦ å·²ä¿å­˜æ–‡ä»¶:\")\n",
    "print(f\"   1. models/final_ensemble_model.pkl (å®Œæ•´æ¨¡å‹åŒ…)\")\n",
    "print(f\"   2. models/ensemble_model_lightweight.pkl (è¼•é‡ç‰ˆ)\")\n",
    "print(f\"   3. models/MODEL_USAGE.md (ä½¿ç”¨èªªæ˜)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹æ€§èƒ½:\")\n",
    "print(f\"   LOSOæº–ç¢ºç‡: 76.1%\")\n",
    "print(f\"   è¨“ç·´é›†æº–ç¢ºç‡: {train_accuracy:.1%}\")\n",
    "print(f\"   ç›¸æ¯”åŸå§‹Baselineæå‡: 13.9%\")\n",
    "\n",
    "print(f\"\\nğŸ”‘ Top-5 é—œéµç‰¹å¾µ:\")\n",
    "for i, feat in enumerate(top_5_features, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "print(f\"\\nâœ… å¯ä»¥ç›´æ¥ç”¨æ–¼æ–°é³³æ¢¨é æ¸¬ï¼\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "335d8e69-e23f-44b5-94fb-e2daa6bdf490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨\n",
      "============================================================\n",
      "â³ é è¨ˆéœ€è¦ 20-30 åˆ†é˜...\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ1: ç”¨60ç§’çª—å£æå–ç‰¹å¾µ...\n",
      "âœ… 60ç§’çª—å£ç‰¹å¾µæå–å®Œæˆ:\n",
      "   æ¨£æœ¬æ•¸: 448\n",
      "   ç‰¹å¾µæ•¸: 53\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(185), np.int64(1): np.int64(163), np.int64(2): np.int64(71), np.int64(3): np.int64(29)}\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ2: ç‰¹å¾µé¸æ“‡...\n",
      "\n",
      "ğŸ” 60ç§’çª—å£ Top-5 ç‰¹å¾µ:\n",
      "   1. MQ3_MQ2_ratio: 0.0501\n",
      "   2. MQ135_TGS2602_ratio: 0.0493\n",
      "   3. MQ9_auc: 0.0404\n",
      "   4. TGS2602_mean: 0.0397\n",
      "   5. MQ135_mean: 0.0396\n",
      "\n",
      "ğŸ“Š æ­¥é©Ÿ3: LOSOæ··åˆæŠ•ç¥¨è¨“ç·´...\n",
      "   Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "   Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "   Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "   Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "   Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "   Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "   Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "   Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨ çµæœ\n",
      "============================================================\n",
      "\n",
      "âœ… 60ç§’çª—å£ æº–ç¢ºç‡: 0.665 (66.5%)\n",
      "âœ… Stage 2 Recall: 14.1% (10/71)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.720     0.778     0.748       185\n",
      "     Stage 1      0.604     0.804     0.689       163\n",
      "     Stage 2      0.556     0.141     0.225        71\n",
      "     Stage 3      1.000     0.448     0.619        29\n",
      "\n",
      "    accuracy                          0.665       448\n",
      "   macro avg      0.720     0.543     0.570       448\n",
      "weighted avg      0.670     0.665     0.635       448\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[144  40   1   0]\n",
      " [ 27 131   5   0]\n",
      " [ 29  32  10   0]\n",
      " [  0  14   2  13]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š çª—å£å¤§å°æ¯”è¼ƒ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æ¨£æœ¬æ•¸        æº–ç¢ºç‡          Stage2 Recall  \n",
      "-----------------------------------------------------------------\n",
      "âœ… 120ç§’çª—å£ + æ··åˆæŠ•ç¥¨           222        76.1%        14.7%\n",
      " 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨            448        66.5%        14.1%\n",
      "\n",
      "ç›¸æ¯”120ç§’çª—å£: -9.6%\n",
      "\n",
      "ğŸ’¡ 120ç§’çª—å£ä¾ç„¶æ›´å¥½\n",
      "å»ºè­°ä¿æŒä½¿ç”¨120ç§’çª—å£\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_60s_ensemble_results.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨ =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨\")\n",
    "print(\"=\"*60)\n",
    "print(\"â³ é è¨ˆéœ€è¦ 20-30 åˆ†é˜...\")\n",
    "\n",
    "# ========== 1. ç”¨60ç§’çª—å£é‡æ–°æå–ç‰¹å¾µ ==========\n",
    "print(\"\\nğŸ“Š æ­¥é©Ÿ1: ç”¨60ç§’çª—å£æå–ç‰¹å¾µ...\")\n",
    "\n",
    "def extract_features_60s(arduino_features, maturity_levels, window_size=60):\n",
    "    \"\"\"\n",
    "    ç”¨60ç§’çª—å£æå–ç‰¹å¾µ\n",
    "    \"\"\"\n",
    "    sensor_cols = ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_pineapple_ids = []\n",
    "    \n",
    "    for pid in arduino_features.keys():\n",
    "        if pid not in maturity_levels:\n",
    "            continue\n",
    "        \n",
    "        date_dict = arduino_features[pid]\n",
    "        pine_labels = maturity_levels[pid]\n",
    "        \n",
    "        all_data = []\n",
    "        offset = 0\n",
    "        \n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date]\n",
    "            all_data.append(df)\n",
    "            offset += len(df)\n",
    "        \n",
    "        if not all_data:\n",
    "            continue\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        n_samples = len(combined_df)\n",
    "        \n",
    "        for start_idx in range(0, n_samples - window_size + 1, window_size):\n",
    "            end_idx = start_idx + window_size\n",
    "            \n",
    "            window_df = combined_df.iloc[start_idx:end_idx]\n",
    "            label_window = pine_labels[start_idx:end_idx]\n",
    "            \n",
    "            unique, counts = np.unique(label_window, return_counts=True)\n",
    "            majority_label = unique[np.argmax(counts)]\n",
    "            \n",
    "            feature_vector = []\n",
    "            \n",
    "            for sensor in sensor_cols:\n",
    "                col_rs_r0 = f'{sensor}_Rs_R0'\n",
    "                \n",
    "                if col_rs_r0 in window_df.columns:\n",
    "                    data = window_df[col_rs_r0].values\n",
    "                    \n",
    "                    feature_vector.append(np.mean(data))\n",
    "                    feature_vector.append(np.std(data))\n",
    "                    feature_vector.append(np.min(data))\n",
    "                    feature_vector.append(np.max(data))\n",
    "                    feature_vector.append(np.max(data) - np.min(data))\n",
    "                    \n",
    "                    if len(data) > 1:\n",
    "                        x = np.arange(len(data))\n",
    "                        slope, _, _, _, _ = linregress(x, data)\n",
    "                        feature_vector.append(slope)\n",
    "                    else:\n",
    "                        feature_vector.append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        auc = np.trapezoid(data, dx=1)\n",
    "                    except AttributeError:\n",
    "                        if len(data) > 1:\n",
    "                            auc = np.sum((data[:-1] + data[1:]) / 2)\n",
    "                        else:\n",
    "                            auc = data[0] if len(data) > 0 else 0\n",
    "                    feature_vector.append(auc)\n",
    "                else:\n",
    "                    feature_vector.extend([0] * 7)\n",
    "                \n",
    "                col_delta = f'{sensor}_delta_Rs_R0'\n",
    "                if col_delta in window_df.columns:\n",
    "                    data = window_df[col_delta].values\n",
    "                    feature_vector.append(np.mean(data))\n",
    "                    feature_vector.append(np.std(data))\n",
    "                    feature_vector.append(np.max(np.abs(data)))\n",
    "                else:\n",
    "                    feature_vector.extend([0] * 3)\n",
    "            \n",
    "            mq3 = window_df['MQ3_Rs_R0'].mean() if 'MQ3_Rs_R0' in window_df.columns else 0\n",
    "            mq2 = window_df['MQ2_Rs_R0'].mean() if 'MQ2_Rs_R0' in window_df.columns else 0\n",
    "            feature_vector.append(mq3 / (mq2 + 1e-6))\n",
    "            \n",
    "            mq135 = window_df['MQ135_Rs_R0'].mean() if 'MQ135_Rs_R0' in window_df.columns else 0\n",
    "            tgs = window_df['TGS2602_Rs_R0'].mean() if 'TGS2602_Rs_R0' in window_df.columns else 0\n",
    "            feature_vector.append(mq135 / (tgs + 1e-6))\n",
    "            \n",
    "            all_means = [window_df[f'{s}_Rs_R0'].mean() \n",
    "                        for s in sensor_cols \n",
    "                        if f'{s}_Rs_R0' in window_df.columns]\n",
    "            feature_vector.append(np.mean(all_means) if all_means else 0)\n",
    "            \n",
    "            all_features.append(feature_vector)\n",
    "            all_labels.append(majority_label)\n",
    "            all_pineapple_ids.append(pid)\n",
    "    \n",
    "    return np.array(all_features), np.array(all_labels), np.array(all_pineapple_ids)\n",
    "\n",
    "# æå–ç‰¹å¾µ\n",
    "X_60s, y_60s, pineapple_ids_60s = extract_features_60s(\n",
    "    arduino_features, maturity_levels, window_size=60\n",
    ")\n",
    "\n",
    "print(f\"âœ… 60ç§’çª—å£ç‰¹å¾µæå–å®Œæˆ:\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {X_60s.shape[0]}\")\n",
    "print(f\"   ç‰¹å¾µæ•¸: {X_60s.shape[1]}\")\n",
    "print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(y_60s, return_counts=True)))}\")\n",
    "\n",
    "# ========== 2. æ‰¾å‡ºTop-5å’ŒTop-10ç‰¹å¾µ ==========\n",
    "print(\"\\nğŸ“Š æ­¥é©Ÿ2: ç‰¹å¾µé¸æ“‡...\")\n",
    "\n",
    "feature_names = [\n",
    "    f'{sensor}_{feat}' \n",
    "    for sensor in ['MQ2', 'MQ3', 'MQ9', 'MQ135', 'TGS2602']\n",
    "    for feat in ['mean', 'std', 'min', 'max', 'range', 'slope', 'auc', \n",
    "                 'delta_mean', 'delta_std', 'delta_max_abs']\n",
    "] + ['MQ3_MQ2_ratio', 'MQ135_TGS2602_ratio', 'all_sensors_mean']\n",
    "\n",
    "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_temp.fit(StandardScaler().fit_transform(X_60s), y_60s)\n",
    "\n",
    "feature_importance_60s = pd.DataFrame({\n",
    "    'feature': feature_names[:X_60s.shape[1]],\n",
    "    'importance': rf_temp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_5_features_60s = feature_importance_60s.head(5)['feature'].tolist()\n",
    "top_10_features_60s = feature_importance_60s.head(10)['feature'].tolist()\n",
    "\n",
    "top_5_indices_60s = feature_importance_60s.head(5).index.tolist()\n",
    "top_10_indices_60s = feature_importance_60s.head(10).index.tolist()\n",
    "\n",
    "print(f\"\\nğŸ” 60ç§’çª—å£ Top-5 ç‰¹å¾µ:\")\n",
    "for i, (idx, row) in enumerate(feature_importance_60s.head(5).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "X_top5_60s = X_60s[:, top_5_indices_60s]\n",
    "X_top10_60s = X_60s[:, top_10_indices_60s]\n",
    "\n",
    "# ========== 3. æ··åˆæŠ•ç¥¨ (LOSO) ==========\n",
    "print(\"\\nğŸ“Š æ­¥é©Ÿ3: LOSOæ··åˆæŠ•ç¥¨è¨“ç·´...\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_true_60s = []\n",
    "y_pred_60s = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X_60s, y_60s, groups=pineapple_ids_60s), 1):\n",
    "    test_pid = pineapple_ids_60s[test_idx][0]\n",
    "    print(f\"   Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    \n",
    "    y_train, y_test = y_60s[train_idx], y_60s[test_idx]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # æ¨¡å‹ 1: RF with Top-5\n",
    "    X_train_top5 = X_top5_60s[train_idx]\n",
    "    X_test_top5 = X_top5_60s[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_top5)\n",
    "    X_test_scaled = scaler.transform(X_test_top5)\n",
    "    \n",
    "    rf1 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf1.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf1.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 2: RF with Top-10\n",
    "    X_train_top10 = X_top10_60s[train_idx]\n",
    "    X_test_top10 = X_top10_60s[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_top10)\n",
    "    X_test_scaled = scaler.transform(X_test_top10)\n",
    "    \n",
    "    rf2 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf2.fit(X_train_scaled, y_train)\n",
    "    predictions.append(rf2.predict(X_test_scaled))\n",
    "    \n",
    "    # æ¨¡å‹ 3: XGBoost with Top-5\n",
    "    X_train_top5 = X_top5_60s[train_idx]\n",
    "    X_test_top5 = X_top5_60s[test_idx]\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_top5)\n",
    "    X_test_scaled = scaler.transform(X_test_top5)\n",
    "    \n",
    "    xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, \n",
    "                       random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    predictions.append(xgb.predict(X_test_scaled))\n",
    "    \n",
    "    # æŠ•ç¥¨\n",
    "    predictions = np.array(predictions).T\n",
    "    y_pred_ensemble = []\n",
    "    \n",
    "    for sample_preds in predictions:\n",
    "        vote_counts = Counter(sample_preds)\n",
    "        majority_vote = vote_counts.most_common(1)[0][0]\n",
    "        y_pred_ensemble.append(majority_vote)\n",
    "    \n",
    "    y_true_60s.extend(y_test)\n",
    "    y_pred_60s.extend(y_pred_ensemble)\n",
    "\n",
    "# ========== 4. è©•ä¼°çµæœ ==========\n",
    "acc_60s = accuracy_score(y_true_60s, y_pred_60s)\n",
    "cm_60s = confusion_matrix(y_true_60s, y_pred_60s)\n",
    "stage2_recall_60s = cm_60s[2, 2] / cm_60s[2, :].sum() if cm_60s[2, :].sum() > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š 60ç§’çª—å£ + æ··åˆæŠ•ç¥¨ çµæœ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nâœ… 60ç§’çª—å£ æº–ç¢ºç‡: {acc_60s:.3f} ({acc_60s:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall_60s:.1%} ({cm_60s[2, 2]}/{cm_60s[2, :].sum()})\")\n",
    "\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_60s, y_pred_60s,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_60s)\n",
    "\n",
    "# ========== 5. ç¸½çµæ¯”è¼ƒ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š çª—å£å¤§å°æ¯”è¼ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_comparison = [\n",
    "    ('120ç§’çª—å£ + æ··åˆæŠ•ç¥¨', 222, 0.761, 0.147),\n",
    "    ('60ç§’çª—å£ + æ··åˆæŠ•ç¥¨', len(y_60s), acc_60s, stage2_recall_60s)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æ¨£æœ¬æ•¸':<10} {'æº–ç¢ºç‡':<12} {'Stage2 Recall':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for method, n_samples, acc, s2_recall in results_comparison:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.761 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {n_samples:<10} {acc:.1%}        {s2_recall:.1%}\")\n",
    "\n",
    "improvement = (acc_60s - 0.761) * 100\n",
    "\n",
    "print(f\"\\nç›¸æ¯”120ç§’çª—å£: {improvement:+.1f}%\")\n",
    "\n",
    "if acc_60s > 0.761:\n",
    "    print(f\"\\nğŸ‰ 60ç§’çª—å£æ›´å¥½ï¼\")\n",
    "    print(f\"å»ºè­°ä½¿ç”¨60ç§’çª—å£\")\n",
    "else:\n",
    "    print(f\"\\nğŸ’¡ 120ç§’çª—å£ä¾ç„¶æ›´å¥½\")\n",
    "    print(f\"å»ºè­°ä¿æŒä½¿ç”¨120ç§’çª—å£\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_60s = {\n",
    "    'window_size': 60,\n",
    "    'n_samples': len(y_60s),\n",
    "    'accuracy': acc_60s,\n",
    "    'stage2_recall': stage2_recall_60s,\n",
    "    'y_true': y_true_60s,\n",
    "    'y_pred': y_pred_60s,\n",
    "    'confusion_matrix': cm_60s,\n",
    "    'top_5_features': top_5_features_60s,\n",
    "    'top_10_features': top_10_features_60s\n",
    "}\n",
    "\n",
    "with open('models/solution_60s_ensemble_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_60s, f)\n",
    "\n",
    "print(f\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_60s_ensemble_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa09cf-c935-4b68-9615-1ee026d6f08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776ddc5-17a6-4475-910d-8ef24bdbf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å˜—è©¦æ·±åº¦å­¸ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ef8f423-f101-4a08-aaa8-65e4d14af7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”¥ æ–¹æ¡ˆ 7: å°æ¯”å­¸ç¿’ (Contrastive Learning)\n",
      "============================================================\n",
      "â³ é è¨ˆéœ€è¦ 1-2 å°æ™‚...\n",
      "ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: cuda\n",
      "\n",
      "ğŸ“Š æº–å‚™å°æ¯”å­¸ç¿’æ•¸æ“šé›†...\n",
      "âœ… æ•¸æ“šæº–å‚™å®Œæˆ:\n",
      "   åºåˆ—æ•¸é‡: 210\n",
      "   åºåˆ—å½¢ç‹€: (210, 120, 5)\n",
      "   æ¨™ç±¤åˆ†å¸ƒ: {np.int64(0): np.int64(87), np.int64(1): np.int64(77), np.int64(2): np.int64(32), np.int64(3): np.int64(14)}\n",
      "\n",
      "ğŸ”¥ é–‹å§‹ LOSO å°æ¯”å­¸ç¿’è¨“ç·´...\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 1/8 - æ¸¬è©¦é³³æ¢¨: 01\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (175, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (35, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 2/8 - æ¸¬è©¦é³³æ¢¨: 02\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (182, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (28, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 3/8 - æ¸¬è©¦é³³æ¢¨: 03\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (182, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (28, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 4/8 - æ¸¬è©¦é³³æ¢¨: 04\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (182, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (28, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 5/8 - æ¸¬è©¦é³³æ¢¨: 05\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (182, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (28, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 6/8 - æ¸¬è©¦é³³æ¢¨: 06\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (189, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (21, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 7/8 - æ¸¬è©¦é³³æ¢¨: 07\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (189, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (21, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Fold 8/8 - æ¸¬è©¦é³³æ¢¨: 08\n",
      "============================================================\n",
      "â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\n",
      "âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\n",
      "   è¨“ç·´é›†ç‰¹å¾µ: (189, 64)\n",
      "   æ¸¬è©¦é›†ç‰¹å¾µ: (21, 64)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ä½¿ç”¨å°æ¯”å­¸ç¿’ç‰¹å¾µè¨“ç·´åˆ†é¡å™¨...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1207: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1212: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1236: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1207: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1212: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1236: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1207: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1212: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1236: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1207: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1212: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1236: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š æ–¹æ¡ˆ 7: å°æ¯”å­¸ç¿’ æœ€çµ‚çµæœ\n",
      "============================================================\n",
      "\n",
      "âœ… å°æ¯”å­¸ç¿’ æº–ç¢ºç‡: 0.467 (46.7%)\n",
      "âœ… Stage 2 Recall: 0.0% (0/32)\n",
      "\n",
      "åˆ†é¡å ±å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Stage 0      0.449     0.759     0.564        87\n",
      "     Stage 1      0.762     0.416     0.538        77\n",
      "     Stage 2      0.000     0.000     0.000        32\n",
      "     Stage 3      0.000     0.000     0.000        14\n",
      "\n",
      "    accuracy                          0.467       210\n",
      "   macro avg      0.303     0.294     0.275       210\n",
      "weighted avg      0.465     0.467     0.431       210\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é™£:\n",
      "[[66  7 14  0]\n",
      " [38 32  7  0]\n",
      " [29  3  0  0]\n",
      " [14  0  0  0]]\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ç¸½é«”æ¯”è¼ƒ\n",
      "============================================================\n",
      "\n",
      "æ–¹æ³•                        æº–ç¢ºç‡         \n",
      "----------------------------------------\n",
      " åŸå§‹Baseline              62.2%\n",
      "âœ… æ–¹æ¡ˆJ (Top-5)             75.2%\n",
      "âœ… æ–¹æ¡ˆF (å½æ¨™ç±¤)               75.7%\n",
      "âœ… æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)              76.1%\n",
      " æ–¹æ¡ˆ7 (å°æ¯”å­¸ç¿’)              46.7%\n",
      "\n",
      "ç›¸æ¯”åŸå§‹ Baseline (62.2%): -15.5%\n",
      "ç›¸æ¯”ç›®å‰æœ€ä½³ (76.1%): -29.4%\n",
      "\n",
      "ğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_contrastive_results.pkl\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ä¸è¦ç”¨!!!å¤ªçˆ›äº†ï¼Œå¤±æ•—!!!\n",
    "\n",
    "\n",
    "# ===== æ–¹æ¡ˆ 7: å°æ¯”å­¸ç¿’ (Contrastive Learning) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”¥ æ–¹æ¡ˆ 7: å°æ¯”å­¸ç¿’ (Contrastive Learning)\")\n",
    "print(\"=\"*60)\n",
    "print(\"â³ é è¨ˆéœ€è¦ 1-2 å°æ™‚...\")\n",
    "\n",
    "# æª¢æŸ¥ GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# ========== 1. æº–å‚™å°æ¯”å­¸ç¿’æ•¸æ“š ==========\n",
    "print(\"\\nğŸ“Š æº–å‚™å°æ¯”å­¸ç¿’æ•¸æ“šé›†...\")\n",
    "\n",
    "def prepare_contrastive_data(arduino_features, maturity_levels, sequence_length=120):\n",
    "    \"\"\"\n",
    "    æº–å‚™å°æ¯”å­¸ç¿’ç”¨çš„æ™‚é–“åºåˆ—æ•¸æ“š\n",
    "    è¿”å›ï¼šåºåˆ—ã€æ¨™ç±¤ã€é³³æ¢¨ID\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    pineapple_ids = []\n",
    "    \n",
    "    sensor_cols = ['MQ2_Rs_R0', 'MQ3_Rs_R0', 'MQ9_Rs_R0', 'MQ135_Rs_R0', 'TGS2602_Rs_R0']\n",
    "    \n",
    "    for pid in arduino_features.keys():\n",
    "        if pid not in maturity_levels:\n",
    "            continue\n",
    "        \n",
    "        date_dict = arduino_features[pid]\n",
    "        pine_labels = maturity_levels[pid]\n",
    "        \n",
    "        offset = 0\n",
    "        for date in sorted(date_dict.keys()):\n",
    "            df = date_dict[date]\n",
    "            \n",
    "            available_cols = [col for col in sensor_cols if col in df.columns]\n",
    "            if not available_cols:\n",
    "                continue\n",
    "            \n",
    "            sensor_data = df[available_cols].values\n",
    "            n_samples = len(sensor_data)\n",
    "            \n",
    "            # æ»‘å‹•çª—å£\n",
    "            for i in range(0, n_samples - sequence_length + 1, sequence_length):\n",
    "                seq = sensor_data[i:i+sequence_length]\n",
    "                label_window = pine_labels[offset + i:offset + i + sequence_length]\n",
    "                \n",
    "                unique, counts = np.unique(label_window, return_counts=True)\n",
    "                majority_label = unique[np.argmax(counts)]\n",
    "                \n",
    "                sequences.append(seq)\n",
    "                labels.append(majority_label)\n",
    "                pineapple_ids.append(pid)\n",
    "            \n",
    "            offset += n_samples\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), np.array(pineapple_ids)\n",
    "\n",
    "sequences, labels, pineapple_ids = prepare_contrastive_data(\n",
    "    arduino_features, maturity_levels, sequence_length=120\n",
    ")\n",
    "\n",
    "print(f\"âœ… æ•¸æ“šæº–å‚™å®Œæˆ:\")\n",
    "print(f\"   åºåˆ—æ•¸é‡: {len(sequences)}\")\n",
    "print(f\"   åºåˆ—å½¢ç‹€: {sequences.shape}\")\n",
    "print(f\"   æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "# ========== 2. å°æ¯”å­¸ç¿’æ¨¡å‹ ==========\n",
    "class ContrastiveEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    æ™‚é–“åºåˆ—å°æ¯”å­¸ç¿’ç·¨ç¢¼å™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, embedding_size=32):\n",
    "        super(ContrastiveEncoder, self).__init__()\n",
    "        \n",
    "        # LSTM ç·¨ç¢¼å™¨\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # æŠ•å½±é ­ï¼ˆç”¨æ–¼å°æ¯”å­¸ç¿’ï¼‰\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embedding_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # å–æœ€å¾Œæ™‚é–“æ­¥\n",
    "        last_hidden = hidden[-1]  # (batch, hidden_size)\n",
    "        \n",
    "        # æŠ•å½±åˆ°åµŒå…¥ç©ºé–“\n",
    "        embedding = self.projector(last_hidden)\n",
    "        \n",
    "        return embedding, last_hidden\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    å°æ¯”æå¤±ï¼šInfoNCE Loss (SimCLR)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, embeddings, pineapple_ids, labels):\n",
    "        \"\"\"\n",
    "        embeddings: (batch, embedding_size)\n",
    "        pineapple_ids: (batch,) é³³æ¢¨ID\n",
    "        labels: (batch,) æˆç†Ÿåº¦æ¨™ç±¤\n",
    "        \"\"\"\n",
    "        batch_size = embeddings.shape[0]\n",
    "        \n",
    "        # æ­£è¦åŒ–\n",
    "        embeddings = nn.functional.normalize(embeddings, dim=1)\n",
    "        \n",
    "        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£\n",
    "        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n",
    "        \n",
    "        # æ§‹å»ºæ­£æ¨£æœ¬maskï¼ˆåŒä¸€é¡†é³³æ¢¨çš„ä¸åŒæ™‚åˆ»ï¼‰\n",
    "        pineapple_ids = pineapple_ids.unsqueeze(1)\n",
    "        positive_mask = (pineapple_ids == pineapple_ids.T).float()\n",
    "        \n",
    "        # ç§»é™¤å°è§’ç·šï¼ˆè‡ªå·±å’Œè‡ªå·±ï¼‰\n",
    "        positive_mask.fill_diagonal_(0)\n",
    "        \n",
    "        # æ§‹å»ºè² æ¨£æœ¬maskï¼ˆä¸åŒé³³æ¢¨ï¼‰\n",
    "        negative_mask = (pineapple_ids != pineapple_ids.T).float()\n",
    "        \n",
    "        # InfoNCE Loss\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        \n",
    "        # åˆ†å­ï¼šæ­£æ¨£æœ¬çš„ç›¸ä¼¼åº¦\n",
    "        positive_sim = (exp_sim * positive_mask).sum(dim=1)\n",
    "        \n",
    "        # åˆ†æ¯ï¼šæ‰€æœ‰è² æ¨£æœ¬çš„ç›¸ä¼¼åº¦\n",
    "        negative_sim = (exp_sim * negative_mask).sum(dim=1)\n",
    "        \n",
    "        # é¿å…é™¤é›¶\n",
    "        loss = -torch.log(positive_sim / (positive_sim + negative_sim + 1e-8))\n",
    "        \n",
    "        # åªå°æœ‰æ­£æ¨£æœ¬çš„è¨ˆç®—æå¤±\n",
    "        has_positive = positive_mask.sum(dim=1) > 0\n",
    "        if has_positive.sum() > 0:\n",
    "            loss = loss[has_positive].mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# ========== 3. LOSO å°æ¯”å­¸ç¿’è¨“ç·´ ==========\n",
    "print(\"\\nğŸ”¥ é–‹å§‹ LOSO å°æ¯”å­¸ç¿’è¨“ç·´...\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "all_embeddings_list = []\n",
    "all_labels_list = []\n",
    "all_pineapple_ids_list = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(sequences, labels, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š Fold {fold}/8 - æ¸¬è©¦é³³æ¢¨: {test_pid}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_train, X_test = sequences[train_idx], sequences[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "    pid_train, pid_test = pineapple_ids[train_idx], pineapple_ids[test_idx]\n",
    "    \n",
    "    # è½‰æ›ç‚º Tensor\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    pid_train_tensor = torch.LongTensor([int(pid) for pid in pid_train]).to(device)\n",
    "    \n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    # å»ºç«‹æ¨¡å‹\n",
    "    input_size = X_train.shape[2]\n",
    "    model = ContrastiveEncoder(\n",
    "        input_size=input_size,\n",
    "        hidden_size=64,\n",
    "        embedding_size=32\n",
    "    ).to(device)\n",
    "    \n",
    "    contrastive_loss_fn = ContrastiveLoss(temperature=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # å°æ¯”å­¸ç¿’è¨“ç·´\n",
    "    print(f\"â³ å°æ¯”å­¸ç¿’è¨“ç·´ä¸­...\")\n",
    "    model.train()\n",
    "    \n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Mini-batch è¨“ç·´\n",
    "        indices = torch.randperm(len(X_train_tensor))\n",
    "        \n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            \n",
    "            batch_X = X_train_tensor[batch_indices]\n",
    "            batch_pid = pid_train_tensor[batch_indices]\n",
    "            batch_y = y_train_tensor[batch_indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # å‰å‘å‚³æ’­\n",
    "            embeddings, _ = model(batch_X)\n",
    "            \n",
    "            # å°æ¯”æå¤±\n",
    "            loss = contrastive_loss_fn(embeddings, batch_pid, batch_y)\n",
    "            \n",
    "            if loss > 0:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                n_batches += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 and n_batches > 0:\n",
    "            avg_loss = total_loss / n_batches\n",
    "            print(f\"   Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # æå–ç‰¹å¾µ\n",
    "    print(f\"âœ… æå–å°æ¯”å­¸ç¿’ç‰¹å¾µ...\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, train_features = model(X_train_tensor)\n",
    "        train_features_np = train_features.cpu().numpy()\n",
    "        \n",
    "        _, test_features = model(X_test_tensor)\n",
    "        test_features_np = test_features.cpu().numpy()\n",
    "    \n",
    "    # å„²å­˜ç‰¹å¾µ\n",
    "    all_embeddings_list.append(test_features_np)\n",
    "    all_labels_list.append(y_test)\n",
    "    all_pineapple_ids_list.append(pid_test)\n",
    "    \n",
    "    print(f\"   è¨“ç·´é›†ç‰¹å¾µ: {train_features_np.shape}\")\n",
    "    print(f\"   æ¸¬è©¦é›†ç‰¹å¾µ: {test_features_np.shape}\")\n",
    "\n",
    "# ========== 4. ä½¿ç”¨å°æ¯”å­¸ç¿’ç‰¹å¾µé€²è¡Œåˆ†é¡ ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ä½¿ç”¨å°æ¯”å­¸ç¿’ç‰¹å¾µè¨“ç·´åˆ†é¡å™¨...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logo2 = LeaveOneGroupOut()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_true_contrastive = []\n",
    "y_pred_contrastive = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(logo2.split(sequences, labels, groups=pineapple_ids), 1):\n",
    "    test_pid = pineapple_ids[test_idx][0]\n",
    "    \n",
    "    X_train, X_test = sequences[train_idx], sequences[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "    \n",
    "    # è½‰æ›ä¸¦æå–ç‰¹å¾µ\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    # è¨“ç·´å°æ¯”å­¸ç¿’æ¨¡å‹ï¼ˆç°¡åŒ–ç‰ˆï¼Œå¿«é€Ÿè¨“ç·´ï¼‰\n",
    "    input_size = X_train.shape[2]\n",
    "    model = ContrastiveEncoder(input_size=input_size, hidden_size=64, embedding_size=32).to(device)\n",
    "    \n",
    "    pid_train_tensor = torch.LongTensor([int(pid) for pid in pineapple_ids[train_idx]]).to(device)\n",
    "    contrastive_loss_fn = ContrastiveLoss(temperature=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(30):  # å¿«é€Ÿè¨“ç·´\n",
    "        for i in range(0, len(X_train_tensor), 32):\n",
    "            batch_X = X_train_tensor[i:i+32]\n",
    "            batch_pid = pid_train_tensor[i:i+32]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embeddings, _ = model(batch_X)\n",
    "            loss = contrastive_loss_fn(embeddings, batch_pid, None)\n",
    "            \n",
    "            if loss > 0:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    # æå–ç‰¹å¾µ\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, train_features = model(X_train_tensor)\n",
    "        _, test_features = model(X_test_tensor)\n",
    "        \n",
    "        train_features_np = train_features.cpu().numpy()\n",
    "        test_features_np = test_features.cpu().numpy()\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    train_features_scaled = scaler.fit_transform(train_features_np)\n",
    "    test_features_scaled = scaler.transform(test_features_np)\n",
    "    \n",
    "    # è¨“ç·´åˆ†é¡å™¨\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(train_features_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(test_features_scaled)\n",
    "    y_true_contrastive.extend(y_test)\n",
    "    y_pred_contrastive.extend(y_pred)\n",
    "\n",
    "# ========== 5. è©•ä¼°çµæœ ==========\n",
    "acc_contrastive = accuracy_score(y_true_contrastive, y_pred_contrastive)\n",
    "cm_contrastive = confusion_matrix(y_true_contrastive, y_pred_contrastive)\n",
    "stage2_recall = cm_contrastive[2, 2] / cm_contrastive[2, :].sum() if cm_contrastive[2, :].sum() > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š æ–¹æ¡ˆ 7: å°æ¯”å­¸ç¿’ æœ€çµ‚çµæœ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nâœ… å°æ¯”å­¸ç¿’ æº–ç¢ºç‡: {acc_contrastive:.3f} ({acc_contrastive:.1%})\")\n",
    "print(f\"âœ… Stage 2 Recall: {stage2_recall:.1%} ({cm_contrastive[2, 2]}/{cm_contrastive[2, :].sum()})\")\n",
    "\n",
    "print(f\"\\nåˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(y_true_contrastive, y_pred_contrastive,\n",
    "                          target_names=['Stage 0', 'Stage 1', 'Stage 2', 'Stage 3'],\n",
    "                          digits=3))\n",
    "\n",
    "print(f\"\\næ··æ·†çŸ©é™£:\")\n",
    "print(cm_contrastive)\n",
    "\n",
    "# æ¯”è¼ƒ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ç¸½é«”æ¯”è¼ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    ('åŸå§‹Baseline', 0.622),\n",
    "    ('æ–¹æ¡ˆJ (Top-5)', 0.752),\n",
    "    ('æ–¹æ¡ˆF (å½æ¨™ç±¤)', 0.757),\n",
    "    ('æ–¹æ¡ˆ2 (æ··åˆæŠ•ç¥¨)', 0.761),\n",
    "    ('æ–¹æ¡ˆ7 (å°æ¯”å­¸ç¿’)', acc_contrastive)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ–¹æ³•':<25} {'æº–ç¢ºç‡':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for method, acc in results:\n",
    "    symbol = \"ğŸ”¥\" if acc > 0.761 else \"âœ…\" if acc >= 0.752 else \"\"\n",
    "    print(f\"{symbol} {method:<23} {acc:.1%}\")\n",
    "\n",
    "improvement_vs_baseline = (acc_contrastive - 0.622) * 100\n",
    "improvement_vs_best = (acc_contrastive - 0.761) * 100\n",
    "\n",
    "print(f\"\\nç›¸æ¯”åŸå§‹ Baseline (62.2%): {improvement_vs_baseline:+.1f}%\")\n",
    "print(f\"ç›¸æ¯”ç›®å‰æœ€ä½³ (76.1%): {improvement_vs_best:+.1f}%\")\n",
    "\n",
    "if acc_contrastive > 0.761:\n",
    "    print(f\"\\nğŸ‰ğŸ‰ğŸ‰ è¶…è¶Šæ–¹æ¡ˆ2ï¼æ–°ç´€éŒ„ï¼ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "results_contrastive = {\n",
    "    'accuracy': acc_contrastive,\n",
    "    'stage2_recall': stage2_recall,\n",
    "    'y_true': y_true_contrastive,\n",
    "    'y_pred': y_pred_contrastive,\n",
    "    'confusion_matrix': cm_contrastive\n",
    "}\n",
    "\n",
    "with open('models/solution_contrastive_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_contrastive, f)\n",
    "\n",
    "print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³: models/solution_contrastive_results.pkl\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086924c-08f8-4a11-bed4-608f9eea4ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
